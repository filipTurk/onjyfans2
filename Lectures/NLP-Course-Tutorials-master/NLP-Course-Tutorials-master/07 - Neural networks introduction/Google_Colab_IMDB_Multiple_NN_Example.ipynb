{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYagNo7w0KR6",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "# Google Colab - Multiple neural network analysis\n",
        "\n",
        "<sup>This notebook is a part of Natural Language Processing class at the University of Ljubljana, Faculty for computer and information science. Please contact [slavko.zitnik@fri.uni-lj.si](mailto:slavko.zitnik@fri.uni-lj.si) for any comments.</sub>\n",
        "\n",
        "## Google Colab specifics\n",
        "\n",
        "Before running this notebook you should select GPU-accelerated environment.\n",
        "\n",
        "### Libraries installation\n",
        "\n",
        "Some of the libraries are already available but you should manually install them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G96VrF880KR9",
        "outputId": "f9e46512-78d3-43bf-86a9-5db793defb31",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.18.0\n",
            "  Using cached tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting keras==3.5.0\n",
            "  Downloading keras-3.5.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting pandas==2.2.2\n",
            "  Using cached pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.70.0)\n",
            "Collecting tensorboard<2.19,>=2.18 (from tensorflow==2.18.0)\n",
            "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (1.26.4)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (3.12.1)\n",
            "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow==2.18.0)\n",
            "  Downloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0) (0.37.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras==3.5.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras==3.5.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras==3.5.0) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.18.0) (0.45.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.1.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras==3.5.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras==3.5.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras==3.5.0) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0) (3.0.2)\n",
            "Downloading tensorflow-2.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.4/615.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-3.5.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, pandas, keras, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.3.2\n",
            "    Uninstalling ml-dtypes-0.3.2:\n",
            "      Successfully uninstalled ml-dtypes-0.3.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.16.2\n",
            "    Uninstalling tensorboard-2.16.2:\n",
            "      Successfully uninstalled tensorboard-2.16.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.1\n",
            "    Uninstalling pandas-2.2.1:\n",
            "      Successfully uninstalled pandas-2.2.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.0.5\n",
            "    Uninstalling keras-3.0.5:\n",
            "      Successfully uninstalled keras-3.0.5\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.16.1\n",
            "    Uninstalling tensorflow-2.16.1:\n",
            "      Successfully uninstalled tensorflow-2.16.1\n",
            "Successfully installed keras-3.5.0 ml-dtypes-0.4.1 pandas-2.2.2 tensorboard-2.18.0 tensorflow-2.18.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.18.0 keras==3.5.0 pandas==2.2.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JdpSdBG0KR-",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### I/O device registering\n",
        "\n",
        "Current working directory is set to `/content` by default. You can also give access to your Google Drive to save models/results/... there."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xHL6AMfe0KR_",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [],
      "source": [
        "# Uncomment to use\n",
        "#from google.colab import drive\n",
        "#drive.mount(\"/content/drive/\")\n",
        "\n",
        "# Access your Drive data using folder '/content/drive/MyDrive'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8BWktGo0KR_",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "### GPU device selection review\n",
        "\n",
        "You can directly use system command `nvidia-smi` or use Python library (e.g. Tensorflow or PyTorch) to check this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFU6m1E90KR_",
        "outputId": "2e004e02-dc77-4e7e-f7bb-8ca21c08400d",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar  9 19:03:54 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLxMk7GN0KR_",
        "outputId": "b11f8967-cecc-492f-b046-523893ecb45c",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensorflow version: 2.18.0\n",
            "The system contains '1' Physical GPUs and '1' Logical GPUs\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "print(f\"Tensorflow version: {tf.__version__}\")\n",
        "\n",
        "# Restrict TensorFlow to only allocate 4GBs of memory on the first GPU\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(f\"The system contains '{len(gpus)}' Physical GPUs and '{len(logical_gpus)}' Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    print(e)\n",
        "else:\n",
        "    print(f\"Your system does not contain a GPU that could be used by Tensorflow!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTQJ_nf70KSA",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## IMDB sentiment analysis example\n",
        "\n",
        "First we download the IMDB dataset. We present each word with a specific index from a vocabulary of 10000 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnuPvw080KSA",
        "outputId": "6ee3f952-9a3b-4554-9c9b-7104ebc88639",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Number of reviews: 25000.\n",
            "First review: \n",
            "\t[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32].\n",
            "First label: 1.\n",
            "Length of first review before padding: 218.\n",
            "\n",
            "After padding:\n",
            "First review: \n",
            "\t[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
            "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
            "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
            "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
            "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
            "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
            "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
            "    4 2223 5244   16  480   66 3785   33    4  130   12   16   38  619\n",
            "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
            "   28   77   52    5   14  407   16   82    2    8    4  107  117 5952\n",
            "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
            "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
            "   98   32 2071   56   26  141    6  194 7486   18    4  226   22   21\n",
            "  134  476   26  480    5  144   30 5535   18   51   36   28  224   92\n",
            "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
            " 4472  113  103   32   15   16 5345   19  178   32].\n",
            "Length of first review after padding: 500.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "# number of distinct words\n",
        "vocabulary_size = 10000\n",
        "\n",
        "# number of words per review\n",
        "max_review_length = 500\n",
        "\n",
        "# load Keras IMDB movie reviews dataset\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
        "\n",
        "print(f'Number of reviews: {len(X_train)}.')\n",
        "print(f'First review: \\n\\t{X_train[0]}.')\n",
        "print(f'First label: {y_train[0]}.')\n",
        "print(f'Length of first review before padding: {len(X_train[0])}.')\n",
        "\n",
        "# padding reviews\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "print(f\"\\nAfter padding:\")\n",
        "print(f'First review: \\n\\t{X_train[0]}.')\n",
        "print(f'Length of first review after padding: {len(X_train[0])}.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJo3CGLv0KSB",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Mapping between real words and indexes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53a245kt0KSC",
        "outputId": "30450702-502f-4f36-a220-31678c782e04",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "First review: \n",
            "\tText: please give this one a miss br br and the rest of the cast rendered terrible performances the show is flat flat flat br br i don't know how michael madison could have allowed this one on his plate he almost seemed to know this wasn't going to work out and his performance was quite so all you madison fans give this a miss\n",
            "\tLabel: NEGATIVE\n",
            "\n",
            "Last review: \n",
            "\tText: a good ol' boy film is almost required to have car chases a storyline that has a vague resemblance to plot and at least one very pretty country gal with short shorts and a low top the pretty gal is here dressed in designer but the redneck stop there jimmy dean is a natural as a but as a tough guy former sheriff he comes up way short big john is big but he isn't convincing with the bad part of his bug eyed jack is a hoot as always and bo hopkins has been playing this same part for decades ned beatty also does his part in a small role but there is no story it more like an episode of in the heat of the night than a feature film with easily predictable sentiment perhaps the most glaring problem with this movie is charlie daniels singing the theme you know the one it was made famous by jimmy dean\n",
            "\tLabel: NEGATIVE\n"
          ]
        }
      ],
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "# The first indices are reserved, so we map the index for our use\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNK>\"] = 2  # unknown\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "# Decode review text\n",
        "def decode_review(text_ids, cls):\n",
        "    text = ' '.join([reverse_word_index.get(i, '?') for i in text_ids if i not in [0,1,2,3]])\n",
        "    label = 'POSITIVE' if cls == 1 else 'NEGATIVE'\n",
        "    return f\"\\tText: {text}\\n\\tLabel: {label}\"\n",
        "\n",
        "# First review\n",
        "print(f\"First review: \\n{decode_review(X_test[0], y_test[0])}\")\n",
        "# Last review\n",
        "print(f\"\\nLast review: \\n{decode_review(X_test[len(X_test)-1], y_test[len(X_test)-1])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04jDwdIp0KSC",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Below we create multiple models and evaluate their performance:\n",
        "\n",
        "* **FFN**: Input to the models are word indices directly fed into a Dense layer.\n",
        "* **FFN with embeddings**: After creation of embedding vectors, the same architecture as in *FFN* is used.\n",
        "* **CNN**: Similar to the *FFN with embeddings* model with a convoluational and pooling layer immediatelly after embedding layer,\n",
        "* **RNN**: Simple RNN model with 100 hidden dimensions and prediction at the end.\n",
        "* **CNN+RNN**: A combination of *CNN* and *RNN* models above.\n",
        "\n",
        "The runtime for the below script should take about a half hour using Tesla V100 32GB GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVbZUPjD0KSC",
        "outputId": "cbeba95e-b448-4b11-f170-329cc54fc20e",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "196/196 - 3s - 13ms/step - accuracy: 0.5022 - loss: 186.4420\n",
            "Epoch 2/20\n",
            "196/196 - 0s - 2ms/step - accuracy: 0.5800 - loss: 53.7939\n",
            "Epoch 3/20\n",
            "196/196 - 0s - 1ms/step - accuracy: 0.6566 - loss: 20.4492\n",
            "Epoch 4/20\n",
            "196/196 - 0s - 1ms/step - accuracy: 0.7277 - loss: 8.8363\n",
            "Epoch 5/20\n",
            "196/196 - 0s - 2ms/step - accuracy: 0.7737 - loss: 4.5837\n",
            "Epoch 6/20\n",
            "196/196 - 0s - 1ms/step - accuracy: 0.8091 - loss: 2.7275\n",
            "Epoch 7/20\n",
            "196/196 - 0s - 1ms/step - accuracy: 0.8320 - loss: 1.7039\n",
            "Epoch 8/20\n",
            "196/196 - 0s - 1ms/step - accuracy: 0.8572 - loss: 1.1779\n",
            "Epoch 9/20\n",
            "196/196 - 0s - 2ms/step - accuracy: 0.8721 - loss: 0.8908\n",
            "Epoch 10/20\n",
            "196/196 - 0s - 2ms/step - accuracy: 0.8769 - loss: 0.7565\n",
            "Epoch 11/20\n",
            "196/196 - 0s - 1ms/step - accuracy: 0.8898 - loss: 0.6402\n",
            "Epoch 12/20\n",
            "196/196 - 0s - 1ms/step - accuracy: 0.9010 - loss: 0.5323\n",
            "Epoch 13/20\n",
            "196/196 - 0s - 1ms/step - accuracy: 0.9005 - loss: 0.5594\n",
            "Epoch 14/20\n",
            "196/196 - 0s - 1ms/step - accuracy: 0.9082 - loss: 0.4993\n",
            "Epoch 15/20\n",
            "196/196 - 0s - 2ms/step - accuracy: 0.9107 - loss: 0.4923\n",
            "Epoch 16/20\n",
            "196/196 - 0s - 2ms/step - accuracy: 0.9132 - loss: 0.5027\n",
            "Epoch 17/20\n",
            "196/196 - 1s - 3ms/step - accuracy: 0.9065 - loss: 0.5928\n",
            "Epoch 18/20\n",
            "196/196 - 0s - 2ms/step - accuracy: 0.9118 - loss: 0.5592\n",
            "Epoch 19/20\n",
            "196/196 - 0s - 3ms/step - accuracy: 0.9126 - loss: 0.5501\n",
            "Epoch 20/20\n",
            "196/196 - 0s - 1ms/step - accuracy: 0.8948 - loss: 0.8145\n",
            "Epoch 1/20\n",
            "196/196 - 7s - 38ms/step - accuracy: 0.7304 - loss: 0.7060\n",
            "Epoch 2/20\n",
            "196/196 - 3s - 14ms/step - accuracy: 0.9384 - loss: 0.1681\n",
            "Epoch 3/20\n",
            "196/196 - 3s - 14ms/step - accuracy: 0.9949 - loss: 0.0276\n",
            "Epoch 4/20\n",
            "196/196 - 3s - 14ms/step - accuracy: 0.9996 - loss: 0.0039\n",
            "Epoch 5/20\n",
            "196/196 - 5s - 26ms/step - accuracy: 0.9999 - loss: 0.0017\n",
            "Epoch 6/20\n",
            "196/196 - 5s - 26ms/step - accuracy: 1.0000 - loss: 4.5355e-04\n",
            "Epoch 7/20\n",
            "196/196 - 5s - 26ms/step - accuracy: 1.0000 - loss: 2.6026e-04\n",
            "Epoch 8/20\n",
            "196/196 - 5s - 26ms/step - accuracy: 1.0000 - loss: 1.7498e-04\n",
            "Epoch 9/20\n",
            "196/196 - 3s - 14ms/step - accuracy: 1.0000 - loss: 1.2984e-04\n",
            "Epoch 10/20\n",
            "196/196 - 5s - 26ms/step - accuracy: 1.0000 - loss: 1.0017e-04\n",
            "Epoch 11/20\n",
            "196/196 - 5s - 26ms/step - accuracy: 1.0000 - loss: 7.9726e-05\n",
            "Epoch 12/20\n",
            "196/196 - 3s - 14ms/step - accuracy: 1.0000 - loss: 6.4605e-05\n",
            "Epoch 13/20\n",
            "196/196 - 3s - 14ms/step - accuracy: 1.0000 - loss: 5.3586e-05\n",
            "Epoch 14/20\n",
            "196/196 - 5s - 27ms/step - accuracy: 1.0000 - loss: 4.4485e-05\n",
            "Epoch 15/20\n",
            "196/196 - 5s - 26ms/step - accuracy: 1.0000 - loss: 3.7601e-05\n",
            "Epoch 16/20\n",
            "196/196 - 3s - 15ms/step - accuracy: 1.0000 - loss: 3.1908e-05\n",
            "Epoch 17/20\n",
            "196/196 - 5s - 27ms/step - accuracy: 1.0000 - loss: 2.7383e-05\n",
            "Epoch 18/20\n",
            "196/196 - 3s - 15ms/step - accuracy: 1.0000 - loss: 2.3569e-05\n",
            "Epoch 19/20\n",
            "196/196 - 5s - 26ms/step - accuracy: 1.0000 - loss: 2.0247e-05\n",
            "Epoch 20/20\n",
            "196/196 - 5s - 26ms/step - accuracy: 1.0000 - loss: 1.7579e-05\n",
            "Epoch 1/20\n",
            "196/196 - 13s - 64ms/step - accuracy: 0.7467 - loss: 0.4642\n",
            "Epoch 2/20\n",
            "196/196 - 16s - 84ms/step - accuracy: 0.9278 - loss: 0.1875\n",
            "Epoch 3/20\n",
            "196/196 - 7s - 34ms/step - accuracy: 0.9649 - loss: 0.1026\n",
            "Epoch 4/20\n",
            "196/196 - 7s - 33ms/step - accuracy: 0.9879 - loss: 0.0419\n",
            "Epoch 5/20\n",
            "196/196 - 7s - 33ms/step - accuracy: 0.9969 - loss: 0.0130\n",
            "Epoch 6/20\n",
            "196/196 - 10s - 52ms/step - accuracy: 0.9952 - loss: 0.0139\n",
            "Epoch 7/20\n",
            "196/196 - 6s - 33ms/step - accuracy: 0.9943 - loss: 0.0163\n",
            "Epoch 8/20\n",
            "196/196 - 6s - 33ms/step - accuracy: 0.9952 - loss: 0.0150\n",
            "Epoch 9/20\n",
            "196/196 - 6s - 33ms/step - accuracy: 0.9978 - loss: 0.0075\n",
            "Epoch 10/20\n",
            "196/196 - 10s - 52ms/step - accuracy: 0.9995 - loss: 0.0022\n",
            "Epoch 11/20\n",
            "196/196 - 6s - 33ms/step - accuracy: 1.0000 - loss: 3.5273e-04\n",
            "Epoch 12/20\n",
            "196/196 - 10s - 53ms/step - accuracy: 1.0000 - loss: 1.0522e-04\n",
            "Epoch 13/20\n",
            "196/196 - 10s - 52ms/step - accuracy: 1.0000 - loss: 6.6694e-05\n",
            "Epoch 14/20\n",
            "196/196 - 10s - 52ms/step - accuracy: 1.0000 - loss: 5.0883e-05\n",
            "Epoch 15/20\n",
            "196/196 - 6s - 33ms/step - accuracy: 1.0000 - loss: 4.0491e-05\n",
            "Epoch 16/20\n",
            "196/196 - 6s - 33ms/step - accuracy: 1.0000 - loss: 3.3121e-05\n",
            "Epoch 17/20\n",
            "196/196 - 10s - 52ms/step - accuracy: 1.0000 - loss: 2.7565e-05\n",
            "Epoch 18/20\n",
            "196/196 - 6s - 33ms/step - accuracy: 1.0000 - loss: 2.3168e-05\n",
            "Epoch 19/20\n",
            "196/196 - 10s - 53ms/step - accuracy: 1.0000 - loss: 1.9696e-05\n",
            "Epoch 20/20\n",
            "196/196 - 10s - 52ms/step - accuracy: 1.0000 - loss: 1.6851e-05\n",
            "Epoch 1/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 45ms/step - accuracy: 0.6027 - loss: 0.6495\n",
            "Epoch 2/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 43ms/step - accuracy: 0.7978 - loss: 0.4578\n",
            "Epoch 3/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 43ms/step - accuracy: 0.8221 - loss: 0.4010\n",
            "Epoch 1/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 30ms/step - accuracy: 0.6432 - loss: 0.6009\n",
            "Epoch 2/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 25ms/step - accuracy: 0.8677 - loss: 0.3193\n",
            "Epoch 3/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.7461 - loss: 0.5529\n",
            "Epoch 4/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.8012 - loss: 0.4332\n",
            "Epoch 5/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.8954 - loss: 0.2607\n",
            "Epoch 6/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9057 - loss: 0.2341\n",
            "Epoch 7/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9460 - loss: 0.1456\n",
            "Epoch 8/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9509 - loss: 0.1365\n",
            "Epoch 9/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9097 - loss: 0.2169\n",
            "Epoch 10/10\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 25ms/step - accuracy: 0.9727 - loss: 0.0796\n",
            "                  Model Accuracy\n",
            "0                  FFNN   50.64%\n",
            "1  FFNN with Embeddings   87.04%\n",
            "2                   CNN   87.56%\n",
            "3                   RNN   63.53%\n",
            "4               CNN+RNN   77.85%\n"
          ]
        }
      ],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, SimpleRNN, Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "import pandas as pd\n",
        "\n",
        "EMBEDDING_DIM = 256\n",
        "\n",
        "# Fully connected neural network\n",
        "model_ffn = Sequential()\n",
        "model_ffn.add(Dense(250, activation='relu'))\n",
        "model_ffn.add(Dense(1, activation='sigmoid'))\n",
        "model_ffn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history_ffn = model_ffn.fit(X_train, y_train, epochs=20, batch_size=128, verbose=2)\n",
        "scores_ffn = model_ffn.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Fully connected neural network with embeddings\n",
        "model_ffne = Sequential()\n",
        "model_ffne.add(Embedding(vocabulary_size, EMBEDDING_DIM))\n",
        "model_ffne.add(Flatten())\n",
        "model_ffne.add(Dense(250, activation='relu'))\n",
        "model_ffne.add(Dense(1, activation='sigmoid'))\n",
        "model_ffne.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history_ffne = model_ffne.fit(X_train, y_train, epochs=20, batch_size=128, verbose=2)\n",
        "scores_ffne = model_ffne.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Convolutional neural network\n",
        "model_cnn = Sequential()\n",
        "model_cnn.add(Embedding(vocabulary_size, EMBEDDING_DIM))\n",
        "model_cnn.add(Conv1D(filters=200, kernel_size=3, padding='same', activation='relu'))\n",
        "model_cnn.add(MaxPooling1D(pool_size=2))\n",
        "model_cnn.add(Flatten())\n",
        "model_cnn.add(Dense(250, activation='relu'))\n",
        "model_cnn.add(Dense(1, activation='sigmoid'))\n",
        "model_cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history_cnn = model_cnn.fit(X_train, y_train, epochs=20, batch_size=128, verbose=2)\n",
        "scores_cnn = model_cnn.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Recurrent Neural Network\n",
        "model_rnn = Sequential()\n",
        "model_rnn.add(Embedding(vocabulary_size, EMBEDDING_DIM))\n",
        "model_rnn.add(SimpleRNN(100))\n",
        "model_rnn.add(Dense(1, activation='sigmoid'))\n",
        "model_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history_rnn = model_rnn.fit(X_train, y_train, epochs=3, batch_size=64)\n",
        "scores_rnn = model_rnn.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Convolutional + Recurrent Neural Network\n",
        "model_cnn_rnn = Sequential()\n",
        "model_cnn_rnn.add(Embedding(vocabulary_size, EMBEDDING_DIM))\n",
        "model_cnn_rnn.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
        "model_cnn_rnn.add(MaxPooling1D(pool_size=2))\n",
        "model_cnn_rnn.add(SimpleRNN(100))\n",
        "model_cnn_rnn.add(Dense(1, activation='sigmoid'))\n",
        "model_cnn_rnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history_cnn_rnn = model_cnn_rnn.fit(X_train, y_train, epochs=10, batch_size=64)\n",
        "scores_cnn_rnn = model_cnn_rnn.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "# Evaluation\n",
        "df = pd.DataFrame({'Model': ['FFNN', 'FFNN with Embeddings', 'CNN', 'RNN', 'CNN+RNN'],\n",
        "                   'Accuracy': [str(round(scores_ffn[1]*100, 2)) + '%',\n",
        "                                str(round(scores_ffne[1]*100, 2)) + '%',\n",
        "                                str(round(scores_cnn[1]*100, 2)) + '%',\n",
        "                                str(round(scores_rnn[1]*100, 2)) + '%',\n",
        "                                str(round(scores_cnn_rnn[1]*100, 2)) + '%']})\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu5IEkTP0KSD",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The code above should output something similar to the following:\n",
        "\n",
        "```\n",
        "                            Model Accuracy\n",
        "          0                  FFNN   50.79%\n",
        "          1  FFNN with Embeddings   87.64%\n",
        "          2                   CNN   87.55%\n",
        "          3                   RNN   61.14%\n",
        "          4               CNN+RNN   82.62%\n",
        "```\n",
        "\n",
        "Surprisingly, the fully connected neural network with embeddings and convolutional neural network outperform the remaining networks. FFN is a very simple network with a single 250 dimensional dense layer.\n",
        "\n",
        "CNNs can be robust to the position and orientation of learned objects in the image, while the same principle can be used on sequences, such as the one-dimensional sequence of words in a movie review. A simple RNN seems not to be very competitive but together with a CNN it achieves a decent performance.\n",
        "\n",
        "Now, let's test the models with some custom examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPL1oXTE0KSE",
        "outputId": "8712d6b3-4440-4f94-d308-c0bc6af5ef6f",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 369ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 399ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f0f9b356ac0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 434ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "                                              review FFN FFNE CNN RNN CNN+RNN\n",
            "0                                          I like it   -    -   +   -       -\n",
            "1                                     I dont like it   -    -   -   -       -\n",
            "2  After 30 min I still did not know what the mov...   +    -   +   +       -\n",
            "3  It is so good that I will never ever watch it ...   -    -   +   +       -\n",
            "4                      It is like the Titanic movie!   +    +   +   +       +\n",
            "5           That is the best movie I have ever seen.   +    -   +   +       -\n",
            "6          That is the worst movie I have ever seen.   -    -   -   -       -\n"
          ]
        }
      ],
      "source": [
        "def movie_sentiment(reviews,\n",
        "                    models=[model_ffn, model_ffne, model_cnn, model_rnn, model_cnn_rnn],\n",
        "                    titles=['FFN', 'FFNE', 'CNN', 'RNN', 'CNN+RNN']):\n",
        "    df = pd.DataFrame(columns=['review'] + titles)\n",
        "    i = 0\n",
        "    for review in reviews:\n",
        "        words = set(text_to_word_sequence(review))\n",
        "        words = [word_index[w] for w in words]\n",
        "        words = sequence.pad_sequences([words], maxlen=max_review_length)\n",
        "        df.loc[i] = [review] + titles\n",
        "        df.loc[i, 'review'] = review\n",
        "        for j, model in enumerate(models):\n",
        "            proba = model.predict(words)\n",
        "            sentiment = '+' if proba>0.5 else '-'\n",
        "            df.loc[i, titles[j]] = sentiment\n",
        "        i = i + 1\n",
        "    return df\n",
        "\n",
        "text1 = 'I like it'\n",
        "text2 = 'I dont like it'\n",
        "text3 = 'After 30 min I still did not know what the movie is about'\n",
        "text4 = 'It is so good that I will never ever watch it again. Boring experience.'\n",
        "text5 = \"It is like the Titanic movie!\"\n",
        "text6 = \"That is the best movie I have ever seen.\"\n",
        "text7 = \"That is the worst movie I have ever seen.\"\n",
        "print(movie_sentiment([text1, text2, text3, text4, text5, text6, text7]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhMH-vxk0KSE",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "The test above should output something similar to the following:\n",
        "\n",
        "```\n",
        "                                              review FFN FFNE CNN RNN CNN+RNN  | EXPECTED\n",
        "0                                          I like it   -    +   +   +       -  |        +\n",
        "1                                     I dont like it   +    -   +   +       -  |        -\n",
        "2  After 30 min I still did not know what the mov...   +    +   +   -       -  |        -\n",
        "3  It is so good that I will never ever watch it ...   -    +   +   -       -  |        -\n",
        "4                      It is like the Titanic movie!   +    +   +   +       -  |        +\n",
        "5           That is the best movie I have ever seen.   +    +   +   +       -  |        +\n",
        "6          That is the worst movie I have ever seen.   -    -   -   -       -  |        -\n",
        "-----------------------------------------------------------------------------------------\n",
        "                                 CORRECT PREDICTIONS   4    5   4   6       4  |        7\n",
        "```\n",
        "\n",
        "It seems that RNN performed as the best on these few short examples.\n",
        "\n",
        "Try different architectures, models, hyperparameters (e.g. embedding dimensions), ... and you might improve the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjPeh9_80KSF",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "## References\n",
        "\n",
        "* The example above follows the post by Michel Kana, PhD: [Sentiment analysis: a practical benchmark](https://towardsdatascience.com/sentiment-analysis-a-benchmark-903279cab44a)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}