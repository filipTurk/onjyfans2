{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vp3XPuaTu9jl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# Text generation fundamentals\n",
    "\n",
    "<sup>This notebook is a part of Natural Language Processing class at the University of Ljubljana, Faculty for computer and information science. Please contact [ales.zagar@fri.uni-lj.si](mailto:ales.zagar@fri.uni-lj.si) for any comments.</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxLvv6UaPa33",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Introduction**\n",
    "\n",
    "In this notebook, we will give a tour of the currently most prominent decoding methods, mainly *Greedy search*, *Beam search*, *Top-K sampling* and *Top-p sampling*. We will mostly work with the `.generate()` method, documented [here](https://huggingface.co/docs/transformers/main_classes/text_generation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import a tokenizer and a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azagar/projects/envs/lab9/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-04 13:38:25.518427: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-04 13:38:25.538984: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-04 13:38:25.685974: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-04 13:38:25.830844: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743766705.945897  286190 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743766705.977362  286190 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743766706.212462  286190 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743766706.212526  286190 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743766706.212527  286190 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743766706.212529  286190 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-04 13:38:26.239504: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8Y7cgu9ohXP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Greedy Search**\n",
    "\n",
    "Greedy search simply selects the word with the highest probability as its next word: $w_t = argmax_{w}P(w | w_{1:t-1})$ at each timestep $t$. The following sketch shows greedy search. \n",
    "\n",
    "![Greedy Search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/greedy_search.png)\n",
    "\n",
    "Starting from the word $\\text{\"The\"}$, the algorithm \n",
    "greedily chooses the next word of highest probability $\\text{\"nice\"}$ and so on, so that the final generated word sequence is $\\text{\"The\", \"nice\", \"woman\"}$ having an overall probability of $0.5 \\times 0.4 = 0.2$.\n",
    "\n",
    "In the following we will generate word sequences using GPT2 on the context $(\\text{\"I\", \"enjoy\", \"walking\", \"with\", \"my\", \"cute\", \"dog\"})$. Let's see how greedy search can be used in `transformers` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWLd_J6lXz_t",
    "outputId": "ba8742ad-8a12-4c03-d920-c5e84670da22",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt')\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BBn1ePmJvhrl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The generated words following the context are reasonable, but the model quickly starts repeating itself! This is a very common problem in language generation in general and seems to be even more so in greedy and beam search - check out [Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424) and [Shao et al., 2017](https://arxiv.org/abs/1701.03185).\n",
    "\n",
    "The major drawback of greedy search though is that it misses high probability words hidden behind a low probability word as can be seen in our sketch above:\n",
    "\n",
    "The word $\\text{\"has\"}$ with its high conditional probability of $0.9$ is hidden behind the word $\\text{\"dog\"}$, which has only the second-highest conditional probability, so that greedy search misses the word sequence $\\text{\"The\"}, \\text{\"dog\"}, \\text{\"has\"}$.\n",
    "\n",
    "Thankfully, we have beam search to alleviate this problem!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8DnXZ1WiuNd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Beam search**\n",
    "\n",
    "Beam search reduces the risk of missing hidden high probability word sequences by keeping the most likely `num_beams` of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability. Let's illustrate with `num_beams=2`:\n",
    "\n",
    "![Beam search](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/beam_search.png)\n",
    "\n",
    "At time step $1$, besides the most likely hypothesis $\\text{\"The\", \"nice\"}$, beam search also keeps track of the second most likely one $\\text{\"The\", \"dog\"}$. At time step $2$, beam search finds that the word sequence $\\text{\"The\", \"dog\", \"has\"}$ has with $0.36$ a higher probability than $\\text{\"The\", \"nice\", \"woman\"}$, which has $0.2$. Great, it has found the most likely word sequence in our toy example! \n",
    "\n",
    "Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output. \n",
    "\n",
    "Let's see how beam search can be used in `transformers`. We set `num_beams > 1` and `early_stopping=True` so that generation is finished when all beam hypotheses reached the EOS token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1R5kx30Ynej",
    "outputId": "8d564c12-d572-4849-da82-e61d5a8c68a1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    input_ids,  \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZ6xs-KLi9jT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "While the result is arguably more fluent, the output still includes repetitions of the same word sequences.  \n",
    "A simple remedy is to introduce *n-grams* (*a.k.a* word sequences of $n$ words) penalties as introduced by [Paulus et al. (2017)](https://arxiv.org/abs/1705.04304) and [Klein et al. (2017)](https://arxiv.org/abs/1701.02810). The most common *n-grams* penalty makes sure that no *n-gram* appears twice by manually setting the probability of next words that could create an already seen *n-gram* to $0$.\n",
    "\n",
    "Let's try it out by setting `no_repeat_ngram_size=2` so that no *2-gram* appears twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jy3iVJgfnkMi",
    "outputId": "477e8054-4e8c-4fb2-f92d-77076a937bf3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxsksOGDpmA0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Nice, that looks much better! We can see that the repetition does not appear anymore. Nevertheless, *n-gram* penalties have to be used with care. An article generated about the city *New York* should not use a *2-gram* penalty or otherwise, the name of the city would only appear once in the whole text!\n",
    "\n",
    "Another important feature about beam search is that we can compare the top beams after generation and choose the generated beam that fits our purpose best. \n",
    "\n",
    "In `transformers`, we simply set the parameter `num_return_sequences` to the number of highest scoring beams that should be returned. Make sure though that `num_return_sequences <= num_beams`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ClO3VphqGp6",
    "outputId": "b5d8c609-761d-42c3-ff68-8e955a61537a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "1: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
      "2: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a break\n",
      "3: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with her again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to get back to\n",
      "4: I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I've been thinking about this for a while now, and I think it's time for me to take a step\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    num_return_sequences=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HhLKyfdbsjXc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As can be seen, the five beam hypotheses are only marginally different to each other - which should not be too surprising when using only 5 beams.\n",
    "\n",
    "In open-ended generation, a couple of reasons have recently been brought forward why beam search might not be the best possible option:\n",
    "\n",
    "- Beam search can work very well in tasks where the length of the desired generation is more or less predictable as in machine translation or summarization - see [Murray et al. (2018)](https://arxiv.org/abs/1808.10006) and [Yang et al. (2018)](https://arxiv.org/abs/1808.09582). But this is not the case for open-ended generation where the desired output length can vary greatly, e.g. dialog and story generation.\n",
    "\n",
    "- We have seen that beam search heavily suffers from repetitive generation. This is especially hard to control with *n-gram*- or other penalties in story generation since finding a good trade-off between forced \"no-repetition\" and repeating cycles of identical *n-grams* requires a lot of finetuning.\n",
    "\n",
    "- As argued in [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751), high quality human language does not follow a distribution of high probability next words. In other words, as humans, we want generated text to surprise us and not to be boring/predictable. The authors show this nicely by plotting the probability, a model would give to human text vs. what beam search does.\n",
    "\n",
    "![alt text](https://blog.fastforwardlabs.com/images/2019/05/Screen_Shot_2019_05_08_at_3_06_36_PM-1557342561886.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbbIyK84wHq6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Sampling**\n",
    "\n",
    "In its most basic form, sampling means randomly picking the next word $w_t$ according to its conditional probability distribution:\n",
    "\n",
    "$$w_t \\sim P(w|w_{1:t-1})$$\n",
    "\n",
    "Taking the example from above, the following graphic visualizes language generation when sampling.\n",
    "\n",
    "![vanilla_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/sampling_search.png)\n",
    "\n",
    "It becomes obvious that language generation using sampling is not *deterministic* anymore. The word \n",
    "$\\text{\"car\"}$ is sampled from the conditioned probability distribution $P(w | \\text{\"The\"})$, followed by sampling $\\text{\"drives\"}$ from $P(w | \\text{\"The\"}, \\text{\"car\"})$.\n",
    "\n",
    "In `transformers`, we set `do_sample=True` and deactivate *Top-K* sampling (more on this later) via `top_k=0`. In the following, we will fix `random_seed=0` for illustration purposes. Feel free to change the `random_seed` to play around with the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRAz4D-Ks0_4",
    "outputId": "f533dd96-c442-4d3b-8cbc-6e3fcdc539e3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but what I love about being a dog cat person is being a pet being with people who can treat you. I feel happy to be such a pet person and get to meet so many people. I don't think\n"
     ]
    }
   ],
   "source": [
    "# Set a random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQHuo911wfT-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Interesting! The text seems alright - but when taking a closer look, it is not very coherent and don't sound like they were written by a human. That is the big problem when sampling word sequences: The models often generate incoherent gibberish, *cf.* [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751).\n",
    "\n",
    "A trick is to make the distribution $P(w|w_{1:t-1})$ sharper (increasing the likelihood of high probability words and decreasing the likelihood of low probability words) by lowering the so-called `temperature` of the [softmax](https://en.wikipedia.org/wiki/Softmax_function#Smooth_arg_max). Check out the temperature demonstration on https://lukesalamone.github.io/posts/what-is-temperature/\n",
    "\n",
    "An illustration of applying temperature to our example from above could look as follows.\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/sampling_search_with_temp.png?raw=true)\n",
    "\n",
    "The conditional next word distribution of step $t=1$ becomes much sharper leaving almost no chance for word $\\text{\"car\"}$ to be selected.\n",
    "\n",
    "\n",
    "Let's see how we can cool down the distribution in the library by setting `temperature=0.7`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WgJredc-0j0Z",
    "outputId": "349d46ba-4196-4134-8c41-926ca48e42f5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but I also love people being nice. I love being in a party with a group of friends and having the opportunity to see people who are so different from me.\n",
      "\n",
      "I'm not sure if I'll be\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "import torch\n",
    "\n",
    "# Set a random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=0, \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kzGuu24hZZnq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "OK. There are less weird n-grams and the output is a bit more coherent now! While applying temperature can make a distribution less random, in its limit, when setting `temperature` $ \\to 0$, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "binNTroyzQBu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Top-K Sampling**\n",
    "\n",
    "[Fan et. al (2018)](https://arxiv.org/pdf/1805.04833.pdf) introduced a simple, but very powerful sampling scheme, called ***Top-K*** sampling. In *Top-K* sampling, the *K* most likely next words are filtered and the probability mass is redistributed among only those *K* next words. \n",
    "GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation. \n",
    "\n",
    "We extend the range of words used for both sampling steps in the example above from 3 words to 10 words to better illustrate *Top-K* sampling.\n",
    "\n",
    "![top_k_sampling](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/top_k_sampling.png)\n",
    "\n",
    "Having set $K = 6$, in both sampling steps we limit our sampling pool to 6 words. While the 6 most likely words, defined as $V_{\\text{top-K}}$ encompass only *ca.* two-thirds of the whole probability mass in the first step, it includes almost all of the probability mass in the second step. Nevertheless, we see that it successfully eliminates the rather weird candidates $\\text{\"not\", \"the\", \"small\", \"told\"}$ \n",
    "in the second sampling step.\n",
    "\n",
    "\n",
    "Let's see how *Top-K* can be used in the library by setting `top_k=10`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HBtDOdD0wx3l",
    "outputId": "c0604e0a-e20f-41ec-9fcc-21c0555c4ef0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but sometimes I get nervous when she is around. I don't like being alone in the house, so I usually just sit down and watch her play with my phone.\"\n",
      "\n",
      "A spokesman for the company said:\n"
     ]
    }
   ],
   "source": [
    "# Set a random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y77H5m4ZmhEX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One concern though with *Top-K* sampling is that it does not dynamically adapt the number of words that are filtered from the next word probability distribution $P(w|w_{1:t-1})$.\n",
    "This can be problematic as some words might be sampled from a very sharp distribution (distribution on the right in the graph above), whereas others from a much more flat distribution (distribution on the left in the graph above). \n",
    "\n",
    "In step $t=1$, *Top-K* eliminates the possibility to \n",
    "sample $\\text{\"people\", \"big\", \"house\", \"cat\"}$, which seem like reasonable candidates. On the other hand, in step $t=2$ the method includes the arguably ill-fitted words $\\text{\"down\", \"a\"}$ in the sample pool of words. Thus, limiting the sample pool to a fixed size *K* could endanger the model to produce gibberish for sharp distributions and limit the model's creativity for flat distribution.\n",
    "This intuition led [Ari Holtzman et al. (2019)](https://arxiv.org/abs/1904.09751) to create ***Top-p***- or ***nucleus***-sampling. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki9LAaexzV3H",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### **Top-p (nucleus) sampling**\n",
    "\n",
    "Instead of sampling only from the most likely *K* words, in *Top-p* sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability *p*. The probability mass is then redistributed among this set of words. This way, the size of the set of words (*a.k.a* the number of words in the set) can dynamically increase and decrease according to the next word's probability distribution. Ok, that was very wordy, let's visualize.\n",
    "\n",
    "![top_p_sampling](https://github.com/patrickvonplaten/scientific_images/blob/master/top_p_sampling.png?raw=true)\n",
    "\n",
    "Having set $p=0.92$, *Top-p* sampling picks the *minimum* number of words to exceed together $p=92\\%$ of the probability mass, defined as $V_{\\text{top-p}}$. In the first example, this included the 9 most likely words, whereas it only has to pick the top 3 words in the second example to exceed 92%. Quite simple actually! It can be seen that it keeps a wide range of words where the next word is arguably less predictable, *e.g.* $P(w | \\text{\"The\"})$, and only a few words when the next word seems more predictable, *e.g.* $P(w | \\text{\"The\", \"car\"})$.\n",
    "\n",
    "Alright, time to check it out in `transformers`!\n",
    "We activate *Top-p* sampling by setting `0 < top_p < 1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EvwIc7YAx77F",
    "outputId": "ac9339ee-6624-42ed-e1a4-62b37980e071",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog but what I love about being a dog cat person is being a pet being with people who can treat you. I feel happy to be such a pet person and get to meet so many people. I don't think\n"
     ]
    }
   ],
   "source": [
    "# Set a random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.92, \n",
    "    top_k=0  # We need to deactivate top_k if we want to use only top_p!\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tn-8gLaR4lat",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "While in theory, *Top-p* seems more elegant than *Top-K*, both methods work  well in practice. *Top-p* can also be used in combination with *Top-K*, which can avoid very low ranked words while allowing for some dynamic selection.\n",
    "\n",
    "Finally, to get multiple independently sampled outputs, we can *again* set the parameter `num_return_sequences > 1`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kY8P9VG8Gi9",
    "outputId": "1cdfc0d1-c606-4c80-fbed-d8344520015c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: I enjoy walking with my cute dog but sometimes I get nervous when she is around. I've been told that with her alone, she will usually wander off and then try to chase me. It's nice to know that I have this control over her\n",
      "1: I enjoy walking with my cute dog. I think she is the same one I like to walk with my dog, I think she is about as girly as my first dog. I hope we can find an apartment for her when we come back.\n",
      "2: I enjoy walking with my cute dog, but there's so much to say about him that I am going to miss it all. He has been so supportive and even had my number in his bag.\n",
      "\n",
      "I hope I can say the following to\n"
     ]
    }
   ],
   "source": [
    "# Set a random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find below a list of the most popular decoding methods and how to use them using `.generate()` method.\n",
    "\n",
    "- **Greedy Decoding**: Invoked by calling `_greedy_search()` when `num_beams=1` and `do_sample=False`. This method selects the word with the highest probability at each step.\n",
    "\n",
    "- **Contrastive Search**: Triggered by `_contrastive_search()` if `penalty_alpha>0` and `top_k>1`. This technique modifies the selection process to diversify the output.\n",
    "\n",
    "- **Multinomial Sampling**: Utilizes `_sample()` when `num_beams=1` and `do_sample=True`, introducing randomness by sampling from the distribution of possible next words.\n",
    "\n",
    "- **Beam-Search Decoding**: Engaged through `_beam_search()` when `num_beams>1` and `do_sample=False`, considering multiple best paths simultaneously to enhance coherency.\n",
    "\n",
    "- **Beam-Search Multinomial Sampling**: Invoked by `_beam_sample()` if `num_beams>1` and `do_sample=True`, combining beam search's thoroughness with the randomness of sampling.\n",
    "\n",
    "- **Diverse Beam-Search Decoding**: Employed via `_group_beam_search()`, when `num_beams>1` and `num_beam_groups>1`, to ensure variety among the considered paths.\n",
    "\n",
    "- **Constrained Beam-Search Decoding**: Activated by `_constrained_beam_search()`, if `constraints!=None` or `force_words_ids!=None`, imposing specific constraints on the search process.\n",
    "\n",
    "- **Assisted Decoding**: Utilized by `_assisted_decoding()`, if `assistant_model` or `prompt_lookup_num_tokens` is passed to `.generate()`, leveraging external models or prompts to guide the generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity\n",
    "Perplexity is a metric commonly used to evaluate the performance of language models in natural language processing (NLP). It measures how well a language model predicts a sample and is a function of the probability that the model assigns to a sequence of words. In essence, perplexity quantifies the uncertainty of a language model in predicting (or \"guessing\") the next token in a sequence.\n",
    "\n",
    "### Definition\n",
    "The perplexity (P) of a language model for a given sequence of words $W = w_1, w_2, ..., w_N$ is defined as the exponential of the entropy (or cross-entropy if comparing against a true distribution) of the sequence, given by:\n",
    "\n",
    "$$ P(W) = 2^{H(W)} $$\n",
    "\n",
    "where $H(W)$ is the entropy of the sequence $W$, calculated as:\n",
    "\n",
    "$$ H(W) = -\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 p(w_i | w_{i-1}, ..., w_1) $$\n",
    "\n",
    "Here, $p(w_i | w_{i-1}, ..., w_1)$ represents the conditional probability of word $w_i$ given all the preceding words $w_1, ..., w_{i-1}$, as predicted by the language model, and $N$ is the length of the sequence.\n",
    "\n",
    "### Interpretation\n",
    "- **Lower Perplexity:** Indicates a better-performing model, as it means the model has less uncertainty in predicting the next word in a sequence. A lower perplexity score suggests that the language model is more confident and accurate in its predictions.\n",
    "- **Higher Perplexity:** Suggests a model with greater uncertainty and less ability to predict the next word accurately. High perplexity indicates that the model's predictions are more spread out over a larger set of possible words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.106895 I am a very hard worker\n",
      "5751.819 worker hard very a am I\n",
      "187.38794 I like ice cream\n",
      "779.6354 I like icecream\n",
      "809.5182 I like driving cars\n",
      "185511.42 I lik driving cars\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "texts = ['I am a very hard worker', \n",
    "         'worker hard very a am I', \n",
    "         'I like ice cream', \n",
    "         'I like icecream', \n",
    "         'I like driving cars', \n",
    "         'I lik driving cars']  # List of texts to process\n",
    "\n",
    "for t in texts:  \n",
    "    encodings = tokenizer(t, return_tensors=\"pt\")  \n",
    "    \n",
    "    max_length = model.config.n_positions  # Maximum sequence length the model can handle\n",
    "    stride = 512  # The step size for breaking the text into overlapping segments if needed\n",
    "    seq_len = encodings.input_ids.size(1)  # The length of the tokenized sequence\n",
    "    \n",
    "    nlls = []  # A list to store the negative log-likelihoods for each segment\n",
    "    prev_end_loc = 0  # Variable to keep track of the end location of the previous segment\n",
    "    \n",
    "    for begin_loc in range(0, seq_len, stride):  # Loop over the sequence in strides to handle long texts\n",
    "        end_loc = min(begin_loc + max_length, seq_len)  # Determine the end location of the current segment\n",
    "        trg_len = end_loc - prev_end_loc  # The target length may differ from stride in the last loop\n",
    "        \n",
    "        input_ids = encodings.input_ids[:, begin_loc:end_loc]  # Extract the input IDs for the current segment\n",
    "        target_ids = input_ids.clone()  # Clone input_ids to create target_ids for calculating loss\n",
    "        target_ids[:, :-trg_len] = -100  # Mask tokens not part of the target length with -100 (ignored by loss function)\n",
    "    \n",
    "        with torch.no_grad():  # Disable gradient calculations\n",
    "            outputs = model(input_ids, labels=target_ids)  # Forward pass through the model with masked target IDs\n",
    "    \n",
    "            # Loss is calculated using CrossEntropyLoss which averages over valid (non-masked) labels\n",
    "            # Note: The model calculates loss over trg_len - 1 labels due to an internal shift to the left by 1\n",
    "            neg_log_likelihood = outputs.loss  # Extract the negative log-likelihood from the model outputs\n",
    "    \n",
    "        nlls.append(neg_log_likelihood)  # Append the NLL of the current segment to the list\n",
    "    \n",
    "        prev_end_loc = end_loc  # Update the end location for the next iteration\n",
    "        if end_loc == seq_len:  # Break the loop if the end of the sequence is reached\n",
    "            break\n",
    "    \n",
    "    ppl = torch.exp(torch.stack(nlls).mean())  # Calculate perplexity as the exponential of the average NLL\n",
    "    print(ppl.numpy(), t)  # Print the calculated perplexity value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the perplexity score for the inversed order of words is much higher. Also, sequences with misspelled words are penalized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "Parts of this notebook were taken from [notebook](https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/02_how_to_generate.ipynb)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lab9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
