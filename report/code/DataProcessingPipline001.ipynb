{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1820c43",
   "metadata": {},
   "source": [
    "**Processing Pipeline 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "751fca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1af976b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pl.read_csv(\"data/PP_ALL.csv\", encoding=\"utf-8\")\n",
    "output_df = pl.read_csv(\"data/Joined_rtf_files.csv\", encoding=\"utf-8\")\n",
    "input_df = input_df.with_columns(pl.col(\"Datum\").str.strptime(pl.Datetime, \"%m/%d/%Y %H:%M\"))\n",
    "output_df = output_df.with_columns(pl.col(\"Datum\").str.strptime(pl.Datetime, \"%m/%d/%Y %H:%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b2448c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target row datetime: 2022-04-26 20:00:00\n",
      "\n",
      "Last row before target time: shape: (5,)\n",
      "Series: 'Datum' [datetime[μs]]\n",
      "[\n",
      "\t2022-04-26 19:42:00\n",
      "\t2022-04-26 19:48:00\n",
      "\t2022-04-26 19:53:00\n",
      "\t2022-04-26 20:12:00\n",
      "\t2022-04-26 20:43:00\n",
      "]\n",
      "\n",
      "Test inputs:\n",
      "   LegacyId               Datum                  Operater    A1  \\\n",
      "0   1318759 2022-04-26 19:42:00     DARS Luka Loboda, PIC  NULL   \n",
      "1   1318760 2022-04-26 19:48:00     DARS Luka Loboda, PIC  NULL   \n",
      "2   1318761 2022-04-26 19:53:00     DARS Luka Loboda, PIC  NULL   \n",
      "3   1318762 2022-04-26 20:12:00   DARS Janko Poženel, GNC  NULL   \n",
      "4   1318763 2022-04-26 20:43:00   DARS Janko Poženel, GNC  NULL   \n",
      "\n",
      "                                                  B1    C1    A2    B2  \\\n",
      "0  <p><strong>Nesreče</strong></p><p>Na ljubljans...  NULL  None  None   \n",
      "1  <p><strong>Nesreče</strong></p><p>Na ljubljans...  NULL  None  None   \n",
      "2  <p><strong>Nesreče</strong></p><p>Na ljubljans...  NULL  None  None   \n",
      "3  <p><strong>Opozorila</strong></p><p>V sredo, 2...  NULL  None  None   \n",
      "4  <p><strong>Opozorila</strong></p><p>V sredo, 2...  NULL  None  None   \n",
      "\n",
      "                                                  C2 TitlePomembnoSLO  ...  \\\n",
      "0  <div><p>Buy vignette for Slovenia online</p><p...             None  ...   \n",
      "1  <div><p>Buy vignette for Slovenia online</p><p...             None  ...   \n",
      "2  <div><p>Buy vignette for Slovenia online</p><p...             None  ...   \n",
      "3  <div><p>Buy vignette for Slovenia online</p><p...             None  ...   \n",
      "4  <div><p>Buy vignette for Slovenia online</p><p...             None  ...   \n",
      "\n",
      "  TitleOvireSLO                                    ContentOvireSLO  \\\n",
      "0          None  <p>Na ljubljanski vzhodni obvoznici je zaradi ...   \n",
      "1          None  <p>Predor Karavanke je trenutno zaprt proti Sl...   \n",
      "2          None                                               None   \n",
      "3          None                                               None   \n",
      "4          None                                               None   \n",
      "\n",
      "  TitleDeloNaCestiSLO ContentDeloNaCestiSLO TitleOpozorilaSLO  \\\n",
      "0                None                  None              None   \n",
      "1                None                  None              None   \n",
      "2                None                  None              None   \n",
      "3                None                  None              None   \n",
      "4                None                  None              None   \n",
      "\n",
      "                                 ContentOpozorilaSLO  \\\n",
      "0  <p>V sredo, 27. 4., bo med 8. in 22. uro, zara...   \n",
      "1  <p>V sredo, 27. 4., bo med 8. in 22. uro, zara...   \n",
      "2  <p>V sredo, 27. 4., bo med 8. in 22. uro, zara...   \n",
      "3  <p>V sredo, 27. 4., bo med 8. in 22. uro, zara...   \n",
      "4  <p>V sredo, 27. 4., bo med 8. in 22. uro, zara...   \n",
      "\n",
      "  TitleMednarodneInformacijeSLO ContentMednarodneInformacijeSLO  \\\n",
      "0                          None                            None   \n",
      "1                          None                            None   \n",
      "2                          None                            None   \n",
      "3                          None                            None   \n",
      "4                          None                            None   \n",
      "\n",
      "  TitleSplosnoSLO ContentSplosnoSLO  \n",
      "0      Prireditve              None  \n",
      "1      Prireditve              None  \n",
      "2      Prireditve              None  \n",
      "3      Prireditve              None  \n",
      "4      Prireditve              None  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "\n",
      "Test output:\n",
      "(datetime.datetime(2022, 4, 26, 20, 0), 'TMP-102.rtf', 'April 2022', 'Na južni ljubljanski obvoznici je zaradi pokvarjenega vozila  zaprt izvoz Ljubljana - jug proti dolenjski avtocesti.', 'Zaradi del je na severni ljubljanski obvoznici zaprt uvoz Nove Jarše proti štajerski avtocesti.', 'Na cesti Rogatec - Dobovec poteka promet zaradi prometne nesreče izmenično enosmerno.', 'Na Obrežju je povečana promet osebnih vozil pri vstopu v državo, v Jelšanah pa pri izstopu iz nje.', '\\x00')\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "def get_first_inputs_before_time(n, output_df_row, input_df, time_window_after_minutes=60):\n",
    "\n",
    "    #'04/30/2022 18:30'\n",
    "    target_datetime = dict(zip(output_df.columns, output_df_row))[\"Datum\"]\n",
    "    start_time = target_datetime.replace(second=0, microsecond=0) - timedelta(minutes=time_window_after_minutes)\n",
    "    end_time = start_time + 2 * timedelta(minutes=time_window_after_minutes)\n",
    "    \n",
    "    filtered = input_df.filter(\n",
    "        (pl.col(\"Datum\") >= start_time) & (pl.col(\"Datum\") <= end_time)\n",
    "    )\n",
    "\n",
    "    def print_all_rows():\n",
    "        for i in range(len(filtered)):\n",
    "            print(filtered.row(i))\n",
    "\n",
    "    if filtered.height > 0:\n",
    "       #print_all_rows()\n",
    "       return filtered.sort(\"Datum\").tail(n)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "row_n = 4\n",
    "test = get_first_inputs_before_time(5, output_df.row(row_n), input_df)\n",
    "print(\"Target row datetime:\", dict(zip(output_df.columns, output_df.row(row_n)))[\"Datum\"])\n",
    "print(\"\\nLast row before target time:\", dict(zip(input_df.columns, test))[\"Datum\"])\n",
    "\n",
    "test_inputs = test.to_pandas()\n",
    "test_output = output_df.row(row_n)\n",
    "print(\"\\nTest inputs:\")\n",
    "print(test_inputs)\n",
    "print(\"\\nTest output:\")\n",
    "print(test_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "80af783d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target row datetime: 2022-04-26 20:00:00\n",
      "\n",
      "Last row before target time: shape: (1,)\n",
      "Series: 'Datum' [datetime[μs]]\n",
      "[\n",
      "\t2022-04-26 19:53:00\n",
      "]\n",
      "\n",
      "Test inputs:\n",
      "   LegacyId               Datum               Operater    A1  \\\n",
      "0   1318761 2022-04-26 19:53:00  DARS Luka Loboda, PIC  NULL   \n",
      "\n",
      "                                                  B1    C1    A2    B2  \\\n",
      "0  <p><strong>Nesreče</strong></p><p>Na ljubljans...  NULL  None  None   \n",
      "\n",
      "                                                  C2 TitlePomembnoSLO  ...  \\\n",
      "0  <div><p>Buy vignette for Slovenia online</p><p...             None  ...   \n",
      "\n",
      "  TitleOvireSLO ContentOvireSLO TitleDeloNaCestiSLO ContentDeloNaCestiSLO  \\\n",
      "0          None            None                None                  None   \n",
      "\n",
      "  TitleOpozorilaSLO                                ContentOpozorilaSLO  \\\n",
      "0              None  <p>V sredo, 27. 4., bo med 8. in 22. uro, zara...   \n",
      "\n",
      "  TitleMednarodneInformacijeSLO ContentMednarodneInformacijeSLO  \\\n",
      "0                          None                            None   \n",
      "\n",
      "  TitleSplosnoSLO ContentSplosnoSLO  \n",
      "0      Prireditve              None  \n",
      "\n",
      "[1 rows x 27 columns]\n",
      "\n",
      "Test output:\n",
      "(datetime.datetime(2022, 4, 26, 20, 0), 'TMP-102.rtf', 'April 2022', 'Na južni ljubljanski obvoznici je zaradi pokvarjenega vozila  zaprt izvoz Ljubljana - jug proti dolenjski avtocesti.', 'Zaradi del je na severni ljubljanski obvoznici zaprt uvoz Nove Jarše proti štajerski avtocesti.', 'Na cesti Rogatec - Dobovec poteka promet zaradi prometne nesreče izmenično enosmerno.', 'Na Obrežju je povečana promet osebnih vozil pri vstopu v državo, v Jelšanah pa pri izstopu iz nje.', '\\x00')\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "def get_first_input_before_time(n, output_df_row, input_df, time_window_after_minutes=0):\n",
    "\n",
    "    #'04/30/2022 18:30'\n",
    "    target_datetime = dict(zip(output_df.columns, output_df_row))[\"Datum\"]\n",
    "    start_time = target_datetime.replace(second=0, microsecond=0) - timedelta(minutes=time_window_after_minutes)\n",
    "    end_time = start_time + 2 * timedelta(minutes=time_window_after_minutes)\n",
    "    \n",
    "    filtered = input_df.filter(\n",
    "        (pl.col(\"Datum\") <= start_time)\n",
    "    )\n",
    "\n",
    "    def print_all_rows():\n",
    "        for i in range(len(filtered)):\n",
    "            print(filtered.row(i))\n",
    "\n",
    "    if filtered.height > 0:\n",
    "       #print_all_rows()\n",
    "       return filtered.sort(\"Datum\").tail(n)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "row_n = 4\n",
    "test = get_first_input_before_time(1, output_df.row(row_n), input_df)\n",
    "print(\"Target row datetime:\", dict(zip(output_df.columns, output_df.row(row_n)))[\"Datum\"])\n",
    "print(\"\\nLast row before target time:\", dict(zip(input_df.columns, test))[\"Datum\"])\n",
    "\n",
    "test_inputs = test.to_pandas()\n",
    "test_output = output_df.row(row_n)\n",
    "print(\"\\nTest inputs:\")\n",
    "print(test_inputs)\n",
    "print(\"\\nTest output:\")\n",
    "print(test_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e719fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned text:\n"
     ]
    }
   ],
   "source": [
    "from html.parser import HTMLParser\n",
    "text_example = \"\"\"<p><strong>Vreme</strong></p><p>Ponekod po Sloveniji megla v pasovih zmanjšuje vidljivost. Prilagodite hitrost!</p><p><strong>Omejitve za tovorna vozila</strong></p><p>Po Sloveniji velja med prazniki omejitev za tovorna vozila z največjo dovoljeno maso nad 7,5 ton:<br>- v soboto, 1. 1., od 8. do 22. ure;<br>- v nedeljo, 2. 1., od 8. do 22. ure.</p><p>Od 30. decembra je v veljavi sprememba omejitve za tovorna vozila nad 7,5 ton. <a href=\"https://www.promet.si/sl/splosne-omejitve\" target=\"_blank\">Več.</a></p><p><strong>Dela</strong></p><p>Na primorski avtocesti je ponovno odprt priključek <strong>Črni</strong><strong> Kal </strong>v obe smeri.</p>\"\"\"\n",
    "\n",
    "class HTMLTextExtractor(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_parts = []\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        self.text_parts.append(data)\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag in {\"p\", \"br\", \"div\", \"li\"}:\n",
    "            self.text_parts.append(\" \")\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        if tag in {\"p\", \"br\", \"div\", \"li\"}:\n",
    "            self.text_parts.append(\" \")\n",
    "\n",
    "    def get_text(self):\n",
    "        # Join and normalize whitespace\n",
    "        return ' '.join(''.join(self.text_parts).split())\n",
    "\n",
    "def strip_html(html):\n",
    "    parser = HTMLTextExtractor()\n",
    "    parser.feed(html)\n",
    "    return parser.get_text()\n",
    "\n",
    "# Example usage\n",
    "cleaned_text = strip_html(text_example)\n",
    "print(\"\\nCleaned text:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "216660f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag counts:\n",
      "<p>: 1983381\n",
      "<strong>: 1533299\n",
      "<br>: 611357\n",
      "<a>: 217990\n",
      "<u>: 16373\n",
      "<em>: 326\n",
      "<ul>: 69\n",
      "<li>: 84\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "input_df = input_df\n",
    "# html is in column \"B1\"\n",
    "html = \"\"\"\n",
    "<p><strong>Vreme</strong></p><p>Ponekod po Sloveniji megla v pasovih zmanjšuje vidljivost. Prilagodite hitrost!</p><p><strong>Omejitve za tovorna vozila</strong></p><p>Po Sloveniji velja med prazniki omejitev za tovorna vozila z največjo dovoljeno maso nad 7,5 ton:<br>- v soboto, 1. 1., od 8. do 22. ure;<br>- v nedeljo, 2. 1., od 8. do 22. ure.</p><p>Od 30. decembra je v veljavi sprememba omejitve za tovorna vozila nad 7,5 ton. <a href=\"https://www.promet.si/sl/splosne-omejitve\" target=\"_blank\">Več.</a></p><p><strong>Dela</strong></p><p>Na primorski avtocesti je ponovno odprt priključek <strong>Črni</strong><strong> Kal </strong>v obe smeri.</p>\n",
    "\"\"\"\n",
    "\n",
    "# Counter to collect tag frequencies\n",
    "tag_counter = Counter()\n",
    "\n",
    "# Loop through each HTML string\n",
    "for html in input_df[\"B1\"]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in soup.find_all(True):\n",
    "        tag_counter[tag.name] += 1  # Count each tag once per appearance\n",
    "\n",
    "# Print result\n",
    "print(\"Tag counts:\")\n",
    "for tag, count in tag_counter.items():\n",
    "    print(f\"<{tag}>: {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd50bc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count contents of <strong> tags\n",
    "strong_text_counter = Counter()\n",
    "\n",
    "for html in input_df[\"B1\"]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in soup.find_all(\"strong\"):\n",
    "        text = tag.get_text(strip=True)\n",
    "        if text:\n",
    "            strong_text_counter[text] += 1\n",
    "\n",
    "# Use most_common() to sort by count descending\n",
    "sorted_counts = strong_text_counter.most_common()\n",
    "\n",
    "# Print sorted results\n",
    "print(\"Counts of <strong> tag contents (sorted):\")\n",
    "for text, count in sorted_counts:\n",
    "    print(f\"{text}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9490c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "html = \"\"\"\n",
    "<p><strong>Vreme</strong></p><p>Ponekod po Sloveniji megla v pasovih zmanjšuje vidljivost. Prilagodite hitrost!</p><p><strong>Omejitve za tovorna vozila</strong></p><p>Po Sloveniji velja med prazniki omejitev za tovorna vozila z največjo dovoljeno maso nad 7,5 ton:<br>- v soboto, 1. 1., od 8. do 22. ure;<br>- v nedeljo, 2. 1., od 8. do 22. ure.</p><p>Od 30. decembra je v veljavi sprememba omejitve za tovorna vozila nad 7,5 ton. <a href=\"https://www.promet.si/sl/splosne-omejitve\" target=\"_blank\">Več.</a></p><p><strong>Dela</strong></p><p>Na primorski avtocesti je ponovno odprt priključek <strong>Črni</strong><strong> Kal </strong>v obe smeri.</p>\n",
    "\"\"\"\n",
    "\n",
    "json_format = {\n",
    "    \n",
    "    \"Delo na cesti:\": \"\",\n",
    "    \"Zastoji:\": \"\",\n",
    "    \"Ovire:\": \"\",\n",
    "    \"Nesreče:\": \"\",\n",
    "    \"Vreme:\": \"\",\n",
    "    \"Prireditve:\": \"\",\n",
    "    \"Opozorila:\": \"\",\n",
    "    \"Mejni prehodi:\": \"\",\n",
    "    \"Zimske razmere:\":\"\",\n",
    "    \"Dela:\":\"\",\n",
    "    \"Popolne zapore:\":\"\",\n",
    "    \"leftover_html\": \"\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "63ba3781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random HTML content from DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p><strong>Nesreče</strong></p><p>Na ljubljanski vzhodni obvoznici je zaprt počasni pas pred priključkom Industrijska Cona Moste proti Zadobrovi. Nastal je zastoj.</p><p><strong>Zastoji</strong></p><p>Promet je povečan na cestah proti večjim mestom. Zastoji nastajajo med drugim na mariborski vzhodni obvoznici na območju razcepa Dragučova, na štajerski avtocesti iz smeri Trojan ter na dolenjski avtocesti iz smeri Grosuplja proti Ljubljani.</p><p>Na štajerski avtocesti med Blagovico in Trojanami proti Mariboru, občasno zapirajo predor Podmilj.</p><p>Na dolenjski avtocesti pred delovno zaporo med Malencami in Šmarjem Sapom proti Novemu mestu.</p><p><strong>Ovire</strong></p><p>Okvara tovornega vozila na primorski avtocesti pred priključkom Brezovica proti Ljubljani.</p><p>Okvara vozila na bivši hitri cesti skozi Maribor pred izvozom Maribor center proti Mariboru.</p><p><strong>Opozorila</strong></p><p>Cesta čez <strong>prelaz Vršič </strong>je prevozna samo za osebna vozila.</p><p><strong>Delo na cesti</strong></p><p><strong>Glavna cesta Slovenska Bistrica - Ptuj je ponovno odprta.</strong></p><p><strong>Popolne zapore:</strong><br><strong>- Na dolenjski avtocesti </strong>bo do 5. 12. zaprt priključek Dobruška vas, izvoz iz smeri Obrežja in uvoz proti Ljubljani.<br><strong>- Na gorenjski avtocesti </strong>bo do 2. decembra zaprt priključek Radovljica, izvoz iz smeri Jesenic in uvoz proti Ljubljani. Zaprto je tudi počivališče.<br>- <strong>Na štajerski avtocesti </strong>bosta predvidoma do 28. novembra zaprta izvoz in uvoz Maribor sever proti Šentilju.<br>- Do 2. 12. bo zaprta cesta <strong>Žuniči - Vinica</strong>, med Preloko in Balkovci. </p><p><strong>Napovedane zapore cest:</strong><br><strong>- </strong>Na<strong> </strong><strong>dolenjski</strong><strong> avtocesti </strong>bo od četrtka, 24. 11., do 5. decembra <strong>zaprt priključek Dobruška vas</strong> <strong>proti Ljubljani</strong>.<br>- Od sobote, 26. 11. od 6. ure, do nedelje 27. 11. do 10. ure, bo <strong>zaprta primorska avtocesta </strong>med <strong>razcepom </strong><strong>Srmin</strong><strong> </strong>in <strong>priključkom Črni Kal proti Ljubljani</strong>. <br>- Cesta <strong>Podpeč - Ig </strong>bo zaprta od sobote, 26. 11. od 7. ure, do nedelje, 27. 11. do 21. ure.<br>- Cesta <strong>Žirovnica - Begunje </strong>bo v Žirovnici zaprta v nedeljo, 27. 11., med 7. in 17. uro.</p><p><a href=\"https://www.promet.si/sl/aktualna-napoved\" target=\"_blank\">Več o delovnih zaporah, prometna napoved.</a></p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Delo na cesti\": \"Glavna cesta Slovenska Bistrica - Ptuj je ponovno odprta.\",\n",
      "  \"Dela\": \"\",\n",
      "  \"Delovne zapore\": \"\",\n",
      "  \"Popolne zapore\": \"- Na dolenjski avtocesti bo do 5. 12. zaprt priključek Dobruška vas, izvoz iz smeri Obrežja in uvoz proti Ljubljani. - Na gorenjski avtocesti bo do 2. decembra zaprt priključek Radovljica, izvoz iz smeri Jesenic in uvoz proti Ljubljani. Zaprto je tudi počivališče. - Na štajerski avtocesti bosta predvidoma do 28. novembra zaprta izvoz in uvoz Maribor sever proti Šentilju. - Do 2. 12. bo zaprta cesta Žuniči - Vinica , med Preloko in Balkovci. Napovedane zapore cest: - Na dolenjski avtocesti bo od četrtka, 24. 11., do 5. decembra zaprt priključek Dobruška vas proti Ljubljani . - Od sobote, 26. 11. od 6. ure, do nedelje 27. 11. do 10. ure, bo zaprta primorska avtocesta med razcepom Srmin in priključkom Črni Kal proti Ljubljani . - Cesta Podpeč - Ig bo zaprta od sobote, 26. 11. od 7. ure, do nedelje, 27. 11. do 21. ure. - Cesta Žirovnica - Begunje bo v Žirovnici zaprta v nedeljo, 27. 11., med 7. in 17. uro. Več o delovnih zaporah, prometna napoved. Več o delovnih zaporah, prometna napoved.\",\n",
      "  \"Zastoji\": \"Promet je povečan na cestah proti večjim mestom. Zastoji nastajajo med drugim na mariborski vzhodni obvoznici na območju razcepa Dragučova, na štajerski avtocesti iz smeri Trojan ter na dolenjski avtocesti iz smeri Grosuplja proti Ljubljani. Na štajerski avtocesti med Blagovico in Trojanami proti Mariboru, občasno zapirajo predor Podmilj. Na dolenjski avtocesti pred delovno zaporo med Malencami in Šmarjem Sapom proti Novemu mestu.\",\n",
      "  \"Ovire\": \"Okvara tovornega vozila na primorski avtocesti pred priključkom Brezovica proti Ljubljani. Okvara vozila na bivši hitri cesti skozi Maribor pred izvozom Maribor center proti Mariboru.\",\n",
      "  \"Nesreče\": \"Na ljubljanski vzhodni obvoznici je zaprt počasni pas pred priključkom Industrijska Cona Moste proti Zadobrovi. Nastal je zastoj.\",\n",
      "  \"Vreme\": \"\",\n",
      "  \"Opozorila\": \"Cesta čez prelaz Vršič je prevozna samo za osebna vozila.\",\n",
      "  \"Zimske razmere\": \"\",\n",
      "  \"Prireditve\": \"\",\n",
      "  \"Mejni prehodi\": \"\",\n",
      "  \"Omejitve za tovorna vozila\": \"\",\n",
      "  \"leftover_html\": \"\",\n",
      "  \"timestamp\": \"24-11-2022 07:24:00\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Template\n",
    "json_format = {\n",
    "    \"Delo na cesti\": \"\",\n",
    "    \"Dela\": \"\",\n",
    "    \"Delovne zapore\": \"\",\n",
    "    \"Popolne zapore\": \"\",\n",
    "    \"Zastoji\": \"\",\n",
    "    \"Ovire\": \"\",\n",
    "    \"Nesreče\": \"\",\n",
    "    \"Vreme\": \"\",\n",
    "    \"Opozorila\": \"\",\n",
    "    \"Zimske razmere\": \"\",\n",
    "    \"Prireditve\": \"\",\n",
    "    \"Mejni prehodi\": \"\",\n",
    "    \"Omejitve za tovorna vozila\": \"\",\n",
    "    \"leftover_html\": \"\"\n",
    "}\n",
    "\n",
    "def parse_html_into_json(html, json_format, timestamp=None):\n",
    "    # Normalize keys for matching\n",
    "    normalized_keys = {\n",
    "        re.sub(r\":+\", \"\", k).strip().lower(): k\n",
    "        for k in json_format if k != \"leftover_html\"\n",
    "    }\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    elements = soup.find_all([\"p\", \"br\", \"a\"])\n",
    "\n",
    "    current_heading = None\n",
    "    content_map = {k: \"\" for k in json_format if k != \"leftover_html\"}\n",
    "    leftover = []\n",
    "\n",
    "    for tag in elements:\n",
    "        text = tag.get_text(separator=\" \", strip=True)\n",
    "\n",
    "        # Skip empty elements\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # Check if tag starts with a known section heading\n",
    "        for normalized_heading, real_key in normalized_keys.items():\n",
    "            if text.lower().startswith(normalized_heading):\n",
    "                current_heading = real_key\n",
    "                # Remove the heading from the text itself\n",
    "                cleaned = re.sub(f\"^{re.escape(normalized_heading)}:?\\s*\", \"\", text, flags=re.I)\n",
    "                if cleaned:\n",
    "                    content_map[real_key] += cleaned + \" \"\n",
    "                break\n",
    "        else:\n",
    "            if current_heading:\n",
    "                content_map[current_heading] += text + \" \"\n",
    "            else:\n",
    "                leftover.append(text)\n",
    "\n",
    "    # Strip trailing whitespace\n",
    "    for key in content_map:\n",
    "        content_map[key] = content_map[key].strip()\n",
    "\n",
    "    json_format.update(content_map)\n",
    "    json_format[\"leftover_html\"] = \"\\n\".join(leftover).strip()\n",
    "    if timestamp:\n",
    "        json_format[\"timestamp\"] = timestamp.strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "    return json_format\n",
    "\n",
    "    \n",
    "\n",
    "# Call the function to parse the HTML and fill the json_format\n",
    "row = input_df.select([\"B1\", \"Datum\"]).sample(1).row(0)\n",
    "random_html, timestamp = row\n",
    "\n",
    "print(\"Random HTML content from DataFrame:\")\n",
    "display(HTML(random_html))\n",
    "\n",
    "myjson=parse_html_into_json(random_html, json_format, timestamp=timestamp)\n",
    "print(json.dumps(myjson, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ebfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns = [\"Datum\", \"A1\", \"B1\", \"C1\",\"ContentPomembnoSLO\", \"ContentNesreceSLO\" , \"ContentNesreceSLO\", \"ContentVremeSLO\", \"ContentOvireSLO \", \"ContentDeloNaCestiSLO\", \"ContentOpozorilaSLO\"]\n",
    "semantic_compare = {\n",
    "  \"Delo na cesti\": \"ContentDeloNaCestiSLO\",\n",
    "  \"Zastoji\": \"\",\n",
    "  \"Ovire\": \"ContentOvireSLO\",\n",
    "  \"Nesreče\": \"ContentNesreceSLO\",\n",
    "  \"Vreme\": \"ContentVremeSLO\",\n",
    "  \"Prireditve\": \"\",\n",
    "  \"Opozorila\": \"ContentOpozorilaSLO\",\n",
    "  \"Mejni prehodi\": \"\",\n",
    "  \"Delovne zapore\": \"\",\n",
    "  \"Zimske razmere\": \"\",\n",
    "  \"Dela\": \"\",\n",
    "  \"Popolne zapore\": \"\",\n",
    "  \"Omejitve za tovorna vozila\": \"\",\n",
    "  \"leftover_html\": \"ContentPomembnoSLO\"\n",
    "}\n",
    "# Call the function to parse the HTML and fill the json_format\n",
    "#random_html = input_df[\"B1\"].sample(1).item()  # Randomly sample one HTML entry from the DataFrame\n",
    "#print(\"Random HTML content from DataFrame:\")\n",
    "#display(HTML(random_html))\n",
    "\n",
    "#myjson=parse_html_into_json(random_html, json_format)\n",
    "#print(json.dumps(myjson, indent=2, ensure_ascii=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5b2536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Semantic Comparison ===\n",
      "❗️ Low similarity for key: Delo na cesti (ratio: 0.78)\n",
      "🧠 Delo na cesti → ContentDeloNaCestiSLO\n",
      "✅ Similarity: 0.78\n",
      "📤 Extracted:\n",
      "Več o delovnih zaporah v prometni napovedi . Več o delovnih zaporah v prometni napovedi Na južni ljubljanski obvoznici bo zaradi rekonstrukcije zaprt priključek Ljubljana Vič v obe smeri od srede,16. 8., do 25. 8. 2023. Od nedelje do četrtka, 31. 8., bo na severni ljubljanski obvoznici zaprt uvoz iz...\n",
      "📥 Actual:\n",
      "<p><a href=\"https://www.promet.si/sl/aktualna-napoved\" target=\"_blank\">Več o delovnih zaporah v prometni napovedi</a>.</p><p><strong>Na južni ljubljanski obvoznici </strong>bo zaradi rekonstrukcije zaprt priključek Ljubljana Vič v obe smeri od srede,16. 8., do 25. 8. 2023.</p><p>Od nedelje do četrtk...\n",
      "--------------------------------------------------------------------------------\n",
      "🧠 Ovire → ContentOvireSLO\n",
      "✅ Similarity: 0.96\n",
      "📤 Extracted:\n",
      "Na gorenjski avtocesti je pred galerijo Moste proti Karavankam zaprt vozni pas, okvara vozila. Na primorski avtocesti je pred Senožečami proti Ljubljani zaprt počasni pas, okvara vozila.\n",
      "📥 Actual:\n",
      "<p>Na gorenjski avtocesti je pred galerijo Moste proti Karavankam zaprt vozni pas, okvara vozila.</p><p>Na primorski avtocesti je pred Senožečami proti Ljubljani zaprt počasni pas, okvara vozila.</p>\n",
      "--------------------------------------------------------------------------------\n",
      "🧠 Nesreče → ContentNesreceSLO\n",
      "✅ Similarity: 0.95\n",
      "📤 Extracted:\n",
      "Na primorski avtocesti je pred Brezovico proti Ljubljani oviran promet. Nastaja zastoj. Na cesti Bovec - Vršič je pri Soči promet urejen izmenično enosmerno.\n",
      "📥 Actual:\n",
      "<p>Na primorski avtocesti je pred Brezovico proti Ljubljani oviran promet. Nastaja zastoj.</p><p>Na cesti Bovec - Vršič je pri Soči promet urejen izmenično enosmerno.</p>\n",
      "--------------------------------------------------------------------------------\n",
      "❗️ Low similarity for key: Vreme (ratio: 0.00)\n",
      "🧠 Vreme → ContentVremeSLO\n",
      "✅ Similarity: 0.00\n",
      "📤 Extracted:\n",
      "\n",
      "📥 Actual:\n",
      "<p><strong>Popolne zapore nekaterih cest zaradi poškodovanih vozišč</strong><strong>:</strong><br><strong></strong>- Dravograd - Slovenj Gradec, v Otiškem Vrhu, Poljana - Mežica - Črna na Koroškem.<br>- Dravograd - Libeliče, pri Tribeju.<br>- Poljana - Šentvid, Koprivna - Črna, Šoštanj - Črna, pri Č...\n",
      "--------------------------------------------------------------------------------\n",
      "❗️ Low similarity for key: Opozorila (ratio: 0.00)\n",
      "🧠 Opozorila → ContentOpozorilaSLO\n",
      "✅ Similarity: 0.00\n",
      "📤 Extracted:\n",
      "\n",
      "📥 Actual:\n",
      "<p>Do 15 ure bo zaprta cesta Ilirska Bistrica - Grda Draga.<br>Med 16. in 17. uro bodo zaprte ceste v Dutovljah.<br>Med 15. in 18.30 bo polovična zapora ceste Nova Gorica -Tolmin v Kanalu.<br></p>\n",
      "--------------------------------------------------------------------------------\n",
      "❗️ Low similarity for key: leftover_html (ratio: 0.00)\n",
      "🧠 leftover_html → ContentPomembnoSLO\n",
      "✅ Similarity: 0.00\n",
      "📤 Extracted:\n",
      "\n",
      "📥 Actual:\n",
      "<p>Zaradi posledic ujme je omejen dostop nepooblaščenim osebam na nekatera območja. Točke z omejenim dostopom so:<br>- na <strong>Pavličevem sedlu </strong>(za Solčavo in Luče), <br>- na cesti <strong>Šentvid - Šoštanj, v Šoštanju pri letališču Lajše </strong>(za koroško regijo) <br> - na cesti <str...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a.strip(), b.strip()).ratio()\n",
    "\n",
    "# Sample the row (you already sampled the HTML)\n",
    "row = input_df.sample(1)\n",
    "random_html = row[\"B1\"].item()\n",
    "parsed = parse_html_into_json(random_html, json_format.copy())  # use copy to avoid overwriting\n",
    "\n",
    "print(\"=== Semantic Comparison ===\")\n",
    "for key, column in semantic_compare.items():\n",
    "    if not column:\n",
    "        continue  # skip if no mapped column\n",
    "\n",
    "    parsed_value = (parsed.get(key) or \"\").strip()\n",
    "    actual_value = (row[column].item() if column in row else \"\") or \"\"\n",
    "    actual_value = actual_value.strip()\n",
    "\n",
    "    match_ratio = similarity(parsed_value, actual_value)\n",
    "    \n",
    "    if match_ratio < 0.8:\n",
    "        print(f\"❗️ Low similarity for key: {key} (ratio: {match_ratio:.2f})\")\n",
    "\n",
    "    print(f\"🧠 {key} → {column}\")\n",
    "    print(f\"✅ Similarity: {match_ratio:.2f}\")\n",
    "    print(f\"📤 Extracted:\\n{parsed_value[:300]}{'...' if len(parsed_value) > 300 else ''}\")\n",
    "    print(f\"📥 Actual:\\n{actual_value[:300]}{'...' if len(actual_value) > 300 else ''}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d0d7d",
   "metadata": {},
   "source": [
    "We now have a json structure for the data. now we need to compare the json input and the gennerated output to see if they match.\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2cb0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load a pre-trained sentence embedding model\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def get_similarity_score(output_message, json_string):\n",
    "    # Compute embeddings\n",
    "    embeddings = model.encode([output_message, json_string], convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity_score = util.cos_sim(embeddings[0], embeddings[1])\n",
    "    \n",
    "    return similarity_score.item()\n",
    "\n",
    "\n",
    "def parse_output_messages(output_row, columns):\n",
    "    row_dict = dict(zip(columns, output_row))\n",
    "    message = \"\"\n",
    "    for column in [\"content_01\", \"content_02\", \"content_03\", \"content_04\", \"content_05\"]:\n",
    "        if row_dict.get(column) is not None:\n",
    "            if column == \"content_05\":\n",
    "                if '\\x00' == row_dict[column]:\n",
    "                    continue\n",
    "            message += f\"{row_dict[column]} \"\n",
    "    return message.strip()\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def compare_json_to_output(input_df, output_df, n_examples=1, n_samples=5):\n",
    "    traffic_examples = pd.DataFrame(columns=[\"Output Message\", \"Input_Message\", \"Input JSON\", \"Datum\", \"Similarity Score\"])\n",
    "    print(f\"Comparing {n_examples} examples from output_df with input_df...\")\n",
    "\n",
    "    for i in tqdm(range(n_examples), desc=\"Examples\"):  # outer loop progress bar\n",
    "        try:\n",
    "            traffic_examples_specific = pd.DataFrame(columns=[\"Output Message\", \"Input_Message\", \"Input JSON\", \"Datum\", \"Similarity Score\"])\n",
    "            timestamps = []\n",
    "            similarity_scores = []\n",
    "            timestamp_scores_dict = {}\n",
    "            output_df_row = output_df.sample(1).row(0)  # Randomly sample one row from output_df\n",
    "            input_rows = get_first_inputs_before_time(n_samples, output_df_row, input_df)\n",
    "\n",
    "            # Inner loop over input rows with tqdm progress bar\n",
    "            for _, input_row in tqdm(input_rows.to_pandas().iterrows(), total=len(input_rows), desc=f\"Samples for example {i+1}\", leave=False):\n",
    "                try:\n",
    "                \n",
    "                    timestamps.append(input_row[\"Datum\"])\n",
    "                    html = input_row[\"B1\"]\n",
    "                    if isinstance(html, pl.Series):  # if it's a Polars Series, get the first item\n",
    "                        html = html[0]\n",
    "                    html = str(html) \n",
    "                    myjson = parse_html_into_json(html, json_format)\n",
    "                    output = parse_output_messages(output_df_row, output_df.columns)\n",
    "                    \n",
    "                    full_string = \"\"\n",
    "                    for key, value in myjson.items():\n",
    "                        if value != \"\":\n",
    "                            full_string += f\"{value} \"\n",
    "                            \n",
    "                    similarity_score = get_similarity_score(output, full_string)\n",
    "                    # print(f\"Similarity score: {similarity_score:.4f}\")\n",
    "                    similarity_scores.append(similarity_score)\n",
    "                    timestamp_scores_dict[input_row[\"Datum\"]] = similarity_score\n",
    "                    \n",
    "                    new_row = {\n",
    "                        \"Output Message\": output,\n",
    "                        \"Input_Message\": full_string,\n",
    "                        \"Input JSON\": json.dumps(myjson, indent=2, ensure_ascii=False),\n",
    "                        \"Datum\": input_row[\"Datum\"],\n",
    "                        \"Similarity Score\": similarity_score\n",
    "                    }\n",
    "\n",
    "                    new_df = pd.DataFrame([new_row])\n",
    "                    traffic_examples_specific = pd.concat([traffic_examples_specific, new_df], ignore_index=True)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing input row: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # print(f\"Average similarity score for example {i+1}: {sum(similarity_scores) / len(similarity_scores):.4f}\")\n",
    "            # print(similarity_scores)\n",
    "            \n",
    "            output_timestamp = output_df_row[0]  # or 0 — depends on your schema\n",
    "            \n",
    "            # print(\"*\" * 80)\n",
    "            # print(f\"Output timestamp: {output_timestamp}\")\n",
    "\n",
    "            converted_dict = {str(k): v for k, v in timestamp_scores_dict.items()}\n",
    "            # print(json.dumps(converted_dict, indent=2, ensure_ascii=False))\n",
    "            \n",
    "            best_example = traffic_examples_specific.loc[traffic_examples_specific[\"Similarity Score\"].idxmax()]\n",
    "            # print(f\"Best example for output row {i+1}:\", best_example[\"Similarity Score\"])\n",
    "            traffic_examples = pd.concat([traffic_examples, best_example.to_frame().T], ignore_index=True)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating average similarity score for example {i+1}: {e}\")\n",
    "            continue\n",
    "        \n",
    "    return traffic_examples\n",
    "\n",
    "traffic_examples = compare_json_to_output(input_df, output_df, n_examples=len(output_df), n_samples=15)\n",
    "# print(\"\\nTraffic Examples DataFrame:\")\n",
    "# print(traffic_examples.head())\n",
    "\n",
    "traffic_examples.to_csv(\"trainingdataset3.csv\", index=False, encoding=\"utf-8\")\n",
    "print(\"\\nTraffic examples saved to 'trainingdataset3.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "7b88b8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 100 output examples with 20 input samples each...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  21%|██        | 21/100 [00:05<00:14,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-07-23 18:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  78%|███████▊  | 78/100 [00:21<00:05,  3.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[214]\u001b[39m\u001b[32m, line 117\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Assuming input_df and output_df are already loaded as Polars DataFrames\u001b[39;00m\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# And json_format is defined\u001b[39;00m\n\u001b[32m    115\u001b[39m \u001b[38;5;66;03m#sample the outputdf  \u001b[39;00m\n\u001b[32m    116\u001b[39m output_df_sample = output_df.sample(n=\u001b[32m100\u001b[39m)  \u001b[38;5;66;03m# Sample 15 rows for testing\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m traffic_examples = \u001b[43mcompare_json_to_output_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_df_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m traffic_examples.to_csv(\u001b[33m\"\u001b[39m\u001b[33mtrainingdataset_optimized.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# print(\"\\nOptimized traffic examples saved to 'trainingdataset_optimized.csv'.\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[214]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mcompare_json_to_output_optimized\u001b[39m\u001b[34m(input_df, output_df, n_samples)\u001b[39m\n\u001b[32m     69\u001b[39m     processed_inputs.append({\n\u001b[32m     70\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInput_Message\u001b[39m\u001b[33m\"\u001b[39m: full_string,\n\u001b[32m     71\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInput_JSON_str\u001b[39m\u001b[33m\"\u001b[39m: json.dumps(myjson, indent=\u001b[32m2\u001b[39m, ensure_ascii=\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     72\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDatum\u001b[39m\u001b[33m\"\u001b[39m: input_row[\u001b[33m\"\u001b[39m\u001b[33mDatum\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     73\u001b[39m     })\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# --- 4. BATCH encode all input texts at once ---\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m input_embeddings = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_texts_to_encode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m \u001b[38;5;66;03m# --- 5. Compute all similarities in a single, fast operation ---\u001b[39;00m\n\u001b[32m     79\u001b[39m cosine_scores = util.cos_sim(output_embedding, input_embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sentence_transformers\\SentenceTransformer.py:685\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    682\u001b[39m features.update(extra_features)\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    687\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sentence_transformers\\SentenceTransformer.py:758\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    756\u001b[39m     module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m    757\u001b[39m     module_kwargs = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m trans_features = {\n\u001b[32m    437\u001b[39m     key: value\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    440\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    444\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1016\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1014\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1029\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\bert\\modeling_bert.py:662\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    651\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    652\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    653\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    659\u001b[39m         output_attentions,\n\u001b[32m    660\u001b[39m     )\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    541\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    542\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    549\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    550\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    551\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    561\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\bert\\modeling_bert.py:482\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    473\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    474\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    480\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    481\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    492\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\models\\bert\\modeling_bert.py:362\u001b[39m, in \u001b[36mBertSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().forward(\n\u001b[32m    351\u001b[39m         hidden_states,\n\u001b[32m    352\u001b[39m         attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    357\u001b[39m         output_attentions,\n\u001b[32m    358\u001b[39m     )\n\u001b[32m    360\u001b[39m bsz, tgt_len, _ = hidden_states.size()\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m query_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001b[39;00m\n\u001b[32m    365\u001b[39m \u001b[38;5;66;03m# mask needs to be such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[32m    366\u001b[39m is_cross_attention = encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#optimizzed version\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Assume these functions are defined elsewhere in your script\n",
    "# from your_utils import parse_html_into_json, get_first_inputs_before_time\n",
    "\n",
    "# It's good practice to load the model once\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "\n",
    "def parse_output_messages(output_row, columns):\n",
    "    # This function is fine as is, but we'll call it more efficiently\n",
    "    # Assuming output_row is a tuple or list from iter_rows()\n",
    "    row_dict = dict(zip(columns, output_row))\n",
    "    message_parts = []\n",
    "    for i in range(1, 6): # A slightly cleaner way to loop through content_01 to content_05\n",
    "        col_name = f\"content_0{i}\"\n",
    "        content = row_dict.get(col_name)\n",
    "        if content and content != '\\x00':\n",
    "            message_parts.append(str(content))\n",
    "    return \" \".join(message_parts)\n",
    "\n",
    "\n",
    "def compare_json_to_output_optimized(input_df, output_df, n_samples=15):\n",
    "    \"\"\"\n",
    "    Optimized function to find the best input match for each output message.\n",
    "    \"\"\"\n",
    "    best_results_list = []\n",
    "    output_columns = output_df.columns\n",
    "\n",
    "    print(f\"Comparing {len(output_df)} output examples with {n_samples} input samples each...\")\n",
    "\n",
    "    # Iterate directly over output_df rows. This is much cleaner than sampling in a loop.\n",
    "    for output_row_tuple in tqdm(output_df.iter_rows(), total=len(output_df), desc=\"Processing Output Examples\"):\n",
    "\n",
    "        try:\n",
    "            # --- 1. Process the output ONCE per outer loop ---\n",
    "            output_message = parse_output_messages(output_row_tuple, output_columns)\n",
    "            if not output_message: # Skip if the output message is empty\n",
    "                continue\n",
    "            \n",
    "            # Encode the single output message\n",
    "            output_embedding = model.encode(output_message, convert_to_tensor=True)\n",
    "\n",
    "            # --- 2. Get candidate inputs ---\n",
    "            input_rows_df = get_first_inputs_before_time(n_samples, output_row_tuple, input_df).to_pandas()\n",
    "            if input_rows_df.empty:\n",
    "                continue\n",
    "\n",
    "            # --- 3. Prepare all input texts for BATCH processing ---\n",
    "            input_texts_to_encode = []\n",
    "            processed_inputs = [] # Keep track of the full parsed data\n",
    "\n",
    "            for _, input_row in input_rows_df.iterrows():\n",
    "                html = str(input_row.get(\"B1\", \"\"))\n",
    "                \n",
    "                # Assume parse_html_into_json is defined and returns a dict\n",
    "                myjson = parse_html_into_json(html, json_format) \n",
    "                \n",
    "                full_string = \" \".join(value for value in myjson.values() if value)\n",
    "                input_texts_to_encode.append(full_string)\n",
    "                processed_inputs.append({\n",
    "                    \"Input_Message\": full_string,\n",
    "                    \"Input_JSON_str\": json.dumps(myjson, indent=2, ensure_ascii=False),\n",
    "                    \"Datum\": input_row[\"Datum\"]\n",
    "                })\n",
    "\n",
    "            # --- 4. BATCH encode all input texts at once ---\n",
    "            input_embeddings = model.encode(input_texts_to_encode, convert_to_tensor=True)\n",
    "            \n",
    "            # --- 5. Compute all similarities in a single, fast operation ---\n",
    "            cosine_scores = util.cos_sim(output_embedding, input_embeddings)\n",
    "            # print(f\"Cosine scores: {cosine_scores}\")\n",
    "            \n",
    "            # Find the index of the best score\n",
    "            best_sample_idx = cosine_scores[0].argmax().item()\n",
    "\n",
    "            # --- 6. Select the best result ---\n",
    "            best_input_data = processed_inputs[best_sample_idx]\n",
    "            best_score = cosine_scores[0][best_sample_idx].item()\n",
    "            # print(f\"Best score for output row {output_row_tuple[0]}: {best_score:.4f}\")\n",
    "            \n",
    "            # Append the single best result for this output to our list\n",
    "            best_results_list.append({\n",
    "                \"Output Message\": output_message,\n",
    "                \"Input_Message\": best_input_data[\"Input_Message\"],\n",
    "                \"Input JSON\": best_input_data[\"Input_JSON_str\"],\n",
    "                \"Datum\": best_input_data[\"Datum\"],\n",
    "                \"Similarity Score\": best_score\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            # It's good to know which row caused an error\n",
    "            output_timestamp = output_row_tuple[0] \n",
    "            print(f\"Error processing output row for timestamp {output_timestamp}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # --- 7. Create the final DataFrame ONCE from the list of results ---\n",
    "    if not best_results_list:\n",
    "        print(\"No valid examples were processed.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    final_df = pd.DataFrame(best_results_list)\n",
    "    return final_df\n",
    "\n",
    "# Assuming input_df and output_df are already loaded as Polars DataFrames\n",
    "# And json_format is defined\n",
    "#sample the outputdf  \n",
    "output_df_sample = output_df.sample(n=100)  # Sample 15 rows for testing\n",
    "traffic_examples = compare_json_to_output_optimized(input_df, output_df_sample, n_samples=20)\n",
    "traffic_examples.to_csv(\"trainingdataset_optimized.csv\", index=False, encoding=\"utf-8\")\n",
    "# print(\"\\nOptimized traffic examples saved to 'trainingdataset_optimized.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914a452",
   "metadata": {},
   "source": [
    "Now we hhave a system that checks the output of the model against the concated json structure. And we can comare which specific innput matches the output the most. We will match the output to the input by comparing the output to the input and finding the closest match. and recording the similarity score and json structure along with the input and output in the training dataframe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1adc27e",
   "metadata": {},
   "source": [
    "B1  conntains tags like  vreme nesreče ovire.... whic is also i separate columns. Now tis information is often duplicated and i need to join it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ea4cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test inputs:\n",
      "LegacyId                                                                     1318757\n",
      "Datum                                                            2022-04-26 19:33:00\n",
      "Operater                                                       DARS Luka Loboda, PIC\n",
      "A1                                                                              NULL\n",
      "B1                                 <p><strong>Nesreče</strong></p><p>Na ljubljans...\n",
      "C1                                                                              NULL\n",
      "A2                                                                              None\n",
      "B2                                                                              None\n",
      "C2                                 <div><p>Buy vignette for Slovenia online</p><p...\n",
      "TitlePomembnoSLO                                                                None\n",
      "ContentPomembnoSLO                                                              None\n",
      "TitleNesreceSLO                                                                 None\n",
      "ContentNesreceSLO                  <p>Na ljubljanski severni obvoznici zaradi pro...\n",
      "TitleZastojiSLO                                                                 None\n",
      "ContentZastojiSLO                                                               None\n",
      "TitleVremeSLO                                                                   None\n",
      "ContentVremeSLO                                                                 None\n",
      "TitleOvireSLO                                                                   None\n",
      "ContentOvireSLO                    <p>Na štajerski avtocesti je zaradi živali zap...\n",
      "TitleDeloNaCestiSLO                                                             None\n",
      "ContentDeloNaCestiSLO                                                           None\n",
      "TitleOpozorilaSLO                                                               None\n",
      "ContentOpozorilaSLO                <p>V sredo, 27. 4., bo med 8. in 22. uro, zara...\n",
      "TitleMednarodneInformacijeSLO                                                   None\n",
      "ContentMednarodneInformacijeSLO                                                 None\n",
      "TitleSplosnoSLO                                                           Prireditve\n",
      "ContentSplosnoSLO                                                               None\n",
      "Name: 0, dtype: object\n",
      "Start tag: p, attrs: []\n",
      "Start tag: p, attrs: []\n",
      "Start tag: p, attrs: []\n",
      "Start tag: p, attrs: []\n",
      "Start tag: p, attrs: []\n",
      "Start tag: p, attrs: []\n",
      "Start tag: p, attrs: []\n",
      "Start tag: p, attrs: []\n",
      "Start tag: p, attrs: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Datum: 2022-04-26 19:33:00 B1: Nesreče Na ljubljanski severni obvoznici zaradi prometne nesreče med krožiščem Tomačevo in Bežigradom promet proti Kosezam poteka samo po enem prometnem pasu. Ovire Na štajerski avtocesti je zaradi živali zaprt prehitevalni pas med Šentjakobom in Zadobrovo proti Ljubljani. Opozorila V sredo, 27. 4., bo med 8. in 22. uro, zaradi praznika veljala splošna omejitev prometa tovornih vozil, katerih največja dovoljena masa presega 7,5 t. ContentNesreceSLO: Na ljubljanski severni obvoznici zaradi prometne nesreče med krožiščem Tomačevo in Bežigradom promet proti Kosezam poteka samo po enem prometnem pasu. ContentNesreceSLO: Na ljubljanski severni obvoznici zaradi prometne nesreče med krožiščem Tomačevo in Bežigradom promet proti Kosezam poteka samo po enem prometnem pasu. ContentOpozorilaSLO: V sredo, 27. 4., bo med 8. in 22. uro, zaradi praznika veljala splošna omejitev prometa tovornih vozil, katerih največja dovoljena masa presega 7,5 t.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTest inputs:\")\n",
    "print(test_inputs.iloc[0])\n",
    "\n",
    "def parse_input_message(input_row, columns):\n",
    "    row_dict = dict(zip(columns, input_row))\n",
    "    message = \"\"\n",
    "    for column in [\"Datum\", \"A1\", \"B1\", \"C1\",\"ContentPomembnoSLO\" , \"ContentNesreceSLO\", \"ContentVremeSLO\", \"ContentOvireSLO \", \"ContentDeloNaCestiSLO\", \"ContentOpozorilaSLO\"]:\n",
    "        value = row_dict.get(column)\n",
    "        if value is not None and value != \"NULL\":\n",
    "            if isinstance(value, datetime):\n",
    "                clean_value = value.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            elif isinstance(value, str):\n",
    "                clean_value = strip_html(value)\n",
    "                #new line \n",
    "            message += f\"{column}: {clean_value} \"\n",
    "    #print(\"Input message:\", message.strip())\n",
    "    return message.strip()\n",
    "parse_input_message(test_inputs.iloc[0], test_inputs.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "acadd90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test output:\n",
      "(datetime.datetime(2022, 4, 26, 20, 0), 'TMP-102.rtf', 'April 2022', 'Na južni ljubljanski obvoznici je zaradi pokvarjenega vozila  zaprt izvoz Ljubljana - jug proti dolenjski avtocesti.', 'Zaradi del je na severni ljubljanski obvoznici zaprt uvoz Nove Jarše proti štajerski avtocesti.', 'Na cesti Rogatec - Dobovec poteka promet zaradi prometne nesreče izmenično enosmerno.', 'Na Obrežju je povečana promet osebnih vozil pri vstopu v državo, v Jelšanah pa pri izstopu iz nje.', '\\x00')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Na južni ljubljanski obvoznici je zaradi pokvarjenega vozila  zaprt izvoz Ljubljana - jug proti dolenjski avtocesti. Zaradi del je na severni ljubljanski obvoznici zaprt uvoz Nove Jarše proti štajerski avtocesti. Na cesti Rogatec - Dobovec poteka promet zaradi prometne nesreče izmenično enosmerno. Na Obrežju je povečana promet osebnih vozil pri vstopu v državo, v Jelšanah pa pri izstopu iz nje.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nTest output:\")\n",
    "print(test_output)\n",
    "def parse_output_messages(output_row, columns):\n",
    "    row_dict = dict(zip(columns, output_row))\n",
    "    message = \"\"\n",
    "    for column in [\"content_01\", \"content_02\", \"content_03\", \"content_04\", \"content_05\"]:\n",
    "        if row_dict.get(column) is not None:\n",
    "            if column == \"content_05\":\n",
    "                if '\\x00' == row_dict[column]:\n",
    "                    continue\n",
    "            message += f\"{row_dict[column]} \"\n",
    "    return message.strip()\n",
    "\n",
    "\n",
    "parse_output_messages(test_output, output_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45f4cf3",
   "metadata": {},
   "source": [
    "Okay now we have a way to smeantically check the output of the model against the input json structure. But we still need a parser that will check if events match. I believe that semantic similarity is not enough inn this case. We need to check if the events match. also maybe just count the number of events that match and dont match. Create a parser thhat will be able to identify 80% of the events that match and 20% that dont match. I dont need to go into to much detail here, just create a parser that will be able to identify the events that match and the ones that dont match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19845672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Parsing Input Text ---\n",
      "Na gorenjski avtocesti bo promet skozi predor Karavanke potekal izmenično enosmerno danes in jutri med 8. in 16. uro. Na podravski avtocesti med Podlehnikom in Gruškovjem v obe smeri bo v četrtek, 9. 3. med 12. in 12.30, popolna zapora zaradi minerskih del v kamnolomu Zakl. Na vipavski hitri cesti je zaprt priključek Šempeter v obe smeri. Cesta Litija - Zagorje bo zaprta pri Šklendrovcu vsako soboto med 7:30 in 13:30 do septembra 2023. Več o delovnih zaporah v prometni napovedi. Več o delovnih zaporah v prometni napovedi. Cesta čez prelaz Vršič je prevozna samo za osebna vozila z verigami.\n",
      "--- Parsing Output Text ---\n",
      "Na vseh vpadnicah v Ljubljano je oviran promet zaradi nedelujočih semaforjev. Nastajajo zastoji. Na štajerski avtocesti proti Mariboru je obtičal pokvarjen tovornjak na izvozu Trojane. Zaradi del poteka promet skozi predor Karavanke izmenično enosmerno.\n",
      "\n",
      "\n",
      "Found 2/2 events\n",
      "-------------------------\n",
      "Event #1:\n",
      "{\n",
      "  \"event_type\": \"Delo na cesti\",\n",
      "  \"location_description\": \"\",\n",
      "  \"delay_minutes\": null,\n",
      "  \"detour_available\": null,\n",
      "  \"source_text\": \"Več o delovnih zaporah v prometni napovedi\"\n",
      "}\n",
      "-------------------------\n",
      "Event #2:\n",
      "{\n",
      "  \"event_type\": \"Delo na cesti\",\n",
      "  \"location_description\": \"\",\n",
      "  \"delay_minutes\": null,\n",
      "  \"detour_available\": null,\n",
      "  \"source_text\": \"Več o delovnih zaporah v prometni napovedi\"\n",
      "}\n",
      "-------------------------\n",
      "Event #1:\n",
      "{\n",
      "  \"event_type\": \"Ovira\",\n",
      "  \"location_description\": \"Na vseh vpadnicah v Ljubljano je oviran promet zaradi nedelujočih semaforjev\",\n",
      "  \"delay_minutes\": null,\n",
      "  \"detour_available\": null,\n",
      "  \"source_text\": \"Na vseh vpadnicah v Ljubljano je oviran promet zaradi nedelujočih semaforjev\"\n",
      "}\n",
      "Event #2:\n",
      "{\n",
      "  \"event_type\": \"Zastoj\",\n",
      "  \"location_description\": \"Nastajajo zastoji\",\n",
      "  \"delay_minutes\": null,\n",
      "  \"detour_available\": null,\n",
      "  \"source_text\": \"Nastajajo zastoji\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from enum import Enum\n",
    "from typing import List, Optional, Dict, Any\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "json_thingy = {\n",
    "  \"Delo na cesti\": \"ContentDeloNaCestiSLO\",\n",
    "  \"Zastoji\": \"\",\n",
    "  \"Ovire\": \"ContentOvireSLO\",\n",
    "  \"Nesreče\": \"ContentNesreceSLO\",\n",
    "  \"Vreme\": \"ContentVremeSLO\",\n",
    "  \"Prireditve\": \"\",\n",
    "  \"Opozorila\": \"ContentOpozorilaSLO\",\n",
    "  \"Mejni prehodi\": \"\",\n",
    "  \"Delovne zapore\": \"\",\n",
    "  \"Zimske razmere\": \"\",\n",
    "  \"Dela\": \"\",\n",
    "  \"Popolne zapore\": \"\",\n",
    "  \"Omejitve za tovorna vozila\": \"\",\n",
    "  \"leftover_html\": \"ContentPomembnoSLO\"\n",
    "}\n",
    "\n",
    "class EventType(str, Enum):\n",
    "    ROADWORK = \"Delo na cesti\"                      \n",
    "    CONGESTION = \"Zastoji\"                   \n",
    "    OBSTACLE = \"Ovire\"                       \n",
    "    ACCIDENT = \"Nesreče\"                    \n",
    "    WEATHER = \"Vreme\"\n",
    "    EVENT = \"Prireditve\"                     \n",
    "    WARNING = \"Opozorila\"                    \n",
    "    BORDER_CROSSING = \"Mejni prehodi\"         \n",
    "    ROAD_CLOSURE = \"Delovne zapore\"         \n",
    "    WINTER_CONDITIONS = \"Zimske razmere\"       \n",
    "    WORK = \"Dela\"                            \n",
    "    FULL_CLOSURE = \"Popolne zapore\"          \n",
    "    TRUCK_RESTRICTION = \"Omejitve za tovorna vozila\"   \n",
    "    UNKNOWN = \"unknown\"                         \n",
    "\n",
    "class ParsedTrafficEvent(BaseModel):\n",
    "    \"\"\"\n",
    "    A simplified, practical model representing a single parsed traffic event.\n",
    "    \"\"\"\n",
    "    event_type: EventType = Field(..., description=\"The classified type of the event.\")\n",
    "    location_description: str = Field(..., description=\"A description of the road or area affected.\")\n",
    "    \n",
    "    delay_minutes: Optional[int] = Field(None, description=\"Estimated delay in minutes.\")\n",
    "    detour_available: Optional[bool] = Field(None, description=\"Indicates if a detour is mentioned.\")\n",
    "    \n",
    "    source_text: str = Field(..., description=\"The original text segment this event was parsed from.\")\n",
    "    parse_timestamp: datetime = Field(default_factory=datetime.now)\n",
    "\n",
    "# --- 2. Keyword and Regex Definitions (The \"Brain\" of the Parser) ---\n",
    "\n",
    "# Map keywords to an EventType. Order matters: more specific keywords should come first.\n",
    "# We use word stems to catch variations (e.g., \"dela\", \"delovnih\").\n",
    "EVENT_TYPE_KEYWORDS: Dict[EventType, List[str]] = {\n",
    "    EventType.TRUCK_RESTRICTION: [\"omejitev prometa tovornih\", \"tovorna vozila\"],\n",
    "    EventType.ACCIDENT: [\"nesreč\", \"nesr\"],\n",
    "    EventType.ROADWORK: [\"del na cesti\", \"delovna zapora\", \"delovnih zaporah\", \"montažni most\"],\n",
    "    EventType.CONGESTION: [\"zastoj\", \"zamuda\", \"promet upočasnjen\", \"kolona\", \"zgostitev prometa\"],\n",
    "    EventType.OBSTACLE: [\"ovira\", \"predmet na cesti\", \"razsut tovor\", \"žival\"],\n",
    "    EventType.WEATHER: [\"vremensk\", \"burja\", \"sneg\", \"poledic\", \"megla\", \"vrem\", \"piha\"],\n",
    "}\n",
    "\n",
    "def _find_event_type(text: str) -> EventType:\n",
    "    \"\"\"Finds the event type based on keywords in the text.\"\"\"\n",
    "    lower_text = text.lower()\n",
    "    for event_type, keywords in EVENT_TYPE_KEYWORDS.items():\n",
    "        if any(keyword in lower_text for keyword in keywords):\n",
    "            return event_type\n",
    "    return EventType.UNKNOWN\n",
    "\n",
    "def _extract_delay(text: str) -> Optional[int]:\n",
    "    \"\"\"Extracts delay time in minutes. Averages ranges.\"\"\"\n",
    "    # Matches patterns like \"zamuda 5 - 10 minut\", \"zamuda 10 minut\"\n",
    "    match = re.search(r\"zamuda\\s*(\\d+)(?:\\s*-\\s*(\\d+))?\\s*minut\", text.lower())\n",
    "    if match:\n",
    "        start_delay = int(match.group(1))\n",
    "        end_delay = int(match.group(2)) if match.group(2) else start_delay\n",
    "        return (start_delay + end_delay) // 2\n",
    "    return None\n",
    "\n",
    "def _extract_location(text: str) -> str:\n",
    "    \"\"\"A simple location extractor. It cleans up some common clutter.\"\"\"\n",
    "    # This can be improved, but for an 80/20 solution, we remove known non-location phrases.\n",
    "    location = text.strip()\n",
    "    clutter = [\n",
    "        \"Več o delovnih zaporah v prometni napovedi\",\n",
    "        \", zamuda 5 - 10 minut\",\n",
    "        \", zamuda 10 minut\",\n",
    "        \".\"\n",
    "    ]\n",
    "    for c in clutter:\n",
    "        location = location.replace(c, \"\").strip()\n",
    "    return location\n",
    "\n",
    "# --- 3. The Main Parsing Function ---\n",
    "\n",
    "def parse_traffic_text(text: str) -> List[ParsedTrafficEvent]:\n",
    "    \"\"\"\n",
    "    Parses a block of unstructured traffic text into a list of structured event objects.\n",
    "\n",
    "    Args:\n",
    "        text (str): The raw input text containing multiple traffic events.\n",
    "\n",
    "    Returns:\n",
    "        List[ParsedTrafficEvent]: A list of parsed event objects.\n",
    "    \"\"\"\n",
    "    events: List[ParsedTrafficEvent] = []\n",
    "    \n",
    "    # Split text into lines. This is a robust starting point for traffic reports.\n",
    "    # We filter out empty lines.\n",
    "    lines = [line.strip() for line in text.split('.') if line.strip()]\n",
    "\n",
    "    for line in lines:\n",
    "        event_type = _find_event_type(line)\n",
    "\n",
    "        # We only create an event if it's something we can classify.\n",
    "        # This ignores generic lines like \"Več o...\" if they don't contain other keywords.\n",
    "        if event_type != EventType.UNKNOWN:\n",
    "            delay = _extract_delay(line)\n",
    "            detour = \"obvoz\" in line.lower()\n",
    "            location = _extract_location(line)\n",
    "\n",
    "            event = ParsedTrafficEvent(\n",
    "                event_type=event_type,\n",
    "                location_description=location,\n",
    "                delay_minutes=delay,\n",
    "                detour_available=detour if detour else None, # Only set if True\n",
    "                source_text=line\n",
    "            )\n",
    "            events.append(event)\n",
    "    \n",
    "    return events\n",
    "\n",
    "\n",
    "# --- 4. Example Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df = pd.read_csv(\"trainingdataset_optimized.csv\", encoding=\"utf-8\")\n",
    "    sample = df.sample(n=1)  # Sample one row for testing\n",
    "    input_text = sample[\"Input_Message\"].iloc[0] # Get the first input message for testing\n",
    "    output_text = sample[\"Output Message\"].iloc[0] # Get the first output message for testing\n",
    "\n",
    "    print(\"--- Parsing Input Text ---\")\n",
    "    print(input_text)  # Display the input text\n",
    "    parsed_events = parse_traffic_text(input_text)\n",
    "    \n",
    "    print(\"--- Parsing Output Text ---\")\n",
    "    print(output_text)  # Display the output text\n",
    "    parsed_events2 = parse_traffic_text(output_text)  # Parse both input and output texts\n",
    "\n",
    "    print(f\"\\nFound {len(parsed_events)}/{len(parsed_events2)} events\")\n",
    "    print(\"-\" * 25)\n",
    "    \n",
    "    \n",
    "\n",
    "    for i, event in enumerate(parsed_events):\n",
    "        print(f\"Event #{i+1}:\")\n",
    "        # .model_dump_json is a Pydantic v2 method, for v1 use .json()\n",
    "        try:\n",
    "            print(event.model_dump_json(indent=2, exclude={'parse_timestamp'}))\n",
    "        except AttributeError:\n",
    "            print(event.json(indent=2, exclude={'parse_timestamp'})) # For Pydantic v1\n",
    "        print(\"-\" * 25)\n",
    "        \n",
    "    for i, event in enumerate(parsed_events2):\n",
    "        print(f\"Event #{i+1}:\")\n",
    "        # .model_dump_json is a Pydantic v2 method, for v1 use .json()\n",
    "        try:\n",
    "            print(event.model_dump_json(indent=2, exclude={'parse_timestamp'}))\n",
    "        except AttributeError:\n",
    "            print(event.json(indent=2, exclude={'parse_timestamp'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b104b0f",
   "metadata": {},
   "source": [
    "im kind of thinking what is the point of this, why do i have to transform the input and the output into some event formats? liek this \n",
    "Event #2:\n",
    "{\n",
    "  \"event_type\": \"Delo na cesti\",\n",
    "  \"location_description\": \"\",\n",
    "  \"delay_minutes\": null,\n",
    "  \"detour_available\": null,\n",
    "  \"source_text\": \"Več o delovnih zaporah v prometni napovedi\"\n",
    "}\n",
    "\n",
    "when i could have full jsons \n",
    "like \n",
    "json_format = {\n",
    "  \"Delo na cesti\": \"ContentDeloNaCestiSLO\",\n",
    "  \"Zastoji\": \"\",\n",
    "  \"Ovire\": \"ContentOvireSLO\",\n",
    "  \"Nesreče\": \"ContentNesreceSLO\",\n",
    "  \"Vreme\": \"ContentVremeSLO\",\n",
    "  \"Prireditve\": \"\",\n",
    "  \"Opozorila\": \"ContentOpozorilaSLO\",\n",
    "  \"Mejni prehodi\": \"\",\n",
    "  \"Delovne zapore\": \"\",\n",
    "  \"Zimske razmere\": \"\",\n",
    "  \"Dela\": \"\",\n",
    "  \"Popolne zapore\": \"\",\n",
    "  \"Omejitve za tovorna vozila\": \"\",\n",
    "  \"leftover_html\": \"ContentPomembnoSLO\"\n",
    "}\n",
    "\n",
    "i goess that the point is that im trying to match the input to be as close as possible to the output. I can not change the output but i can customize the inputs to some degree. so i think i should create aprser for the outputs so that i get the json format of the output and comare it with the json format of the input which is already correctly structured. and i will see if i have to cut anything from the inputs to match the whole thing better. beacauset his is  nlp model fine tuning not a person reading and thinking whats important based on 3 h of past information and making shit up the fly. No what i want is 100 matching input that then gets put in a public speaking form . So the fact that there is maybe a different information 3 0min before does not matter to me what matter is the information that was actually used. and the end result here is that im hoping for a higher semantic match. of I/O. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16da957",
   "metadata": {},
   "source": [
    "**Mapping output for  json_format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11438734",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_format = {\n",
    "    \"Delo na cesti\": \"\",\n",
    "    \"Dela\": \"\",\n",
    "    \"Delovne zapore\": \"\",\n",
    "    \"Popolne zapore\": \"\",\n",
    "    \"Zastoji\": \"\",\n",
    "    \"Ovire\": \"\",\n",
    "    \"Nesreče\": \"\",\n",
    "    \"Opozorila\": \"\",\n",
    "    \"Vreme\": \"\",\n",
    "    \"Zimske razmere\": \"\",\n",
    "    \"Prireditve\": \"\",\n",
    "    \"Mejni prehodi\": \"\",\n",
    "    \"Omejitve za tovorna vozila\": \"\",\n",
    "    \"leftover_html\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb8cf0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Datum",
         "rawType": "datetime64[us]",
         "type": "unknown"
        },
        {
         "name": "TMP_file_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "TMP_folder_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "content_01",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "content_02",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "content_03",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "content_04",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "content_05",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "86ecc1c8-0dc9-44c5-ad65-6e255cdb3b0f",
       "rows": [
        [
         "0",
         "2022-04-30 18:30:00",
         "TMP-1.rtf",
         "April 2022",
         "Zaradi prometne nesreče je zaprta regionalna cesta Ajševica-Rožna Dolina, in to pri Ajševici.",
         "Na mejnem prehodu Obrežje vozniki na vstop v državo čakajo do dve uri, v Gruškovju pa pol ure.  \nPovečan promet pri izstopu iz države pa je na prehodu Dobovec, na katerem vozniki čakajo uro in pol, ter na Obrežju in v Gruškovju, v katerem vozniki čakajo pol ure.\n\u0000",
         null,
         null,
         null
        ],
        [
         "1",
         "2022-04-30 13:00:00",
         "TMP-10.rtf",
         "April 2022",
         "Zaradi pokvarjenega vozila je na štajerski avtocesti v predoru Jasovnik zaprt vozni pas proti Ljubljani.",
         "Na mejnih prehodih Sečovlje, Petrina, Dragonja in Dobovec vozniki osebnih vozil na izstop iz države čakajo približno pol ure; na Obrežju, v Gruškovju ter Slovenski vasi pa 1 uro pri vstopu v državo.",
         "\u0000",
         null,
         null
        ],
        [
         "2",
         "2022-04-27 06:30:00",
         "TMP-100.rtf",
         "April 2022",
         "Zaradi prometne nesreče je na gorenjski avtocesti proti Avstriji zaprt prehitevalni pas med priključkom Vodice in počivališčem Vóklo.",
         "Na mariborski vzhodni obvoznici promet zaradi del med razcepom Dragučova in krožiščem Pesnica poteka dvosmerno po polovici avtoceste.\n\u0000",
         null,
         null,
         null
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Datum</th>\n",
       "      <th>TMP_file_name</th>\n",
       "      <th>TMP_folder_name</th>\n",
       "      <th>content_01</th>\n",
       "      <th>content_02</th>\n",
       "      <th>content_03</th>\n",
       "      <th>content_04</th>\n",
       "      <th>content_05</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-04-30 18:30:00</td>\n",
       "      <td>TMP-1.rtf</td>\n",
       "      <td>April 2022</td>\n",
       "      <td>Zaradi prometne nesreče je zaprta regionalna c...</td>\n",
       "      <td>Na mejnem prehodu Obrežje vozniki na vstop v d...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-04-30 13:00:00</td>\n",
       "      <td>TMP-10.rtf</td>\n",
       "      <td>April 2022</td>\n",
       "      <td>Zaradi pokvarjenega vozila je na štajerski avt...</td>\n",
       "      <td>Na mejnih prehodih Sečovlje, Petrina, Dragonja...</td>\n",
       "      <td>\u0000</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-04-27 06:30:00</td>\n",
       "      <td>TMP-100.rtf</td>\n",
       "      <td>April 2022</td>\n",
       "      <td>Zaradi prometne nesreče je na gorenjski avtoce...</td>\n",
       "      <td>Na mariborski vzhodni obvoznici promet zaradi ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Datum TMP_file_name TMP_folder_name  \\\n",
       "0 2022-04-30 18:30:00     TMP-1.rtf      April 2022   \n",
       "1 2022-04-30 13:00:00    TMP-10.rtf      April 2022   \n",
       "2 2022-04-27 06:30:00   TMP-100.rtf      April 2022   \n",
       "\n",
       "                                          content_01  \\\n",
       "0  Zaradi prometne nesreče je zaprta regionalna c...   \n",
       "1  Zaradi pokvarjenega vozila je na štajerski avt...   \n",
       "2  Zaradi prometne nesreče je na gorenjski avtoce...   \n",
       "\n",
       "                                          content_02 content_03 content_04  \\\n",
       "0  Na mejnem prehodu Obrežje vozniki na vstop v d...       None       None   \n",
       "1  Na mejnih prehodih Sečovlje, Petrina, Dragonja...          \u0000       None   \n",
       "2  Na mariborski vzhodni obvoznici promet zaradi ...       None       None   \n",
       "\n",
       "  content_05  \n",
       "0       None  \n",
       "1       None  \n",
       "2       None  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df = output_df.to_pandas()\n",
    "output_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "240c8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PARSED OUTPUT FOR EXAMPLE 1 ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style='white-space: pre-wrap;'>Output Message 1: Zaradi pokvarjenega vozila je na primorski avtocesti proti Kopru promet oviran pred priključkom Kozina. Promet na podravski avtocesti proti Gruškovju med priključkoma Lancova vas in Podlehnik ni več oviran zaradi nesreče.</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"Delo na cesti\": \"\",\n",
      "    \"Dela\": \"\",\n",
      "    \"Delovne zapore\": \"\",\n",
      "    \"Popolne zapore\": \"\",\n",
      "    \"Zastoji\": \"\",\n",
      "    \"Ovire\": \"Zaradi pokvarjenega vozila je na primorski avtocesti proti Kopru promet oviran pred priključkom Kozina.\",\n",
      "    \"Nesreče\": \"Promet na podravski avtocesti proti Gruškovju med priključkoma Lancova vas in Podlehnik ni več oviran zaradi nesreče.\",\n",
      "    \"Opozorila\": \"\",\n",
      "    \"Vreme\": \"\",\n",
      "    \"Zimske razmere\": \"\",\n",
      "    \"Prireditve\": \"\",\n",
      "    \"Mejni prehodi\": \"\",\n",
      "    \"Omejitve za tovorna vozila\": \"\",\n",
      "    \"leftover_html\": \"\"\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def _clean_text(text: str) -> str:\n",
    "    # Replace newlines, tabs, and multiple spaces with a single space\n",
    "    text = re.sub(r'[\\n\\r\\t]+', ' ', text)\n",
    "    # Remove list markers like '-' and extra spaces around them\n",
    "    text = re.sub(r'\\s*-\\s*', ' ', text)\n",
    "    # Ensure a space follows a period for sentence splitting, unless it's the end of the string\n",
    "    text = re.sub(r'\\.(?=[a-zA-Z])', '. ', text)\n",
    "    # Consolidate multiple spaces into one\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "DEPENDENT_PHRASES = [\n",
    "    r'^Nastal je krajši zastoj',\n",
    "    r'\\bčas potovanja se(?:\\s+\\w+){0,5}?\\s+podaljša\\b'\n",
    "    r'^čas vožnje se podaljša',\n",
    "    #Predor zaradi varnosti občasno zapirajo\n",
    "    r'^Predor zaradi varnosti občasno zapirajo',\n",
    "]\n",
    "\n",
    "def merge_dependent_sentences(sentences, dependent_phrases):\n",
    "    merged = []\n",
    "    i = 0\n",
    "    while i < len(sentences):\n",
    "        current = sentences[i]\n",
    "        is_dependent = (\n",
    "            i > 0 and (\n",
    "                any(re.match(p, current.lower()) for p in dependent_phrases)\n",
    "                or len(current) < 85\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if is_dependent and merged:\n",
    "            # Safe to merge with previous\n",
    "            merged[-1] += ' ' + current\n",
    "        else:\n",
    "            # Either it's the first sentence or not dependent\n",
    "            merged.append(current)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "def parse_traffic_report_exclusive(text: str) -> dict:\n",
    "    json_format = {\n",
    "        \"Delo na cesti\": \"\",\n",
    "        \"Dela\": \"\",\n",
    "        \"Delovne zapore\": \"\",\n",
    "        \"Popolne zapore\": \"\",\n",
    "        \"Zastoji\": \"\", \"Ovire\": \"\", \"Nesreče\": \"\", \"Opozorila\": \"\", \"Vreme\": \"\",\n",
    "        \"Zimske razmere\": \"\", \"Prireditve\": \"\", \"Mejni prehodi\": \"\",\n",
    "        \"Omejitve za tovorna vozila\": \"\", \"leftover_html\": \"\"\n",
    "    }\n",
    "    results = {key: [] for key in json_format.keys()}\n",
    "\n",
    "    # 1. Pre-process the text to handle formatting issues\n",
    "    cleaned_text = _clean_text(text)\n",
    "    \n",
    "    # Split text into sentences and filter out empty strings\n",
    "    sentences = [s.strip() for s in re.split(r'(?<=[.?!])\\s+', cleaned_text) if s.strip()]\n",
    "    sentences = merge_dependent_sentences(sentences, DEPENDENT_PHRASES)\n",
    "    # 2. Priority-based Classification Rules\n",
    "    # The script checks categories in this order. The first match wins.\n",
    "    # `\\b` is a word boundary to prevent partial mat\n",
    "        \n",
    "    classification_rules = [\n",
    "        ('Zastoji', [\n",
    "            r'\\bzastoj\\w*\\b', r'\\bgost.*?promet\\b', r'\\bzgošč.*?promet\\b',\n",
    "            r'čas.*?potovanj.*?podaljš\\w*', r'potoval.*?res.*?podaljš\\w*',\n",
    "            r'\\bkolon\\w*\\b', r'\\bupoč\\w*\\b', r'\\bzamud\\w*\\b'\n",
    "        ]),\n",
    "\n",
    "        ('Opozorila', [ \n",
    "            r'\\bopozarj\\w*\\b', r'\\bnevarn\\w*\\b', r'\\bpriporoč\\w*\\b',\n",
    "            r'\\bpozorn\\w*\\b', r'\\bobvoz\\w*\\b', r'\\bpreusmer\\w*\\b',\n",
    "            r'\\bpozar\\w*\\b', r'\\bspolz\\w*\\b', r'\\bbo\\b', r'\\bbodo\\b',\n",
    "            \n",
    "            # New patterns for extended travel time\n",
    "            r'\\b(potovaln\\w*|čas\\s+potovanja)\\s+se\\s+podaljš\\w*\\b',\n",
    "            r'\\bse\\s+podaljš\\w*\\s+(potovaln\\w*|čas\\s+potovanja)\\b',\n",
    "            r'\\bčas\\s+se\\s+podaljš\\w*\\b',\n",
    "            r'\\bpodaljšan\\s+(čas|potovaln\\w*)\\b'\n",
    "        ]),\n",
    "\n",
    "\n",
    "        ('Zimske razmere', [\n",
    "            r'\\bsneg\\w*\\b', r'\\bverig\\w*\\b', r'\\bzims\\w*\\b', r'\\bled\\w*\\b'\n",
    "        ]),\n",
    "\n",
    "        ('Vreme', [\n",
    "            r'\\bmegl\\w*\\b', r'\\bpoledic\\w*\\b', r'\\bburj\\w*\\b', r'\\bvidljiv\\w*\\b'\n",
    "        ]),\n",
    "\n",
    "        ('Prireditve', [\n",
    "            r'\\bprired\\w*\\b', r'\\bdogod\\w*\\b', r'\\bmaraton\\w*\\b'\n",
    "        ]),\n",
    "\n",
    "        ('Nesreče', [\n",
    "            r'\\bnesreč\\w*\\b', r'\\btrčen\\w*\\b', r'\\bnezgod\\w*\\b'\n",
    "        ]),\n",
    "\n",
    "        ('Ovire', [\n",
    "            r'\\bovir\\w*\\b', r'\\bpokvarj\\w*\\s+(voz\\w*|tovornjak\\w*)\\b',\n",
    "            r'\\bobtič\\w*\\b', r'\\brazsut\\w*\\s+tovor\\w*\\b'\n",
    "        ]),\n",
    "\n",
    "        ('Popolne zapore', [\n",
    "            r'zap\\w*.*?(regionaln\\w*\\s+cest\\w*|avtocest\\w*|cest\\w*|hitr\\w*\\s+cest\\w*|obvoznic\\w*|križišč\\w*|predor\\w*|uvoz\\w*|izvoz\\w*)',\n",
    "            r'\\bpopoln\\w*\\s+zapor\\w*\\b'\n",
    "        ]),\n",
    "\n",
    "        ('Delovne zapore', [\n",
    "            r'zap\\w*.*?(\\bvozn\\w*\\s+pas\\w*|\\bprehit\\w*\\s+pas\\w*|\\bodstav\\w*\\s+pas\\w*|\\bpočasn\\w*\\s+vozil\\w*\\s+pas\\w*)',\n",
    "            r'zožen\\w*\\s+pas\\w*'\n",
    "        ]),\n",
    "\n",
    "        ('Delo na cesti', [\n",
    "            r'del\\w*.*?cest\\w*', r'promet.*?(urej\\w*|potek\\w*)', r'gradbišč\\w*', r'delovišč\\w*'\n",
    "        ]),\n",
    "\n",
    "        ('Omejitve za tovorna vozila', [\n",
    "            r'prepoved\\w*\\s+promet\\w*\\s+za\\b', r'tovorn\\w*\\s+vozil\\w*\\b', r'tovornjak\\w*\\b',\n",
    "            r'hladilnik\\w*\\b', r'ponjav\\w*\\b'\n",
    "        ]),\n",
    "\n",
    "        ('Mejni prehodi', [\n",
    "            r'mejn\\w*\\b', r'carin\\w*\\b', r'\\b(prehod|vstop\\w*).*dr\\w*\\b',  r'\\b(prehod|izstop\\w*).*dr\\w*\\b'\n",
    "        ]),\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    unclassified_sentences = list(sentences)\n",
    "\n",
    "    for category, patterns in classification_rules:\n",
    "        remaining_sentences = []\n",
    "\n",
    "        for sentence in unclassified_sentences:\n",
    "            matched = False\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, sentence.lower()):\n",
    "                    results[category].append(sentence)\n",
    "                    matched = True\n",
    "                    break  # Stop checking further patterns for this category\n",
    "\n",
    "            if not matched:\n",
    "                remaining_sentences.append(sentence)\n",
    "\n",
    "        unclassified_sentences = remaining_sentences  # Only keep unmatched for next category\n",
    "\n",
    "\n",
    "\n",
    "    # 3. Handle leftovers\n",
    "    results['leftover_html'] = unclassified_sentences\n",
    "\n",
    "    # 4. Aggregate 'Dela' category\n",
    "    results['Dela'] = results['Popolne zapore'] + results['Delovne zapore'] + results['Delo na cesti']\n",
    "    \n",
    "    # 5. Finalize the JSON by joining sentence lists into strings\n",
    "    final_json = {}\n",
    "    for key, sent_list in results.items():\n",
    "        final_json[key] = \" \".join(sent_list)\n",
    "\n",
    "    return final_json, cleaned_text\n",
    "\n",
    "\n",
    "def parse_output_messages(output_row, columns):\n",
    "    row_dict = dict(zip(columns, output_row))\n",
    "    message_parts = []\n",
    "    for i in range(1, 6):  # Loop through content_01 to content_05\n",
    "        col_name = f\"content_0{i}\"\n",
    "        content = row_dict.get(col_name)\n",
    "        if content:\n",
    "            cleaned = str(content).replace('\\x00', '')  # Remove ALL null characters\n",
    "            if cleaned.strip():  # Only append if there's still visible text\n",
    "                message_parts.append(cleaned)\n",
    "    return \" \".join(message_parts)\n",
    "\n",
    "\n",
    "sample_df = output_df.sample(n=1)  # Sample 3 rows for testing\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "for i, output_row in enumerate(sample_df.iter_rows(), 1):\n",
    "    print(f\"--- PARSED OUTPUT FOR EXAMPLE {i} ---\")\n",
    "    output_message = parse_output_messages(output_row, output_df.columns)\n",
    "    parsed_json, cleaned_output_message = parse_traffic_report_exclusive(output_message)\n",
    "    # Use json.dumps for pretty printing with UTF-8 support\n",
    "    display(HTML(f\"<pre style='white-space: pre-wrap;'>Output Message {i}: {cleaned_output_message}</pre>\"))\n",
    "    print(json.dumps(parsed_json, indent=4, ensure_ascii=False))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "5e81eae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing 23617 output examples with 10 input samples each...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:   0%|          | 29/23617 [00:06<55:59,  7.02it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-04-26 08:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:   5%|▌         | 1223/23617 [05:00<1:07:46,  5.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-08-10 20:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:   7%|▋         | 1585/23617 [06:31<55:47,  6.58it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-12-26 01:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:   7%|▋         | 1602/23617 [06:34<47:49,  7.67it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-12-25 10:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:   8%|▊         | 1803/23617 [07:22<46:26,  7.83it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-12-18 10:30:00: 'NoneType' object has no attribute 'to_pandas'\n",
      "Error processing output row for timestamp 2022-12-18 10:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:   9%|▊         | 2015/23617 [08:12<1:12:34,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-12-10 18:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:   9%|▉         | 2205/23617 [08:59<1:19:59,  4.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-12-04 09:30:00: 'NoneType' object has no attribute 'to_pandas'\n",
      "Error processing output row for timestamp 2022-12-04 09:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  10%|█         | 2447/23617 [09:57<49:57,  7.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-02-20 18:30:00: 'NoneType' object has no attribute 'to_pandas'\n",
      "Error processing output row for timestamp 2022-02-20 18:00:00: 'NoneType' object has no attribute 'to_pandas'\n",
      "Error processing output row for timestamp 2022-02-20 17:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  10%|█         | 2474/23617 [10:02<50:03,  7.04it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-02-19 20:00:00: 'NoneType' object has no attribute 'to_pandas'\n",
      "Error processing output row for timestamp 2022-02-19 20:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  11%|█▏        | 2668/23617 [10:47<1:02:01,  5.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-02-13 09:30:00: 'NoneType' object has no attribute 'to_pandas'\n",
      "Error processing output row for timestamp 2022-02-13 09:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  12%|█▏        | 2851/23617 [11:29<44:16,  7.82it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-02-06 17:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  12%|█▏        | 2866/23617 [11:31<51:36,  6.70it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-02-06 09:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  12%|█▏        | 2868/23617 [11:32<40:18,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-02-06 08:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  13%|█▎        | 3071/23617 [12:17<46:16,  7.40it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-01-27 09:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  14%|█▍        | 3277/23617 [12:58<1:15:21,  4.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-01-30 11:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  14%|█▍        | 3300/23617 [13:03<1:27:05,  3.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-01-30 10:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  16%|█▋        | 3856/23617 [15:03<57:11,  5.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-07-24 22:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  16%|█▋        | 3890/23617 [15:10<43:59,  7.47it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-07-23 18:30:00: 'NoneType' object has no attribute 'to_pandas'\n",
      "Error processing output row for timestamp 2022-07-23 18:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  18%|█▊        | 4317/23617 [17:00<1:03:44,  5.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-07-06 20:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  28%|██▊       | 6701/23617 [26:54<34:05,  8.27it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-03-06 07:00:00: 'NoneType' object has no attribute 'to_pandas'\n",
      "Error processing output row for timestamp 2022-03-06 07:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  32%|███▏      | 7601/23617 [30:34<36:39,  7.28it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2022-11-27 12:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  39%|███▊      | 9130/23617 [36:49<57:54,  4.17it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2023-04-30 18:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  39%|███▉      | 9243/23617 [37:17<47:20,  5.06it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2023-04-30 18:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  40%|███▉      | 9354/23617 [37:44<42:06,  5.65it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2023-04-30 18:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  40%|████      | 9463/23617 [38:05<46:32,  5.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2023-04-30 17:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  45%|████▍     | 10528/23617 [42:19<55:19,  3.94it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2023-08-07 21:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  55%|█████▍    | 12954/23617 [51:51<24:30,  7.25it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2023-01-02 17:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  56%|█████▌    | 13191/23617 [52:48<44:52,  3.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2023-07-22 22:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  66%|██████▌   | 15498/23617 [1:02:25<34:31,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2023-03-25 15:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  71%|███████   | 16787/23617 [1:07:37<18:11,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2023-11-13 10:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  72%|███████▏  | 16903/23617 [1:08:07<19:59,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2023-11-09 10:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  72%|███████▏  | 16983/23617 [1:08:26<13:39,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2023-11-06 18:30:00: 'NoneType' object has no attribute 'to_pandas'\n",
      "Error processing output row for timestamp 2023-11-06 18:30:00: 'NoneType' object has no attribute 'to_pandas'\n",
      "Error processing output row for timestamp 2023-11-06 18:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  80%|███████▉  | 18817/23617 [1:15:52<22:01,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2024-04-06 22:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  80%|████████  | 18969/23617 [1:16:27<18:53,  4.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2024-05-01 00:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  84%|████████▎ | 19723/23617 [1:18:59<07:58,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2024-12-31 08:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  84%|████████▎ | 19732/23617 [1:19:01<16:59,  3.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2024-12-31 08:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  86%|████████▌ | 20227/23617 [1:20:47<14:38,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2024-12-28 14:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  88%|████████▊ | 20711/23617 [1:22:19<07:29,  6.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2024-01-13 22:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  93%|█████████▎| 21994/23617 [1:26:52<03:51,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2024-11-24 10:30:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples:  96%|█████████▌| 22702/23617 [1:29:45<02:18,  6.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing output row for timestamp 2024-10-21 18:00:00: 'NoneType' object has no attribute 'to_pandas'\n",
      "Error processing output row for timestamp 2024-10-21 17:30:00: 'NoneType' object has no attribute 'to_pandas'\n",
      "Error processing output row for timestamp 2024-10-21 17:00:00: 'NoneType' object has no attribute 'to_pandas'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Output Examples: 100%|██████████| 23617/23617 [1:32:08<00:00,  4.27it/s]\n"
     ]
    }
   ],
   "source": [
    "#optimizzed version\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Assume these functions are defined elsewhere in your script\n",
    "# from your_utils import parse_html_into_json, get_first_inputs_before_time\n",
    "\n",
    "# It's good practice to load the model once\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "\n",
    "def compare_json_to_output_optimized(input_df, output_df, n_samples=15):\n",
    "    \"\"\"\n",
    "    Optimized function to find the best input match for each output message.\n",
    "    \"\"\"\n",
    "    best_results_list = []\n",
    "    output_columns = output_df.columns\n",
    "\n",
    "    print(f\"Comparing {len(output_df)} output examples with {n_samples} input samples each...\")\n",
    "\n",
    "    # Iterate directly over output_df rows. This is much cleaner than sampling in a loop.\n",
    "    for output_row_tuple in tqdm(output_df.iter_rows(), total=len(output_df), desc=\"Processing Output Examples\"):\n",
    "\n",
    "        try:\n",
    "            # --- 1. Process the output ONCE per outer loop ---\n",
    "            output_message = parse_output_messages(output_row_tuple, output_columns)\n",
    "            output_json, output_message = parse_traffic_report_exclusive(output_message)\n",
    "            output_json[\"Timestamp\"] = output_row_tuple[0].strftime('%Y-%m-%d %H:%M:%S')\n",
    "            output_json_str = json.dumps(output_json, indent=2, ensure_ascii=False)\n",
    "            if not output_message: # Skip if the output message is empty\n",
    "                continue\n",
    "            \n",
    "            # Encode the single output message\n",
    "            output_embedding = model.encode(output_message, convert_to_tensor=True)\n",
    "\n",
    "            # --- 2. Get candidate inputs ---\n",
    "            input_rows_df = get_first_inputs_before_time(n_samples, output_row_tuple, input_df).to_pandas()\n",
    "            input_rows_df_timesort = get_first_input_before_time(1, output_row_tuple, input_df).to_pandas()\n",
    "            # print(f\"Input rows for output row {output_row_tuple[0]}: {len(input_rows_df_timesort)} candidates found.\")\n",
    "            \n",
    "            for _, input_row in input_rows_df_timesort.iterrows():\n",
    "                html_timesort = str(input_row.get(\"B1\", \"\"))\n",
    "                myjson_timesort = parse_html_into_json(html_timesort, json_format, input_row[\"Datum\"]) \n",
    "                # print(f\"Parsed JSON for timesort: {myjson_timesort}\")\n",
    "            \n",
    "            \n",
    "            \n",
    "            if input_rows_df.empty:\n",
    "                continue\n",
    "\n",
    "            # --- 3. Prepare all input texts for BATCH processing ---\n",
    "            input_texts_to_encode = []\n",
    "            processed_inputs = [] # Keep track of the full parsed data\n",
    "\n",
    "            for _, input_row in input_rows_df.iterrows():\n",
    "                html = str(input_row.get(\"B1\", \"\"))\n",
    "                \n",
    "                # Assume parse_html_into_json is defined and returns a dict\n",
    "                myjson = parse_html_into_json(html, json_format, input_row[\"Datum\"]) \n",
    "                \n",
    "                full_string = \" \".join(value for value in myjson.values() if value)\n",
    "                input_texts_to_encode.append(full_string)\n",
    "                processed_inputs.append({\n",
    "                    \"Input_Message\": full_string,\n",
    "                    \"Input_JSON_str\": json.dumps(myjson, indent=2, ensure_ascii=False),\n",
    "                    \"Datum\": input_row[\"Datum\"]\n",
    "                })\n",
    "\n",
    "            # --- 4. BATCH encode all input texts at once ---\n",
    "            input_embeddings = model.encode(input_texts_to_encode, convert_to_tensor=True)\n",
    "            \n",
    "            # --- 5. Compute all similarities in a single, fast operation ---\n",
    "            cosine_scores = util.cos_sim(output_embedding, input_embeddings)\n",
    "            # print(f\"Cosine scores: {cosine_scores}\")\n",
    "            \n",
    "            # Find the index of the best score\n",
    "            best_sample_idx = cosine_scores[0].argmax().item()\n",
    "\n",
    "            # --- 6. Select the best result ---\n",
    "            best_input_data = processed_inputs[best_sample_idx]\n",
    "            best_score = cosine_scores[0][best_sample_idx].item()\n",
    "            # print(f\"Best score for output row {output_row_tuple[0]}: {best_score:.4f}\")\n",
    "            \n",
    "            # Append the single best result for this output to our list\n",
    "            best_results_list.append({\n",
    "                \"Input_Message\": best_input_data[\"Input_Message\"],\n",
    "                \"Input_JSON\": best_input_data[\"Input_JSON_str\"],\n",
    "                \"Output_Message\": output_message,\n",
    "                \"Output_JSON\": output_json_str,\n",
    "                \"Datum\": best_input_data[\"Datum\"],\n",
    "                \"Similarity_Score\": best_score,\n",
    "                \"Input_timesort\": html_timesort,\n",
    "                \"json_timesort\": json.dumps(myjson_timesort, indent=2, ensure_ascii=False),\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            # It's good to know which row caused an error\n",
    "            output_timestamp = output_row_tuple[0] \n",
    "            print(f\"Error processing output row for timestamp {output_timestamp}: {e}\")\n",
    "            continue\n",
    "\n",
    "    # --- 7. Create the final DataFrame ONCE from the list of results ---\n",
    "    if not best_results_list:\n",
    "        print(\"No valid examples were processed.\")\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    final_df = pd.DataFrame(best_results_list)\n",
    "    return final_df\n",
    "\n",
    "# Assuming input_df and output_df are already loaded as Polars DataFrames\n",
    "# And json_format is defined\n",
    "#sample the outputdf  \n",
    "output_df_sample = output_df.sample(n=1)  # Sample 15 rows for testing\n",
    "traffic_examples = compare_json_to_output_optimized(input_df, output_df, n_samples=10)\n",
    "traffic_examples.to_csv(\"trainingdataset_optimized.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def display_jsons_side_by_side(input_json, output_json, json_timesort):\n",
    "    input_pretty = json.dumps(input_json, indent=2, ensure_ascii=False)\n",
    "    output_pretty = json.dumps(output_json, indent=2, ensure_ascii=False)\n",
    "    timesort_pretty = json.dumps(json_timesort, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    html = f\"\"\"\n",
    "    <div style=\"display: flex; gap: 20px; font-family: monospace;\">\n",
    "        <div style=\"flex: 1; white-space: pre; border: 1px solid #ccc; padding: 10px; overflow: auto; max-height: 400px;\">\n",
    "            <h3>Input JSON</h3>\n",
    "            <pre>{input_pretty}</pre>\n",
    "        </div>\n",
    "        <div style=\"flex: 1; white-space: pre; border: 1px solid #ccc; padding: 10px; overflow: auto; max-height: 400px;\">\n",
    "            <h3>Output JSON</h3>\n",
    "            <pre>{output_pretty}</pre>\n",
    "        </div>\n",
    "    </div>\n",
    "        <div style=\"display: flex; gap: 20px; font-family: monospace;\">\n",
    "        <div style=\"flex: 1; white-space: pre; border: 1px solid #ccc; padding: 10px; overflow: auto; max-height: 400px;\">\n",
    "            <h3>Timesort JSON</h3>\n",
    "            <pre>{input_pretty}</pre>\n",
    "        </div>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    display(HTML(html))\n",
    "    \n",
    "# for i, row in traffic_examples.iterrows():\n",
    "#     print(f\"--- Example {i+1} ---\")\n",
    "#     input_json = json.loads(row[\"Input_JSON\"])\n",
    "#     output_json = json.loads(row[\"Output_JSON\"])\n",
    "#     json_timesort = json.loads(row[\"json_timesort\"])\n",
    "    \n",
    "    \n",
    "#     display(HTML(f\"<pre style='white-space: pre-wrap;'>Original Timesort Message : {row[\"Input_timesort\"]}</pre>\"))\n",
    "#     display(HTML(f\"<pre style='white-space: pre-wrap;'>Input Semanticsort Message : {row[\"Input_Message\"]}</pre>\"))\n",
    "#     display(HTML(f\"<pre style='white-space: pre-wrap;'>Output Message : {row[\"Output_Message\"]}</pre>\"))\n",
    "#     display_jsons_side_by_side(input_json, output_json, json_timesort)\n",
    "    \n",
    "#     print(f\"Similarity Score: {row['Similarity_Score']:.4f}\")\n",
    "#     print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "029a3007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Determining Relevance ---\n",
      "MATCH (Score: 0.62): Na gorenjski avtocesti bo promet skozi predor Karavanke urejen izmenično enosmerno s čakanjem pred predorom 30 minut, med 8.\n",
      "NO MATCH (Score: 0.05): in 16.\n",
      "NO MATCH (Score: 0.07): uro: 2., 3., 6., 8., 9.\n",
      "NO MATCH (Score: 0.01): in 10.\n",
      "NO MATCH (Score: 0.13): marca.\n",
      "MATCH (Score: 0.74): Na primorski avtocesti bo od jutri, predvidoma do konca marca, promet med razcepom Srmin in priključkom Črni Kal potekal po enem pasu v vsako smer.\n",
      "MATCH (Score: 0.76): Zaprt bo razcep Srmin iz smeri Škofij proti Ljubljani ter priključek Luka Koper.\n",
      "MATCH (Score: 0.77): Obvoz iz smeri Škofij proti Ljubljani bo preko priključka Bertoki.\n",
      "NO MATCH (Score: 0.32): Zaradi jutranje prometne konice je promet povečan na mestnih obvoznicah in vpadnicah.\n",
      "MATCH (Score: 0.75): Na štajerski avtocesti je zaradi okvare tovornega vozila oviran promet med priključkoma Slovenska Bistrica jug in Slovenske Konjice proti Ljubljani.\n",
      "NO MATCH (Score: 0.45): Na ljubljanski zahodni obvoznici je oviran promet na uvozu Brdo proti Kosezam.\n",
      "NO MATCH (Score: 0.30): Nastal je zastoj do razcepa Kozarje.\n",
      "NO MATCH (Score: 0.53): Cesta čez prelaz Vršič je prevozna samo za osebna vozila.\n",
      "\n",
      "--- Original Input ---\n",
      "{\n",
      "  \"Delo na cesti\": \"\",\n",
      "  \"Dela\": \"Na gorenjski avtocesti bo promet skozi predor Karavanke urejen izmenično enosmerno s čakanjem pred predorom 30 minut, med 8. in 16. uro: 2., 3., 6., 8., 9. in 10. marca. Na primorski avtocesti bo od jutri, predvidoma do konca marca, promet med razcepom Srmin in priključkom Črni Kal potekal po enem pasu v vsako smer. Zaprt bo razcep Srmin iz smeri Škofij proti Ljubljani ter priključek Luka Koper. Obvoz iz smeri Škofij proti Ljubljani bo preko priključka Bertoki.\",\n",
      "  \"Delovne zapore\": \"\",\n",
      "  \"Popolne zapore\": \"\",\n",
      "  \"Zastoji\": \"Zaradi jutranje prometne konice je promet povečan na mestnih obvoznicah in vpadnicah.\",\n",
      "  \"Ovire\": \"Na štajerski avtocesti je zaradi okvare tovornega vozila oviran promet med priključkoma Slovenska Bistrica jug in Slovenske Konjice proti Ljubljani.\",\n",
      "  \"Nesreče\": \"Na ljubljanski zahodni obvoznici je oviran promet na uvozu Brdo proti Kosezam. Nastal je zastoj do razcepa Kozarje.\",\n",
      "  \"Vreme\": \"Cesta čez prelaz Vršič je prevozna samo za osebna vozila.\",\n",
      "  \"Opozorila\": \"\",\n",
      "  \"Zimske razmere\": \"\",\n",
      "  \"Prireditve\": \"\",\n",
      "  \"Mejni prehodi\": \"\",\n",
      "  \"Omejitve za tovorna vozila\": \"\",\n",
      "  \"leftover_html\": \"\",\n",
      "  \"timestamp\": \"02-03-2022 07:15:00\"\n",
      "}\n",
      "\n",
      "--- NEW, CLEANED Input for Training ---\n",
      "{\n",
      "  \"Delo na cesti\": \"\",\n",
      "  \"Dela\": \"\",\n",
      "  \"Delovne zapore\": \"\",\n",
      "  \"Popolne zapore\": \"\",\n",
      "  \"Zastoji\": \"\",\n",
      "  \"Ovire\": \"Na štajerski avtocesti je zaradi okvare tovornega vozila oviran promet med priključkoma Slovenska Bistrica jug in Slovenske Konjice proti Ljubljani.\",\n",
      "  \"Nesreče\": \"\",\n",
      "  \"Vreme\": \"\",\n",
      "  \"Opozorila\": \"\",\n",
      "  \"Zimske razmere\": \"\",\n",
      "  \"Prireditve\": \"\",\n",
      "  \"Mejni prehodi\": \"\",\n",
      "  \"Omejitve za tovorna vozila\": \"\",\n",
      "  \"leftover_html\": \"\",\n",
      "  \"timestamp\": \"02-03-2022 07:15:00\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# It's best to load the model only once\n",
    "# 'paraphrase-multilingual-MiniLM-L12-v2' is a good, lightweight model for many languages.\n",
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "def get_all_text_from_json(data: dict, exclude_keys=None) -> list:\n",
    "    \"\"\"Extracts all sentences from the text fields of the JSON.\"\"\"\n",
    "    if exclude_keys is None:\n",
    "        exclude_keys = [\"timestamp\", \"Dela\"] # Exclude timestamp and aggregated 'Dela'\n",
    "        \n",
    "    all_sentences = []\n",
    "    for key, value in data.items():\n",
    "        if key.lower() in exclude_keys or not isinstance(value, str) or not value:\n",
    "            continue\n",
    "        # Split value into sentences, handling cases where there are multiple sentences in one field\n",
    "        sentences = re.split(r'(?<=[. ?!])\\s+', value.strip())\n",
    "        for sentence in sentences:\n",
    "            if sentence:\n",
    "                # Store sentence with its original category\n",
    "                all_sentences.append({'sentence': sentence, 'category': key})\n",
    "    return all_sentences\n",
    "\n",
    "\n",
    "def filter_input_by_output_relevance(input_json: dict, output_json: dict, similarity_threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Filters the input_json to only include sentences that are semantically\n",
    "    relevant to the text in the output_json.\n",
    "\n",
    "    Args:\n",
    "        input_json: The original, noisy input data.\n",
    "        output_json: The ground truth broadcasted output.\n",
    "        similarity_threshold: The cosine similarity score required to be considered 'relevant'.\n",
    "\n",
    "    Returns:\n",
    "        A new, cleaned input JSON containing only relevant information.\n",
    "    \"\"\"\n",
    "    # 1. Extract all sentences from input and all text from output\n",
    "    input_sentences_with_meta = get_all_text_from_json(input_json)\n",
    "    output_sentences_with_meta = get_all_text_from_json(output_json)\n",
    "    \n",
    "    # If there's nothing in the output, we can't determine relevance.\n",
    "    if not output_sentences_with_meta:\n",
    "        # Return a mostly empty structure matching the input\n",
    "        clean_input = {k: \"\" for k in input_json}\n",
    "        clean_input['timestamp'] = input_json.get('timestamp', '')\n",
    "        return clean_input\n",
    "\n",
    "    # Combine all output sentences into one target script for a single comparison point\n",
    "    target_script = \" \".join([item['sentence'] for item in output_sentences_with_meta])\n",
    "    \n",
    "    # Extract just the sentence text for encoding\n",
    "    input_sentence_texts = [item['sentence'] for item in input_sentences_with_meta]\n",
    "\n",
    "    # 2. Compute embeddings\n",
    "    target_embedding = model.encode(target_script, convert_to_tensor=True)\n",
    "    input_embeddings = model.encode(input_sentence_texts, convert_to_tensor=True)\n",
    "\n",
    "    # 3. Calculate cosine similarity\n",
    "    cosine_scores = util.cos_sim(target_embedding, input_embeddings)\n",
    "\n",
    "    # 4. Filter relevant sentences\n",
    "    relevant_sentences_with_meta = []\n",
    "    for i, item in enumerate(input_sentences_with_meta):\n",
    "        if cosine_scores[0][i] > similarity_threshold:\n",
    "            relevant_sentences_with_meta.append(item)\n",
    "            print(f\"MATCH (Score: {cosine_scores[0][i]:.2f}): {item['sentence']}\")\n",
    "        else:\n",
    "            print(f\"NO MATCH (Score: {cosine_scores[0][i]:.2f}): {item['sentence']}\")\n",
    "\n",
    "\n",
    "    # 5. Reconstruct the new, clean input JSON\n",
    "    # Start with an empty structure\n",
    "    clean_input_json = {key: [] for key in input_json.keys()}\n",
    "\n",
    "    for item in relevant_sentences_with_meta:\n",
    "        clean_input_json[item['category']].append(item['sentence'])\n",
    "    \n",
    "    # Join the lists of sentences back into strings\n",
    "    for key, value_list in clean_input_json.items():\n",
    "        clean_input_json[key] = \" \".join(value_list)\n",
    "        \n",
    "    # Re-aggregate the 'Dela' field from its constituent parts\n",
    "    dela_parts = [\n",
    "        clean_input_json.get('Popolne zapore', ''),\n",
    "        clean_input_json.get('Delovne zapore', ''),\n",
    "        clean_input_json.get('Delo na cesti', '')\n",
    "    ]\n",
    "    clean_input_json['Dela'] = \" \".join(part for part in dela_parts if part)\n",
    "\n",
    "\n",
    "    # Preserve the original timestamp\n",
    "    clean_input_json['timestamp'] = input_json.get('timestamp')\n",
    "    \n",
    "    return clean_input_json\n",
    "\n",
    "# --- Your Example Data ---\n",
    "input_json = {\n",
    "  \"Delo na cesti\": \"\",\n",
    "  \"Dela\": \"Na gorenjski avtocesti bo promet skozi predor Karavanke urejen izmenično enosmerno s čakanjem pred predorom 30 minut, med 8. in 16. uro: 2., 3., 6., 8., 9. in 10. marca. Na primorski avtocesti bo od jutri, predvidoma do konca marca, promet med razcepom Srmin in priključkom Črni Kal potekal po enem pasu v vsako smer. Zaprt bo razcep Srmin iz smeri Škofij proti Ljubljani ter priključek Luka Koper. Obvoz iz smeri Škofij proti Ljubljani bo preko priključka Bertoki.\",\n",
    "  \"Delovne zapore\": \"\",\n",
    "  \"Popolne zapore\": \"\",\n",
    "  \"Zastoji\": \"Zaradi jutranje prometne konice je promet povečan na mestnih obvoznicah in vpadnicah.\",\n",
    "  \"Ovire\": \"Na štajerski avtocesti je zaradi okvare tovornega vozila oviran promet med priključkoma Slovenska Bistrica jug in Slovenske Konjice proti Ljubljani.\",\n",
    "  \"Nesreče\": \"Na ljubljanski zahodni obvoznici je oviran promet na uvozu Brdo proti Kosezam. Nastal je zastoj do razcepa Kozarje.\",\n",
    "  \"Vreme\": \"Cesta čez prelaz Vršič je prevozna samo za osebna vozila.\",\n",
    "  \"Opozorila\": \"\",\n",
    "  \"Zimske razmere\": \"\",\n",
    "  \"Prireditve\": \"\",\n",
    "  \"Mejni prehodi\": \"\",\n",
    "  \"Omejitve za tovorna vozila\": \"\",\n",
    "  \"leftover_html\": \"\",\n",
    "  \"timestamp\": \"02-03-2022 07:15:00\"\n",
    "}\n",
    "\n",
    "output_json = {\n",
    "  \"Delo na cesti\": \"\",\n",
    "  \"Dela\": \"\",\n",
    "  \"Delovne zapore\": \"\",\n",
    "  \"Popolne zapore\": \"\",\n",
    "  \"Zastoji\": \"\",\n",
    "  \"Ovire\": \"\",\n",
    "  \"Nesreče\": \"\",\n",
    "  \"Opozorila\": \"Na primorski avtocesti bo od danes zaradi del med razcepom Srmin in priključkom Črni Kal promet v obe smeri potekal le po enem prometnem pasu. Razcep Srmin bo iz smeri Škofij zaprt proti Ljubljani. Obvoz bo prek priključka Bertoki. Zaprt bo tudi priključek Luka Koper proti Ljubljani.\",\n",
    "  \"Vreme\": \"\",\n",
    "  \"Zimske razmere\": \"\",\n",
    "  \"Prireditve\": \"\",\n",
    "  \"Mejni prehodi\": \"\",\n",
    "  \"Omejitve za tovorna vozila\": \"\",\n",
    "  \"leftover_html\": \"Cesta čez prelaz Vršič je prevozna samo za osebna vozila.\",\n",
    "  \"Timestamp\": \"2022-03-02 07:00:00\"\n",
    "}\n",
    "\n",
    "# --- Run the filter ---\n",
    "print(\"--- Determining Relevance ---\")\n",
    "clean_training_input = filter_input_by_output_relevance(input_json, output_json, similarity_threshold=0.6)\n",
    "\n",
    "print(\"\\n--- Original Input ---\")\n",
    "print(json.dumps(input_json, indent=2, ensure_ascii=False))\n",
    "\n",
    "print(\"\\n--- NEW, CLEANED Input for Training ---\")\n",
    "print(json.dumps(clean_training_input, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
