{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fc01675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "input_df = pl.read_csv(\"../data/RTVSlo/PrometnoPorocilo2022.csv\", encoding=\"Windows-1252\")\n",
    "output_df = pl.read_csv(\"../data/RTVSlo/Joined_rtf_files.csv\", encoding=\"utf-8\")\n",
    "input_df = input_df.with_columns(pl.col(\"Datum\").str.strptime(pl.Datetime, \"%m/%d/%Y %H:%M\"))\n",
    "output_df = output_df.with_columns(pl.col(\"Datum\").str.strptime(pl.Datetime, \"%m/%d/%Y %H:%M\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08893f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'encoding': 'Windows-1252', 'confidence': 0.73, 'language': ''}\n"
     ]
    }
   ],
   "source": [
    "import chardet\n",
    "with open(\"../data/RTVSlo/PrometnoPorocilo2022.csv\", \"rb\") as f:\n",
    "    print(chardet.detect(f.read(1000000)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c420196",
   "metadata": {},
   "source": [
    "Fine tunning with transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f730fa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target row datetime: 2022-04-26 13:00:00\n",
      "\n",
      "Last row before target time: 2022-04-26 12:48:00\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime\n",
    "\n",
    "def get_first_input_before_time(output_df_row, input_df):\n",
    "\n",
    "    #'04/30/2022 18:30'\n",
    "    target_datetime = dict(zip(output_df.columns, output_df_row))[\"Datum\"]\n",
    "\n",
    "    \n",
    "    filtered = input_df.filter(\n",
    "        (pl.col(\"Datum\").dt.date() == target_datetime.date()) &\n",
    "        (pl.col(\"Datum\").dt.time() < target_datetime.time())\n",
    "    )\n",
    "\n",
    "    def print_all_rows():\n",
    "        for i in range(len(filtered)):\n",
    "            print(filtered.row(i))\n",
    "\n",
    "    if filtered.height > 0:\n",
    "       #print_all_rows()\n",
    "       return filtered.sort(\"Datum\").row(-1)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "test = get_first_input_before_time(output_df.row(16), input_df)\n",
    "print(\"Target row datetime:\", dict(zip(output_df.columns, output_df.row(16)))[\"Datum\"])\n",
    "print(\"\\nLast row before target time:\", dict(zip(input_df.columns, test))[\"Datum\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326dbc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser\n",
    "\n",
    "class HTMLTextExtractor(HTMLParser):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.text_parts = []\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        self.text_parts.append(data)\n",
    "\n",
    "    def get_text(self):\n",
    "        return ''.join(self.text_parts).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "664f8981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"B1: Nesre?eCesta Rožna Dolina - Ajševica je pri Ajševici zaprta.Cesta Pesek - Oplotnica je v Oplotnici zaprta.OpozorilaNa avtocesti od Sežane proti Mariboru, do Slovenskih Konjic, pelje izredni prevoz. Ob?asno je lahko promet oviran in upo?asnjen.Mejni prehodi?akalna doba je na mejnem prehodu Obrežje.Tovorni prometZaradi praznikov bo po Sloveniji v nedeljo, 1. 5. in v ponedeljek, 2. 5. med 8. in 22. uro, veljala omejitev prometa tovornih vozil, katerih najve?ja dovoljena masa presega 7,5 t.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Zaradi prometne nesreče je zaprta regionalna cesta Ajševica-Rožna Dolina, in to pri Ajševici. Na mejnem prehodu Obrežje vozniki na vstop v državo čakajo do dve uri, v Gruškovju pa pol ure.  \\nPovečan promet pri izstopu iz države pa je na prehodu Dobovec, na katerem vozniki čakajo uro in pol, ter na Obrežju in v Gruškovju, v katerem vozniki čakajo pol ure.\\n\\u0000\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"B1: Mejni prehodi?akalna doba je na mejnih prehodih Obrežje, Dobovec in Gruškovje.Tovorni prometZaradi praznikov bo po Sloveniji v nedeljo, 1. 5. in v ponedeljek, 2. 5. med 8. in 22. uro, veljala omejitev prometa tovornih vozil, katerih najve?ja dovoljena masa presega 7,5 t.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Zaradi pokvarjenega vozila je na štajerski avtocesti v predoru Jasovnik zaprt vozni pas proti Ljubljani. Na mejnih prehodih Sečovlje, Petrina, Dragonja in Dobovec vozniki osebnih vozil na izstop iz države čakajo približno pol ure; na Obrežju, v Gruškovju ter Slovenski vasi pa 1 uro pri vstopu v državo. \\u0000\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"B1: Nesre?eNa primorski avtocesti je pred Vrhniko proti Kopru zaprt prehitevalni pas.Mejni prehodi?akalna doba je na Gruškovje, Jelšanah in Vinici.Delo na cestiCesta Dole - Šentjur bo v Šentjurju zaradi asfaltiranja do 18. ure zaprta. Obvoz za vozila do 7,5 t na relaciji Šentjur - Teharje - Bukovžlak - Dole in obratno. Za vozila nad 7,5 t po državnih cestah in avtocesti.OpozorilaDanes, 27. 4., med 8. in 22. uro, zaradi praznika velja splošna omejitev prometa tovornih vozil, katerih najve?ja dovoljena masa presega 7,5 t. Kljub prazniku bo izjemoma dovoljen promet težkih tovornih vozil namenjenih v in iz Luke Koper.PrireditveCesta ?atež ob Savi - Mokrice bo pri Prilipah zaprta do 20. ure. Obvoz je po delu regionalne ceste ?atež ob Savi - Mokrice in po dolenjski avtocesti med priklju?koma Brežice in Obrežje\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Zaradi prometne nesreče je na gorenjski avtocesti proti Avstriji zaprt prehitevalni pas med priključkom Vodice in počivališčem Vóklo. Na mariborski vzhodni obvoznici promet zaradi del med razcepom Dragučova in krožiščem Pesnica poteka dvosmerno po polovici avtoceste.\\n\\u0000\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"B1: Mejni prehodi?akalna doba je na Jelšanah in Vinici.Delo na cestiCesta Dole - Šentjur bo v Šentjurju zaradi asfaltiranja do 18. ure zaprta. Obvoz za vozila do 7,5 t na relaciji Šentjur - Teharje - Bukovžlak - Dole in obratno. Za vozila nad 7,5 t po državnih cestah in avtocesti.OpozorilaDanes, 27. 4., med 8. in 22. uro, zaradi praznika velja splošna omejitev prometa tovornih vozil, katerih najve?ja dovoljena masa presega 7,5 t. Kljub prazniku bo izjemoma dovoljen promet težkih tovornih vozil namenjenih v in iz Luke Koper.PrireditveCesta ?atež ob Savi - Mokrice bo pri Prilipah zaprta do 20. ure. Obvoz je po delu regionalne ceste ?atež ob Savi - Mokrice in po dolenjski avtocesti med priklju?koma Brežice in Obrežje\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Zaradi prometne nesreče je na gorenjski avtocesti proti Avstriji zaprt prehitevalni pas med priključkom Vodice in počivališčem Vóklo. Od 8-ih do 22-ih bo veljala omejitev prometa tovornih vozil, težjih od 7 ton in pol.\\n\\u0000\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"B1: Nesre?eNa ljubljanski severni obvoznici zaradi prometne nesre?e med krožiš?em Toma?evo in Bežigradom promet proti Kosezam poteka samo po enem prometnem pasu.OpozorilaV sredo, 27. 4., bo med 8. in 22. uro, zaradi praznika veljala splošna omejitev prometa tovornih vozil, katerih najve?ja dovoljena masa presega 7,5 t.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Na južni ljubljanski obvoznici je zaradi pokvarjenega vozila  zaprt izvoz Ljubljana - jug proti dolenjski avtocesti. Zaradi del je na severni ljubljanski obvoznici zaprt uvoz Nove Jarše proti štajerski avtocesti. Na cesti Rogatec - Dobovec poteka promet zaradi prometne nesreče izmenično enosmerno. Na Obrežju je povečana promet osebnih vozil pri vstopu v državo, v Jelšanah pa pri izstopu iz nje. \\u0000\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"B1: Mejni prehodi?akalna doba je v Jelšanah, Obrežju in Gruškovju.OpozorilaV sredo, 27. 4., bo med 8. in 22. uro, zaradi praznika veljala splošna omejitev prometa tovornih vozil, katerih najve?ja dovoljena masa presega 7,5 t.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Na južni ljubljanski obvoznici ni več daljših zastojev proti primorski avtocesti. Tudi na regionalni cesti Ljubljana - Vrhnika se je promet pred delovno zaporo pri priključku Brezovica umiril. Glavna cesta Rogatec - Dobovec je zaradi prometne nesreče še vedno zaprta. Na Obrežju je povečana promet osebnih vozil pri izstopu. V Gruškovju  vozniki tovornih vozil na vstop v državo čakajo do 2 uri.\\n\\u0000\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"B1: Mejni prehodi?akalna doba je v Jelšanah, Obrežju in Gruškovju.OpozorilaV sredo, 27. 4., bo med 8. in 22. uro, zaradi praznika veljala splošna omejitev prometa tovornih vozil, katerih najve?ja dovoljena masa presega 7,5 t.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Na južni ljubljanski obvoznici je zastoj proti primorski avtocesti, ki sega do počivališča Barje. Čas potovanja se tam podaljša za približno četrt ure. Na regionalni cesti Ljubljana - Vrhnika je zastoj pred delovno zaporo pri priključku Brezovica proti Vrhniki. Glavna cesta Rogatec - Dobovec je zaradi prometne nesreče zaprta. Na Obrežju sta povečana promet osebnih vozil pri izstopu iz države ter promet tovornih vozil pri vstopu vanjo.\\nV Gruškovju vozniki tovornih vozil na vstop čakajo do 2 uri.\\n\\u0000\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"B1: Nesre?eNa cesti Rogatec - Dobovec je zaradi prometne nesre?e promet urejen izmeni?no enosmerno pri mejnem prehodu Dobovec.OvireNa južni ljubljanski obvoznici je zaradi tovornega vozila v okvari zaprt izvoz Ljubljana Jug proti Malencam.Zaradi izrednega prevoza je trenutno kratkotrajno zaprt predor Karavanke proti Avstriji.Mejni prehodi?akalna doba je v Jelšanah, Obrežju in Gruškovju.OpozorilaV sredo, 27. 4., bo med 8. in 22. uro, zaradi praznika veljala splošna omejitev prometa tovornih vozil, katerih najve?ja dovoljena masa presega 7,5 t.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Na južni ljubljanski obvoznici je zastoj proti primorski avtocesti, ki sega skoraj do priključka Rudnik. Čas potovanja se tam podaljša za približno četrt ure. Na regionalni cesti Ljubljana - Vrhnika je zastoj pred delovno zaporo pri priključku Brezovica proti Vrhniki. Glavna cesta Rogatec - Dobovec je zaradi prometne nesreče zaprta. Na Obrežju sta povečana promet osebnih vozil pri izstopu iz države ter promet tovornih vozil pri vstopu vanjo.\\nV Gruškovju  vozniki tovornih vozil na vstop čakajo do 2 uri. \\u0000\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"B1: Nesre?eNa cesti Rogatec - Dobovec je zaradi prometne nesre?e promet urejen izmeni?no enosmerno pri mejnem prehodu Dobovec.Mejni prehodi?akalna doba je na Obrežju in Gruškovju.OpozorilaV sredo, 27. 4., bo med 8. in 22. uro, zaradi praznika veljala splošna omejitev prometa tovornih vozil, katerih najve?ja dovoljena masa presega 7,5 t.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Na južni ljubljanski obvoznici je daljši zastoj proti primorski avtocesti, ki sega do priključka Rudnik. Čas potovanja se tam podaljša za približno pol ure.  Zastoj je tudi na zahodni obvoznici od Brda do razcepa Kozarje ter naprej do Brezovice. Zaradi del je zastoj na regionalni cesti Vrhnika - Ljubljana pred delovno zaporo od Lukovice proti Brezovici ter na cesti Podpeč - Brezovica. Na Dragonji in Obrežju je povečan promet osebnih vozil pri izstopu iz države.\\nNa mejnih prehodih Obrežje in Gruškovje vozniki tovornih vozil na vstop v državo čakajo do 2 uri. \\u0000\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"messages\": [\n",
      "            {\n",
      "                \"role\": \"user\",\n",
      "                \"content\": \"B1: Nesre?eNa cesti Rogatec - Dobovec je zaradi prometne nesre?e promet urejen izmeni?no enosmerno pri mejnem prehodu Dobovec.Mejni prehodi?akalna doba je na Obrežju in Gruškovju.OpozorilaV sredo, 27. 4., bo med 8. in 22. uro, zaradi praznika veljala splošna omejitev prometa tovornih vozil, katerih najve?ja dovoljena masa presega 7,5 t.\"\n",
      "            },\n",
      "            {\n",
      "                \"role\": \"assistant\",\n",
      "                \"content\": \"Na podravski avtocesti je zaradi prometne nesreče promet oviran pri mejnem prehodu Gruškovje proti Mariboru. Zaradi del je zastoj na regionalni cesti Vrhnika-Ljubljana pred delovno zaporo od Lukovice proti Brezovici ter na cesti Podpeč-Brezovica. Na Dragonji in Obrežju je povečan promet osebnih vozil pri izstopu iz države.\\nNa mejnih prehodih Obrežje in Gruškovje vozniki tovornih vozil na vstop v državo čakajo do 2 uri.\\n\\u0000\"\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def generate_taffic_IO_examples(input_df, output_df, n_examples=1):\n",
    "    traffic_examples = []\n",
    "    #for i in range(len(output_df)):\n",
    "    for i in range(n_examples):\n",
    "        output_df_row = output_df.row(i)\n",
    "        input_row = get_first_input_before_time(output_df_row, input_df)\n",
    "\n",
    "        def strip_html(html):\n",
    "            parser = HTMLTextExtractor()\n",
    "            parser.feed(html)\n",
    "            return parser.get_text()\n",
    "\n",
    "        def parse_input_message(input_row, columns):\n",
    "            row_dict = dict(zip(columns, input_row))\n",
    "            message = \"\"\n",
    "            for column in [\"A1\", \"B1\", \"C1\"]:\n",
    "                value = row_dict.get(column)\n",
    "                if value is not None and value != \"NULL\":\n",
    "                    clean_value = strip_html(value)\n",
    "                    message += f\"{column}: {clean_value} \"\n",
    "            #print(\"Input message:\", message.strip())\n",
    "            return message.strip()\n",
    "\n",
    "        \n",
    "        def parse_output_message(output_row, columns):\n",
    "            row_dict = dict(zip(columns, output_row))\n",
    "            message = \"\"\n",
    "            for column in [\"content_01\", \"content_02\", \"content_03\", \"content_04\", \"content_05\"]:\n",
    "                if row_dict.get(column) is not None:\n",
    "                    message += f\"{row_dict[column]} \"\n",
    "            return message.strip()\n",
    "\n",
    "\n",
    "        traffic_example = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": parse_input_message(input_row, input_df.columns)},\n",
    "                {\"role\": \"assistant\", \"content\": parse_output_message(output_df_row, output_df.columns)}\n",
    "            ]\n",
    "        }\n",
    "        traffic_examples.append(traffic_example)\n",
    "    return traffic_examples\n",
    "\n",
    "traffic_examples = generate_taffic_IO_examples(input_df, output_df, n_examples=10)\n",
    "#print(traffic_examples)\n",
    "#print in json format\n",
    "import json\n",
    "print(json.dumps(traffic_examples, indent=4, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbad2fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\turkf\\Pictures\\mag\\ONJ\\ONJyfans\\ul-fri-nlp-course-project-2024-2025-onjyfans\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 8\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(traffic_examples)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "print(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9330b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU device: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "GPU memory: 6.00 GB\n",
      "CPU cores: 14 physical, 20 logical\n",
      "RAM: 31.69 GB total\n",
      "Available RAM: 15.99 GB\n"
     ]
    }
   ],
   "source": [
    "# First, let's check what resources we have available\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "model.to(\"cuda\")\n",
    "def check_resources():\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    \n",
    "    import psutil\n",
    "    print(f\"CPU cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count()} logical\")\n",
    "    print(f\"RAM: {psutil.virtual_memory().total / 1024**3:.2f} GB total\")\n",
    "    print(f\"Available RAM: {psutil.virtual_memory().available / 1024**3:.2f} GB\")\n",
    "\n",
    "# Check resources before starting\n",
    "check_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "408a00b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 26.92it/s]\n",
      "Map: 100%|██████████| 8/8 [00:00<00:00, 1019.86 examples/s]\n",
      "Map: 100%|██████████| 2/2 [00:00<00:00, 584.65 examples/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 590.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.86 GiB is allocated by PyTorch, and 87.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m#model_id = \"cjvt/OPT_GaMS-1B-Chat\"\u001b[39;00m\n\u001b[32m     58\u001b[39m model_id = \u001b[33m\"\u001b[39m\u001b[33mcjvt/OPT_GaMS-1B-Chat\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m model, tokenizer = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model_id, dataset)\u001b[39m\n\u001b[32m     37\u001b[39m data_collator = DataCollatorForLanguageModeling(\n\u001b[32m     38\u001b[39m     tokenizer=tokenizer, \n\u001b[32m     39\u001b[39m     mlm=\u001b[38;5;28;01mFalse\u001b[39;00m  \n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m trainer = Trainer(\n\u001b[32m     43\u001b[39m     model=model,\n\u001b[32m     44\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     47\u001b[39m     data_collator=data_collator,\n\u001b[32m     48\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m model.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./fine-tuned-traffic-model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     53\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./fine-tuned-traffic-model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\turkf\\Pictures\\mag\\ONJ\\ONJyfans\\ul-fri-nlp-course-project-2024-2025-onjyfans\\venv\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2243\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2246\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2250\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\turkf\\Pictures\\mag\\ONJ\\ONJyfans\\ul-fri-nlp-course-project-2024-2025-onjyfans\\venv\\Lib\\site-packages\\transformers\\trainer.py:2560\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2553\u001b[39m context = (\n\u001b[32m   2554\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2555\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2557\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2558\u001b[39m )\n\u001b[32m   2559\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2560\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2563\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2564\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2565\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2566\u001b[39m ):\n\u001b[32m   2567\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2568\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\turkf\\Pictures\\mag\\ONJ\\ONJyfans\\ul-fri-nlp-course-project-2024-2025-onjyfans\\venv\\Lib\\site-packages\\transformers\\trainer.py:3736\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3733\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3735\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3736\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3738\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3739\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3740\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3741\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3742\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\turkf\\Pictures\\mag\\ONJ\\ONJyfans\\ul-fri-nlp-course-project-2024-2025-onjyfans\\venv\\Lib\\site-packages\\transformers\\trainer.py:3801\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3799\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3800\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3801\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3802\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3803\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\turkf\\Pictures\\mag\\ONJ\\ONJyfans\\ul-fri-nlp-course-project-2024-2025-onjyfans\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\turkf\\Pictures\\mag\\ONJ\\ONJyfans\\ul-fri-nlp-course-project-2024-2025-onjyfans\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\turkf\\Pictures\\mag\\ONJ\\ONJyfans\\ul-fri-nlp-course-project-2024-2025-onjyfans\\venv\\Lib\\site-packages\\transformers\\models\\opt\\modeling_opt.py:1207\u001b[39m, in \u001b[36mOPTForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, position_ids, cache_position, **kwargs)\u001b[39m\n\u001b[32m   1192\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m   1193\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model.decoder(\n\u001b[32m   1194\u001b[39m     input_ids=input_ids,\n\u001b[32m   1195\u001b[39m     attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1204\u001b[39m     cache_position=cache_position,\n\u001b[32m   1205\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1207\u001b[39m logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.contiguous()\n\u001b[32m   1209\u001b[39m loss = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1211\u001b[39m     \u001b[38;5;66;03m# move labels to correct device to enable model parallelism\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\turkf\\Pictures\\mag\\ONJ\\ONJyfans\\ul-fri-nlp-course-project-2024-2025-onjyfans\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1755\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1753\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1754\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1755\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\turkf\\Pictures\\mag\\ONJ\\ONJyfans\\ul-fri-nlp-course-project-2024-2025-onjyfans\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1766\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1761\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1762\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1764\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1765\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1766\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1768\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1769\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\turkf\\Pictures\\mag\\ONJ\\ONJyfans\\ul-fri-nlp-course-project-2024-2025-onjyfans\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 590.00 MiB. GPU 0 has a total capacity of 6.00 GiB of which 0 bytes is free. Of the allocated memory 12.86 GiB is allocated by PyTorch, and 87.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "def train_model(model_id, dataset):\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "    model.to(\"cuda\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        texts = []\n",
    "        for conversation in examples[\"messages\"]:\n",
    "            formatted_text = \"\"\n",
    "            for message in conversation:\n",
    "                if message[\"role\"] == \"user\":\n",
    "                    formatted_text += f\"Uporabnik: {message['content']}\\n\"\n",
    "                else:\n",
    "                    formatted_text += f\"Asistent: {message['content']}\\n\"\n",
    "            texts.append(formatted_text)\n",
    "        \n",
    "        tokenized = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "        return tokenized\n",
    "    \n",
    "    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=6,\n",
    "        per_device_eval_batch_size=6,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=False  \n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset[\"train\"],\n",
    "        eval_dataset=tokenized_dataset[\"test\"],\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    model.save_pretrained(\"./fine-tuned-traffic-model\")\n",
    "    tokenizer.save_pretrained(\"./fine-tuned-traffic-model\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "#model_id = \"cjvt/OPT_GaMS-1B-Chat\"\n",
    "model_id = \"cjvt/OPT_GaMS-1B-Chat\"\n",
    "model, tokenizer = train_model(model_id, dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d25860ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# First, clear memory\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# Example traffic data (small sample to test with)\n",
    "traffic_examples = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Generiraj poročilo o prometu za avtocesto A1 med Ljubljano in Mariborom.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Na avtocesti A1 med Ljubljano in Mariborom trenutno ni posebnosti. Promet poteka tekoče v obeh smereh.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Stanje na cestah v Ljubljani?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"V Ljubljani trenutno ni večjih zastojev. Na Celovški cesti so dela, ki upočasnjujejo promet.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create a smaller dataset for initial testing\n",
    "dataset = Dataset.from_list(traffic_examples)\n",
    "dataset = dataset.train_test_split(test_size=0.5)  # 50/50 split for this tiny example\n",
    "\n",
    "def train_model(model_id, dataset, device_map=\"auto\"):\n",
    "    try:\n",
    "        # More careful model loading with resource management\n",
    "        print(f\"Loading model from {model_id}...\")\n",
    "        print(f\"Available GPU: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "        \n",
    "        # Try to load with lower precision to save memory\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=device_map,       # Distribute across available GPUs or use CPU\n",
    "            torch_dtype=torch.float16,   # Use half precision to save memory\n",
    "            low_cpu_mem_usage=True       # More memory-efficient loading\n",
    "        )\n",
    "        \n",
    "        print(\"Model loaded successfully\")\n",
    "        \n",
    "        # Load tokenizer (this is usually much lighter)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        print(\"Tokenizer loaded successfully\")\n",
    "        \n",
    "        def preprocess_function(examples):\n",
    "            texts = []\n",
    "            for conversation in examples[\"messages\"]:\n",
    "                formatted_text = \"\"\n",
    "                for message in conversation:\n",
    "                    if message[\"role\"] == \"user\":\n",
    "                        formatted_text += f\"Uporabnik: {message['content']}\\n\"\n",
    "                    else:\n",
    "                        formatted_text += f\"Asistent: {message['content']}\\n\"\n",
    "                texts.append(formatted_text)\n",
    "            \n",
    "            tokenized = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512)\n",
    "            return tokenized\n",
    "        \n",
    "        # Process the dataset\n",
    "        print(\"Preprocessing dataset...\")\n",
    "        tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "        print(\"Dataset preprocessed successfully\")\n",
    "        \n",
    "        # Training arguments with lower memory footprint\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            num_train_epochs=1,             # Start with fewer epochs for testing\n",
    "            per_device_train_batch_size=1,  # Smaller batch size\n",
    "            per_device_eval_batch_size=1,   # Smaller batch size\n",
    "            warmup_steps=10,                # Fewer warmup steps\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=\"./logs\",\n",
    "            logging_steps=1,\n",
    "            # Memory optimization settings\n",
    "            gradient_accumulation_steps=4,  # Accumulate gradients to simulate larger batch\n",
    "            fp16=torch.cuda.is_available(), # Use FP16 if GPU available\n",
    "            dataloader_num_workers=0,       # Don't use multiple workers\n",
    "        )\n",
    "        \n",
    "        # Create data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False\n",
    "        )\n",
    "        \n",
    "        # Create trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_dataset[\"train\"],\n",
    "            eval_dataset=tokenized_dataset[\"test\"],\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        trainer.train()\n",
    "        print(\"Training completed\")\n",
    "        \n",
    "        # Save model\n",
    "        model.save_pretrained(\"./fine-tuned-traffic-model\")\n",
    "        tokenizer.save_pretrained(\"./fine-tuned-traffic-model\")\n",
    "        \n",
    "        return model, tokenizer\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during model loading or training: {e}\")\n",
    "        \n",
    "        # If the error is related to CUDA out of memory, try CPU\n",
    "        if \"CUDA out of memory\" in str(e) and device_map == \"auto\":\n",
    "            print(\"GPU memory error detected. Trying to load on CPU instead...\")\n",
    "            return train_model(model_id, dataset, device_map=\"cpu\")\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# Try loading with smaller model first to test\n",
    "try:\n",
    "    print(\"First trying a smaller model to verify environment...\")\n",
    "    small_model_id = \"cjvt/OPT_SloT-300M-Chat\"  # Much smaller model\n",
    "    test_model = AutoModelForCausalLM.from_pretrained(small_model_id, torch_dtype=torch.float16)\n",
    "    print(\"Small model loaded successfully, environment seems functional\")\n",
    "    del test_model  # Free the memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading test model: {e}\")\n",
    "\n",
    "# Now try the actual model\n",
    "try:\n",
    "    model_id = \"cjvt/OPT_GaMS-1B-Chat\"\n",
    "    model, tokenizer = train_model(model_id, dataset)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to train model: {e}\")\n",
    "    print(\"Attempting to load model without training...\")\n",
    "    try:\n",
    "        # Just load the model to check if that's possible\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            device_map=\"cpu\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        print(\"Model loaded successfully without training\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Model loading also failed: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "def generate_traffic_report(prompt, model_path=\"./fine-tuned-traffic-model\"):\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "    \n",
    "    formatted_input = f\"Uporabnik: {prompt}\\nAsistent:\"\n",
    "    \n",
    "    response = generator(formatted_input, max_length=200, do_sample=True, temperature=0.7, truncation=True)\n",
    "    \n",
    "    assistant_response = response[0][\"generated_text\"].split(\"Asistent:\")[1].strip()\n",
    "    \n",
    "    return assistant_response\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Poročaj o stanju na primorski avtocesti med Vrhniko in Koprom. Dela na cesti pri Postojni.\"\n",
    "report = generate_traffic_report(prompt)\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde54db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def evaluate_model(model_path, test_examples):\n",
    "    # Create evaluation dataset\n",
    "    eval_dataset = Dataset.from_list(test_examples)\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    # Create generator\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "    \n",
    "    # Evaluate on each example\n",
    "    results = []\n",
    "    for example in test_examples:\n",
    "        user_input = example[\"messages\"][0][\"content\"]\n",
    "        expected_output = example[\"messages\"][1][\"content\"]\n",
    "        \n",
    "        # Generate response\n",
    "        formatted_input = f\"Uporabnik: {user_input}\\nAsistent:\"\n",
    "        response = generator(formatted_input, max_length=200)\n",
    "        generated = response[0][\"generated_text\"].split(\"Asistent:\")[1].strip()\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            \"input\": user_input,\n",
    "            \"expected\": expected_output,\n",
    "            \"generated\": generated\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example evaluation\n",
    "test_examples = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Kakšno je stanje na gorenjski avtocesti proti Jesenicam?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Na gorenjski avtocesti proti Jesenicam trenutno ni posebnosti ali zastojev. Promet poteka tekoče na celotnem odseku.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "evaluation_results = evaluate_model(\"./fine-tuned-traffic-model\", test_examples)\n",
    "print(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8a82dc",
   "metadata": {},
   "source": [
    "Evaluation will then follow next procedures.: \n",
    "\n",
    "1. Consistent Examples in Training Data\n",
    "The most straightforward approach is providing numerous consistent examples:\n",
    "\n",
    "Include many examples with correct road naming formats\n",
    "Ensure consistency across all training examples\n",
    "Cover all variations of road types (highways, regional roads, streets)\n",
    "\n",
    "For example, always using formats like:\n",
    "\n",
    "\"avtocesta A1\" (not just \"A1\")\n",
    "\"regionalna cesta R2-409\" (with full designation)\n",
    "\"Celovška cesta\" (with proper capitalization)\n",
    "\n",
    "2. Instruction-Based Fine-tuning\n",
    "You can explicitly teach the model road naming rules:\n",
    "pythonroad_naming_examples = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Kako pravilno poimenovati avtocesto A1?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Avtocesta A1 se pravilno imenuje 'avtocesta A1' ali 'avtocesta A1 Šentilj-Srmin'. Pri prvem omenjanju v poročilu uporabite polno ime, kasneje lahko uporabite krajšo obliko 'A1'.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Navedi pravila za poimenovanje cest v prometnih poročilih.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Pri poimenovanju cest v prometnih poročilih upoštevajte naslednja pravila:\\n1. Avtoceste: 'avtocesta A1', ne samo 'A1'\\n2. Hitre ceste: 'hitra cesta H4', ne samo 'H4'\\n3. Regionalne ceste: 'regionalna cesta R2-409'\\n4. Mestne ceste: uporabite polno ime, npr. 'Celovška cesta', ne 'Celovška'\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "Add these instructional examples to your training data to explicitly teach rules.\n",
    "3. Data Augmentation with Rule Enforcement\n",
    "Create augmented examples that specifically focus on proper naming:\n",
    "python# Original example\n",
    "original = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Poročaj o stanju na A1.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Na avtocesti A1 med Ljubljano in Mariborom trenutno ni zastojev.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Add variations with correct naming to reinforce the rule\n",
    "variations = [\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Poročaj o stanju na A1 proti Mariboru.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Na avtocesti A1 v smeri proti Mariboru trenutno ni posebnosti.\"}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Kako je na A1?\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"Na avtocesti A1 Šentilj-Srmin je promet tekoč v obeh smereh.\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "4. Post-Processing Rules\n",
    "For critical naming conventions, you can implement post-processing:\n",
    "pythondef enforce_road_naming(generated_text):\n",
    "    # Dictionary of road type patterns and their correct forms\n",
    "    road_patterns = {\n",
    "        r'\\b(A\\d+)\\b(?! avtocest)': r'avtocesta \\1',  # A1 → avtocesta A1\n",
    "        r'\\b(H\\d+)\\b(?! hitr)': r'hitra cesta \\1',    # H4 → hitra cesta H4\n",
    "        # Add more patterns as needed\n",
    "    }\n",
    "    \n",
    "    # Apply all patterns\n",
    "    result = generated_text\n",
    "    for pattern, replacement in road_patterns.items():\n",
    "        result = re.sub(pattern, replacement, result)\n",
    "        \n",
    "    return result\n",
    "\n",
    "# Use this after generation\n",
    "response = generate_traffic_report(prompt)\n",
    "corrected_response = enforce_road_naming(response)\n",
    "5. Evaluation and Filtering\n",
    "Create a specific evaluation metric for road naming compliance:\n",
    "pythondef evaluate_road_naming(generated_text):\n",
    "    # Define patterns for improper road naming\n",
    "    improper_patterns = [\n",
    "        r'\\b(A\\d+)\\b(?! avtocest)',  # A1 without \"avtocesta\"\n",
    "        r'\\b(H\\d+)\\b(?! hitr)',      # H4 without \"hitra cesta\"\n",
    "        # Add more patterns\n",
    "    ]\n",
    "    \n",
    "    # Count violations\n",
    "    violations = 0\n",
    "    for pattern in improper_patterns:\n",
    "        violations += len(re.findall(pattern, generated_text))\n",
    "        \n",
    "    return {\n",
    "        \"violations\": violations,\n",
    "        \"compliant\": violations == 0\n",
    "    }\n",
    "Practical Implementation Approach\n",
    "Here's how I recommend implementing these strategies:\n",
    "\n",
    "Start with data quality: Ensure all your training examples use correct road naming\n",
    "Add instruction examples: Include specific examples teaching the naming rules\n",
    "Evaluate during development: Create a custom metric to track naming compliance\n",
    "Implement post-processing: As a safety net for critical applications\n",
    "\n",
    "By combining these approaches, you'll significantly improve your model's adherence to specific road naming conventions while still maintaining natural-sounding Slovenian text.\n",
    "Would you like me to elaborate on any of these strategies or provide more specific examples for Slovenian road naming conventions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c412822",
   "metadata": {},
   "source": [
    "Zero shot prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20278b",
   "metadata": {},
   "source": [
    "few shot prompting with all the rules exmplained?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
