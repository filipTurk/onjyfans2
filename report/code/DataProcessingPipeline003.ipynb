{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2162125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Datum', 'Similarity_Score', 'Input_Message', 'Output_Message',\n",
      "       'Input_JSON', 'Output_JSON', 'Input_timesort', 'json_timesort'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/trainingdataset_optimized_normalized.csv')\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d596bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading forward model: Helsinki-NLP/opus-mt-sl-en\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 128\u001b[39m\n\u001b[32m    123\u001b[39m columns_to_augment = [\u001b[33m'\u001b[39m\u001b[33mInput_Message_Normalized\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mOutput_Message_Normalized\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# 3. Run the augmentation\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# For a large DataFrame, this will take time. Run it on a subset first to test.\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# df_augmented = augment_with_back_translation(df.head(2), columns_to_augment) # Test with 2 rows\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m df_augmented = \u001b[43maugment_with_back_translation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns_to_augment\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Run on the full sample\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# 4. Combine original and augmented data for the final training set\u001b[39;00m\n\u001b[32m    131\u001b[39m df_final_training_set = pd.concat([df, df_augmented], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36maugment_with_back_translation\u001b[39m\u001b[34m(df, columns_to_translate, batch_size)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03mApplies back-translation to specified columns of a DataFrame to create augmented data.\u001b[39;00m\n\u001b[32m     63\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    pd.DataFrame: A new DataFrame containing the original and augmented (back-translated) rows.\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# Initialize the translation engine\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m translator = \u001b[43mBackTranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Create a copy of the original data to be the basis for our augmented data\u001b[39;00m\n\u001b[32m     76\u001b[39m augmented_df = df.copy()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mBackTranslator.__init__\u001b[39m\u001b[34m(self, model_name_fwd, model_name_bwd)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Load forward model (Slovenian to English)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading forward model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_fwd\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28mself\u001b[39m.tokenizer_fwd = \u001b[43mMarianTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(model_name_fwd)\n\u001b[32m     24\u001b[39m \u001b[38;5;28mself\u001b[39m.model_fwd = MarianMTModel.from_pretrained(model_name_fwd).to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Load backward model (English to Slovenian)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:1885\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   1883\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mis_dummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mmro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1884\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m1885\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:1871\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   1868\u001b[39m         failed.append(msg.format(name))\n\u001b[32m   1870\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m1871\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nMarianTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "import time\n",
    "\n",
    "# --- 1. Setup the Back-Translation Engine ---\n",
    "\n",
    "class BackTranslator:\n",
    "    def __init__(self, model_name_fwd='Helsinki-NLP/opus-mt-sl-en', model_name_bwd='Helsinki-NLP/opus-mt-en-sl'):\n",
    "        \"\"\"\n",
    "        Initializes the forward (SL->EN) and backward (EN->SL) translation models.\n",
    "        \n",
    "        Args:\n",
    "            model_name_fwd (str): The name of the Slovenian to English translation model on Hugging Face.\n",
    "            model_name_bwd (str): The name of the English to Slovenian translation model.\n",
    "        \"\"\"\n",
    "        # Check for GPU availability\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        # Load forward model (Slovenian to English)\n",
    "        print(f\"Loading forward model: {model_name_fwd}\")\n",
    "        self.tokenizer_fwd = MarianTokenizer.from_pretrained(model_name_fwd)\n",
    "        self.model_fwd = MarianMTModel.from_pretrained(model_name_fwd).to(self.device)\n",
    "\n",
    "        # Load backward model (English to Slovenian)\n",
    "        print(f\"Loading backward model: {model_name_bwd}\")\n",
    "        self.tokenizer_bwd = MarianTokenizer.from_pretrained(model_name_bwd)\n",
    "        self.model_bwd = MarianMTModel.from_pretrained(model_name_bwd).to(self.device)\n",
    "\n",
    "    def translate(self, texts, model, tokenizer):\n",
    "        \"\"\"Helper function to perform translation on a batch of texts.\"\"\"\n",
    "        # Tokenize the texts\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        \n",
    "        # Generate translated tokens\n",
    "        translated_tokens = model.generate(**inputs)\n",
    "        \n",
    "        # Decode the tokens back to text\n",
    "        return tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    def back_translate_batch(self, texts):\n",
    "        \"\"\"\n",
    "        Performs a round-trip translation on a batch of texts.\n",
    "        SL -> EN -> SL\n",
    "        \"\"\"\n",
    "        if not texts or not any(t.strip() for t in texts):\n",
    "            return [\"\"] * len(texts) # Return empty strings for empty input\n",
    "            \n",
    "        # Forward translation\n",
    "        english_texts = self.translate(texts, self.model_fwd, self.tokenizer_fwd)\n",
    "        \n",
    "        # Backward translation\n",
    "        slovenian_texts_back = self.translate(english_texts, self.model_bwd, self.tokenizer_bwd)\n",
    "        \n",
    "        return slovenian_texts_back\n",
    "\n",
    "# --- 2. Main DataFrame Processing Function ---\n",
    "\n",
    "def augment_with_back_translation(df: pd.DataFrame, columns_to_translate: list, batch_size: int = 16):\n",
    "    \"\"\"\n",
    "    Applies back-translation to specified columns of a DataFrame to create augmented data.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        columns_to_translate (list): A list of column names to apply back-translation to.\n",
    "        batch_size (int): The number of rows to process at once. Helps manage memory.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame containing the original and augmented (back-translated) rows.\n",
    "    \"\"\"\n",
    "    # Initialize the translation engine\n",
    "    translator = BackTranslator()\n",
    "    \n",
    "    # Create a copy of the original data to be the basis for our augmented data\n",
    "    augmented_df = df.copy()\n",
    "    \n",
    "    total_rows = len(df)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process the DataFrame in batches\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        for col in columns_to_translate:\n",
    "            # Get the text from the batch for the current column\n",
    "            texts_to_translate = batch[col].tolist()\n",
    "            \n",
    "            # Perform back-translation\n",
    "            back_translated_texts = translator.back_translate_batch(texts_to_translate)\n",
    "            \n",
    "            # Update the corresponding column in the augmented DataFrame\n",
    "            # Use .iloc indexer to ensure correct assignment\n",
    "            augmented_df.iloc[i:i+batch_size, df.columns.get_loc(col)] = back_translated_texts\n",
    "\n",
    "        # Progress update\n",
    "        elapsed_time = time.time() - start_time\n",
    "        rows_done = min(i + batch_size, total_rows)\n",
    "        print(f\"Processed {rows_done}/{total_rows} rows. Time elapsed: {elapsed_time:.2f}s\")\n",
    "        \n",
    "    return augmented_df\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# 1. Create a sample DataFrame (in a real scenario, you'd load your full df)\n",
    "data = {\n",
    "    'Input_Message_Normalized': [\n",
    "        \"Štajerska avtocesta bo zaprta med priključkoma Blagovica in Vransko do pojutrišnjem.\",\n",
    "        \"Dela bodo potekala tudi jutri.\"\n",
    "    ],\n",
    "    'Output_Message_Normalized': [\n",
    "        \"Zaradi del bo avtocesta med Blagovico in Vranskim zaprta do ponedeljka.\",\n",
    "        \"Dela se bodo nadaljevala jutri.\"\n",
    "    ],\n",
    "    # Add other columns to ensure they are preserved\n",
    "    'Datum': ['30-11-2024 07:16:00', '01-12-2024 09:00:00'],\n",
    "    'Input_timesort': [1, 2]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Define which columns you want to augment\n",
    "# It's best to use the final, normalized columns for this\n",
    "columns_to_augment = ['Input_Message_Normalized', 'Output_Message_Normalized']\n",
    "\n",
    "# 3. Run the augmentation\n",
    "# For a large DataFrame, this will take time. Run it on a subset first to test.\n",
    "# df_augmented = augment_with_back_translation(df.head(2), columns_to_augment) # Test with 2 rows\n",
    "df_augmented = augment_with_back_translation(df, columns_to_augment) # Run on the full sample\n",
    "\n",
    "# 4. Combine original and augmented data for the final training set\n",
    "df_final_training_set = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "# Display the results\n",
    "print(\"\\n--- Original Data ---\")\n",
    "display(df)\n",
    "\n",
    "print(\"\\n--- Augmented (Back-Translated) Data ---\")\n",
    "display(df_augmented)\n",
    "\n",
    "print(\"\\n--- Final Combined Training Set (Original + Augmented) ---\")\n",
    "display(df_final_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37ea13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "\n",
    "# --- 1. Back-Translation Engine ---\n",
    "\n",
    "class BackTranslator:\n",
    "    def __init__(self, model_name_fwd='Helsinki-NLP/opus-mt-sl-en', model_name_bwd='Helsinki-NLP/opus-mt-en-sl'):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device} for translation models.\")\n",
    "        self.tokenizer_fwd = MarianTokenizer.from_pretrained(model_name_fwd)\n",
    "        self.model_fwd = MarianMTModel.from_pretrained(model_name_fwd).to(self.device)\n",
    "        self.tokenizer_bwd = MarianTokenizer.from_pretrained(model_name_bwd)\n",
    "        self.model_bwd = MarianMTModel.from_pretrained(model_name_bwd).to(self.device)\n",
    "\n",
    "    def translate(self, texts, model, tokenizer):\n",
    "        inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        translated_tokens = model.generate(**inputs)\n",
    "        return tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    def back_translate_text(self, text):\n",
    "        \"\"\"Back-translates a single string, handling multiple sentences inside it.\"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return \"\"\n",
    "        \n",
    "        # Split into sentences, translate each, then rejoin. This is more robust.\n",
    "        sentences = re.split(r'(?<=[.?!])\\s+', text.strip())\n",
    "        back_translated_sentences = self.translate(self.translate(sentences, self.model_fwd, self.tokenizer_fwd), self.model_bwd, self.tokenizer_bwd)\n",
    "        return \" \".join(back_translated_sentences)\n",
    "\n",
    "    def back_translate_json(self, json_str):\n",
    "        \"\"\"Back-translates all string values within a JSON string.\"\"\"\n",
    "        try:\n",
    "            json_obj = json.loads(json_str)\n",
    "            \n",
    "            # Recursive helper function\n",
    "            def traverse_and_translate(node):\n",
    "                if isinstance(node, dict):\n",
    "                    return {k: traverse_and_translate(v) for k, v in node.items()}\n",
    "                elif isinstance(node, list):\n",
    "                    return [traverse_and_translate(elem) for elem in node]\n",
    "                elif isinstance(node, str):\n",
    "                    return self.back_translate_text(node)\n",
    "                else:\n",
    "                    return node\n",
    "            \n",
    "            translated_obj = traverse_and_translate(json_obj)\n",
    "            return json.dumps(translated_obj, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            # If JSON is invalid, return it as is.\n",
    "            return json_str\n",
    "\n",
    "# --- 2. Similarity Score Calculator ---\n",
    "\n",
    "class SimilarityCalculator:\n",
    "    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"Using device: {self.device} for similarity model.\")\n",
    "        self.model = SentenceTransformer(model_name, device=self.device)\n",
    "\n",
    "    def get_text_from_json(self, json_str):\n",
    "        \"\"\"Extracts all text content from a JSON string.\"\"\"\n",
    "        try:\n",
    "            data = json.loads(json_str)\n",
    "            text_parts = []\n",
    "            # Recursive helper\n",
    "            def extract(node):\n",
    "                if isinstance(node, dict):\n",
    "                    for value in node.values(): extract(value)\n",
    "                elif isinstance(node, list):\n",
    "                    for item in node: extract(item)\n",
    "                elif isinstance(node, str):\n",
    "                    text_parts.append(node)\n",
    "            extract(data)\n",
    "            return \" \".join(text_parts)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            return \"\"\n",
    "\n",
    "    def calculate_scores(self, df_batch):\n",
    "        \"\"\"Calculates similarity for a batch DataFrame.\"\"\"\n",
    "        # Consolidate all text from Input and Output for a comprehensive comparison\n",
    "        input_texts = df_batch['Input_Message'] + \" \" + df_batch['Input_JSON'].apply(self.get_text_from_json)\n",
    "        output_texts = df_batch['Output_Message'] + \" \" + df_batch['Output_JSON'].apply(self.get_text_from_json)\n",
    "\n",
    "        # Encode the texts\n",
    "        embeddings1 = self.model.encode(input_texts.tolist(), convert_to_tensor=True, show_progress_bar=False)\n",
    "        embeddings2 = self.model.encode(output_texts.tolist(), convert_to_tensor=True, show_progress_bar=False)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "        return [cosine_scores[i][i].item() for i in range(len(df_batch))]\n",
    "\n",
    "# --- 3. Main DataFrame Augmentation Function ---\n",
    "\n",
    "def augment_dataframe(df: pd.DataFrame, batch_size: int = 8):\n",
    "    \"\"\"\n",
    "    Performs back-translation on text and JSON fields, then recalculates similarity scores.\n",
    "    \"\"\"\n",
    "    translator = BackTranslator()\n",
    "    scorer = SimilarityCalculator()\n",
    "    \n",
    "    augmented_rows = []\n",
    "    total_rows = len(df)\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, total_rows, batch_size):\n",
    "        batch_df = df.iloc[i:i+batch_size].copy()\n",
    "        \n",
    "        # --- Back-Translate ---\n",
    "        batch_df['Input_Message'] = batch_df['Input_Message'].apply(translator.back_translate_text)\n",
    "        batch_df['Output_Message'] = batch_df['Output_Message'].apply(translator.back_translate_text)\n",
    "        batch_df['Input_JSON'] = batch_df['Input_JSON'].apply(translator.back_translate_json)\n",
    "        batch_df['Output_JSON'] = batch_df['Output_JSON'].apply(translator.back_translate_json)\n",
    "        \n",
    "        # --- Recalculate Similarity ---\n",
    "        batch_df['Similarity_Score'] = scorer.calculate_scores(batch_df)\n",
    "        \n",
    "        augmented_rows.append(batch_df)\n",
    "        \n",
    "        # Progress update\n",
    "        elapsed_time = time.time() - start_time\n",
    "        rows_done = min(i + batch_size, total_rows)\n",
    "        print(f\"Processed {rows_done}/{total_rows} rows. Time elapsed: {elapsed_time:.2f}s\")\n",
    "\n",
    "    return pd.concat(augmented_rows)\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# 1. Create a sample DataFrame with the correct structure\n",
    "csv_data = \"\"\"Datum,Similarity_Score,Input_Message,Output_Message,Input_JSON,Output_JSON,Input_timesort,json_timesort\n",
    "\"30-11-2024 07:16:00\",0.9,\"Štajerska avtocesta bo zaprta med priključkoma Blagovica in Vransko.\",\"Zaradi del bo avtocesta med Blagovico in Vranskim zaprta.\",\"{\"\"Popolne zapore\"\": \"\"Štajerska avtocesta bo zaprta med priključkoma Blagovica in Vransko.\"\", \"\"Opozorila\"\": \"\"Obvoz je urejen.\"\"}\",\"{\"\"Zastoji\"\": \"\"Na avtocesti med Blagovico in Vranskim je zapora.\"\", \"\"leftover_html\"\": \"\"Voznikom priporočamo obvoz.\"\"}\",\"1\",\"2\"\n",
    "\"\"\"\n",
    "df = pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "# 2. Run the full augmentation pipeline (on the sample)\n",
    "df_augmented = augment_dataframe(df, batch_size=4)\n",
    "\n",
    "# 3. Combine original and new data\n",
    "df_final_training_set = pd.concat([df, df_augmented], ignore_index=True)\n",
    "\n",
    "# 4. Display the results\n",
    "print(\"\\n--- Original Data ---\")\n",
    "display(df)\n",
    "\n",
    "print(\"\\n--- Augmented (Back-Translated) Data with New Similarity Score ---\")\n",
    "display(df_augmented)\n",
    "\n",
    "print(\"\\n--- Final Combined Training Set (Original + Augmented) ---\")\n",
    "display(df_final_training_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
